<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 Otros puntos importantes de regresión lineal | Statistical Notes</title>
  <meta name="description" content="Notas sobre temas relacionados a la estadística" />
  <meta name="generator" content="bookdown 0.21.6 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 Otros puntos importantes de regresión lineal | Statistical Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas sobre temas relacionados a la estadística" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 Otros puntos importantes de regresión lineal | Statistical Notes" />
  
  <meta name="twitter:description" content="Notas sobre temas relacionados a la estadística" />
  

<meta name="author" content="Carlos Fernando Vásquez Guerra" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regresión-lineal-múltiple.html"/>
<link rel="next" href="anova-vs-regresión-lineal.html"/>
<script src="libs/header-attrs-2.7.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i>Objetivos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura"><i class="fa fa-check"></i>Estructura</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#detalles-técnicos"><i class="fa fa-check"></i>Detalles técnicos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I Estadística no paramétrica</b></span></li>
<li class="chapter" data-level="1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html"><i class="fa fa-check"></i><b>1</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="1.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#qué-es-una-prueba-de-hipótesis"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es una Prueba de Hipótesis?</a></li>
<li class="chapter" data-level="1.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#dos-tipos-de-hipótesis"><i class="fa fa-check"></i><b>1.2</b> Dos tipos de hipótesis</a></li>
<li class="chapter" data-level="1.3" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#significancia-y-confianza"><i class="fa fa-check"></i><b>1.3</b> Significancia y confianza</a></li>
<li class="chapter" data-level="1.4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#rechazar-h_0"><i class="fa fa-check"></i><b>1.4</b> Rechazar <span class="math inline">\(H_0\)</span></a></li>
<li class="chapter" data-level="1.5" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#tipos-de-pruebas"><i class="fa fa-check"></i><b>1.5</b> Tipos de pruebas</a></li>
<li class="chapter" data-level="1.6" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#p-value"><i class="fa fa-check"></i><b>1.6</b> <span class="math inline">\(P-value\)</span></a></li>
<li class="chapter" data-level="1.7" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#algunos-puntos-a-recordar"><i class="fa fa-check"></i><b>1.7</b> Algunos puntos a recordar</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pruebas-binomiales-y-de-rangos.html"><a href="pruebas-binomiales-y-de-rangos.html"><i class="fa fa-check"></i><b>2</b> Pruebas binomiales y de rangos</a></li>
<li class="chapter" data-level="3" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html"><i class="fa fa-check"></i><b>3</b> Tablas de contingencia</a></li>
<li class="chapter" data-level="4" data-path="pruebas-de-bondad-y-ajuste.html"><a href="pruebas-de-bondad-y-ajuste.html"><i class="fa fa-check"></i><b>4</b> Pruebas de bondad y ajuste</a>
<ul>
<li class="chapter" data-level="4.1" data-path="pruebas-de-bondad-y-ajuste.html"><a href="pruebas-de-bondad-y-ajuste.html#funciones-de-distribución-comunes-y-técnicas-de-identificación"><i class="fa fa-check"></i><b>4.1</b> Funciones de distribución comunes y técnicas de identificación</a></li>
<li class="chapter" data-level="4.2" data-path="pruebas-de-bondad-y-ajuste.html"><a href="pruebas-de-bondad-y-ajuste.html#programación-y-evaluación"><i class="fa fa-check"></i><b>4.2</b> Programación y evaluación</a></li>
<li class="chapter" data-level="4.3" data-path="pruebas-de-bondad-y-ajuste.html"><a href="pruebas-de-bondad-y-ajuste.html#comparación-de-pruebas"><i class="fa fa-check"></i><b>4.3</b> Comparación de pruebas</a></li>
<li class="chapter" data-level="4.4" data-path="pruebas-de-bondad-y-ajuste.html"><a href="pruebas-de-bondad-y-ajuste.html#el-caso-de-la-normal"><i class="fa fa-check"></i><b>4.4</b> El caso de la normal</a></li>
</ul></li>
<li class="part"><span><b>II Regresión lineal</b></span></li>
<li class="chapter" data-level="5" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html"><i class="fa fa-check"></i><b>5</b> Regresión lineal simple</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#correlación-y-linearidad"><i class="fa fa-check"></i><b>5.1</b> Correlación y linearidad</a></li>
<li class="chapter" data-level="5.2" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#variables-categóricas"><i class="fa fa-check"></i><b>5.2</b> Variables categóricas</a></li>
<li class="chapter" data-level="5.3" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#paquetes-y-funciones-útiles"><i class="fa fa-check"></i><b>5.3</b> Paquetes y funciones útiles</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#correlación"><i class="fa fa-check"></i><b>5.3.1</b> Correlación</a></li>
<li class="chapter" data-level="5.3.2" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#regresión"><i class="fa fa-check"></i><b>5.3.2</b> Regresión</a></li>
<li class="chapter" data-level="5.3.3" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#tidymodelsbroom"><i class="fa fa-check"></i><b>5.3.3</b> TidyModels::Broom</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#ejemplo"><i class="fa fa-check"></i><b>5.4</b> Ejemplo</a></li>
<li class="chapter" data-level="5.5" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#otros-puntos-importantes"><i class="fa fa-check"></i><b>5.5</b> Otros puntos importantes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html"><i class="fa fa-check"></i><b>6</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#pruebas-de-hipótesis-y-anova-e-intervalos"><i class="fa fa-check"></i><b>6.1</b> Pruebas de hipótesis y ANOVA e intervalos</a></li>
<li class="chapter" data-level="6.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#interacción-y-selección-de-variables"><i class="fa fa-check"></i><b>6.2</b> Interacción y selección de variables</a></li>
<li class="chapter" data-level="6.3" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#supuestos-y-problemas-potenciales"><i class="fa fa-check"></i><b>6.3</b> Supuestos y problemas potenciales</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#transformaciones"><i class="fa fa-check"></i><b>6.3.1</b> +Transformaciones</a></li>
<li class="chapter" data-level="6.3.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#interpretaciones"><i class="fa fa-check"></i><b>6.3.2</b> Interpretaciones</a></li>
<li class="chapter" data-level="6.3.3" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#validación"><i class="fa fa-check"></i><b>6.3.3</b> Validación</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#predicción"><i class="fa fa-check"></i><b>6.4</b> Predicción</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="otros-puntos-importantes-de-regresión-lineal.html"><a href="otros-puntos-importantes-de-regresión-lineal.html"><i class="fa fa-check"></i><b>7</b> Otros puntos importantes de regresión lineal</a>
<ul>
<li class="chapter" data-level="7.1" data-path="otros-puntos-importantes-de-regresión-lineal.html"><a href="otros-puntos-importantes-de-regresión-lineal.html#comparación-de-modelos"><i class="fa fa-check"></i><b>7.1</b> Comparación de modelos</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="otros-puntos-importantes-de-regresión-lineal.html"><a href="otros-puntos-importantes-de-regresión-lineal.html#r2"><i class="fa fa-check"></i><b>7.1.1</b> R^2</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="otros-puntos-importantes-de-regresión-lineal.html"><a href="otros-puntos-importantes-de-regresión-lineal.html#análisis-de-valores-influyentes"><i class="fa fa-check"></i><b>7.2</b> Análisis de valores influyentes</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="otros-puntos-importantes-de-regresión-lineal.html"><a href="otros-puntos-importantes-de-regresión-lineal.html#outliers"><i class="fa fa-check"></i><b>7.2.1</b> Outliers</a></li>
<li class="chapter" data-level="7.2.2" data-path="otros-puntos-importantes-de-regresión-lineal.html"><a href="otros-puntos-importantes-de-regresión-lineal.html#valores-con-alto-apalancamiento"><i class="fa fa-check"></i><b>7.2.2</b> Valores con alto “apalancamiento”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="anova-vs-regresión-lineal.html"><a href="anova-vs-regresión-lineal.html"><i class="fa fa-check"></i><b>8</b> ANOVA vs Regresión lineal</a></li>
<li class="chapter" data-level="9" data-path="librerías-para-eda.html"><a href="librerías-para-eda.html"><i class="fa fa-check"></i><b>9</b> Librerías para EDA</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Hecho con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="otros-puntos-importantes-de-regresión-lineal" class="section level1" number="7">
<h1><span class="header-section-number">Capítulo 7</span> Otros puntos importantes de regresión lineal</h1>
<style type="text/css">
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
</style>
<ul>
<li>Si se específica el tipo de regresión, ¿Hay otras técnicas de modelaje?</li>
</ul>
<p>¡Claro! existen muchísimas técnicas de modelaje, en cuanto a la regresión tenemos otros tipos además de la lineal, como la regresión logística, poisson, polinómica, splines, etc.</p>
<ul>
<li>¿El modelo es lineal en que sentido?</li>
</ul>
<p>Es lineal en sus coeficientes, es decir que los siguientes modelos también son lineales:</p>
<p><span class="math display">\[
\begin{array}{c}
\mu\left\{Y|X_1, X_2\right\} = \beta_0+\beta_1X_1+\beta_2X_2\\
\mu\left\{Y|X_1\right\} = \beta_0+\beta_1X_1+\beta_2X_1^2\\
\mu\left\{Y|X_1, X_2\right\} = \beta_0+\beta_1X_1+\beta_2X_1^2+\beta_3X_1X_2\\
\mu\left\{Y|X_1, X_2\right\} = \beta_0+\beta_1\log(X_1)+\beta_2\log(X_2)
\end{array}
\]</span></p>
<ul>
<li><p>No hay que olvidar toda la teoría que esta por detrás de tal modelo, como la utilización del procedimiento de mínimos cuadrados para estimar los parámetros del modelo.</p></li>
<li><p>¿Es suficiente realizar una regresión con todos los datos “limpios?”</p></li>
</ul>
<p>En algunos casos no. Piensen en la <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">paradoja de Simpson</a>, la cual menciona que una tendencia que aparece en varios grupos de datos puede desaparece o invertirse cuando se combinan los grupos. Bajo este enfoque, siempre es bueno segregar la información si se considera que dicha segregación mejorará los resultados del modelo.</p>
<ul>
<li>¿Hay otros métodos para crear un modelo de regresión sin utilizar el método de mínimos cuadrados?</li>
</ul>
<p>SÍ.</p>
<div id="comparación-de-modelos" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Comparación de modelos</h2>
<p><a href="https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html" class="uri">https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html</a></p>
<ul>
<li><a href="https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html">modelsummary: regression tables with side-by-side models</a></li>
<li><a href="https://www.statology.org/partial-f-test/">What is a Partial F-Test?</a></li>
<li><a href="https://stats.stackexchange.com/questions/145790/anova-table-for-model-in-r">StackExchange: ANOVA Table for Model In R</a></li>
<li><a href="https://stats.stackexchange.com/questions/57746/what-is-residual-standard-error">StackExchange: What is residual standard error?</a></li>
</ul>
<div id="r2" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> R^2</h3>
<ul>
<li><a href="https://statisticsbyjim.com/regression/standard-error-regression-vs-r-squared/">Standard Error of the Regression vs. R-squared</a></li>
</ul>
</div>
</div>
<div id="análisis-de-valores-influyentes" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Análisis de valores influyentes</h2>
<div id="outliers" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Outliers</h3>
<p>Como ya se ha mencionado, ay algunos datos que nos pueden generar problemas, y por lo mismo previamente se han eliminado algunos datos, como ciertos outliers y ceros en nuestras variables.</p>
<p>Un <em>outlier</em> en nuestro modelo de regresión será aquel que sea lejano a su valor predicho. Para ver esto gráficamente vamos considerar nuestro ejemplo de regresión lineal simple visto en la sección de predicción (<span class="math inline">\(\log(ing\_cor) = 10.265703 + 0.016142\times \sqrt{vestido}\)</span>)</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> mutated_data_income <span class="sc">%&gt;%</span> <span class="fu">filter</span>(vestido<span class="sc">&lt;</span><span class="dv">7500</span> <span class="sc">&amp;</span> vestido<span class="sc">&gt;</span><span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb134-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">sqrt</span>(vestido) , <span class="at">y =</span> <span class="fu">log</span>(ing_cor))) <span class="sc">+</span></span>
<span id="cb134-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="co">#Hasta aquí los datos</span></span>
<span id="cb134-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="fl">10.265703</span>, <span class="at">slope =</span> <span class="fl">0.016142</span>, <span class="at">color =</span> <span class="st">&quot;red3&quot;</span>) <span class="sc">+</span> </span>
<span id="cb134-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fl">35.440514</span>, <span class="at">y =</span> <span class="fl">7.535708</span>), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span> </span>
<span id="cb134-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lims</span>(<span class="at">y =</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">14</span>)) <span class="sc">+</span></span>
<span id="cb134-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-7" aria-hidden="true" tabindex="-1"></a>  general_theme </span>
<span id="cb134-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-8" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> mutated_data_income <span class="sc">%&gt;%</span> <span class="fu">filter</span>(vestido<span class="sc">&lt;</span><span class="dv">7500</span> <span class="sc">&amp;</span> vestido<span class="sc">&gt;</span><span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb134-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(vestido<span class="sc">!=</span><span class="fl">1256.03</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb134-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">sqrt</span>(vestido) , <span class="at">y =</span> <span class="fu">log</span>(ing_cor))) <span class="sc">+</span></span>
<span id="cb134-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span> </span>
<span id="cb134-12"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lims</span>(<span class="at">y =</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">14</span>)) <span class="sc">+</span> </span>
<span id="cb134-13"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="co">#Hasta aquí los datos</span></span>
<span id="cb134-14"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="fl">10.267257</span>, <span class="at">slope =</span> <span class="fl">0.016167</span>, <span class="at">color =</span> <span class="st">&quot;red3&quot;</span>) <span class="sc">+</span> </span>
<span id="cb134-15"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-15" aria-hidden="true" tabindex="-1"></a>  general_theme</span>
<span id="cb134-16"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb134-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_grid</span>(m1, m2, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">align =</span> <span class="st">&quot;h&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-100-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Le punto rojo es nuestro outlier en nuestro modelo inicial y en la misma subgráfica se aprecia la linea de regresión que se ajusta a tales datos. La segunda gráfica nos muestra la linea de regresión que resultad al omitir dicha observación. Como bien se ve, no se afecta realmente el comportamiento de la linea de regresión, pero veamos como cambiaron en otro aspecto los modelos</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-1" aria-hidden="true" tabindex="-1"></a>data_vestido <span class="ot">&lt;-</span> mutated_data_income <span class="sc">%&gt;%</span> <span class="fu">filter</span>(vestido<span class="sc">&lt;</span><span class="dv">7500</span> <span class="sc">&amp;</span> vestido<span class="sc">&gt;</span><span class="dv">0</span>)</span>
<span id="cb135-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> data_vestido <span class="sc">%&gt;%</span> <span class="fu">lm</span>(<span class="fu">log</span>(ing_cor)<span class="sc">~</span><span class="fu">sqrt</span>(vestido), <span class="at">data =</span> .) </span>
<span id="cb135-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-3" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> data_vestido <span class="sc">%&gt;%</span> </span>
<span id="cb135-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(vestido<span class="sc">!=</span><span class="fl">1256.03</span>) <span class="sc">%&gt;%</span> <span class="fu">lm</span>(<span class="fu">log</span>(ing_cor)<span class="sc">~</span><span class="fu">sqrt</span>(vestido), <span class="at">data =</span> .)</span>
<span id="cb135-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-5" aria-hidden="true" tabindex="-1"></a>l1 <span class="ot">&lt;-</span> m1 <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span>
<span id="cb135-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-6" aria-hidden="true" tabindex="-1"></a>l2 <span class="ot">&lt;-</span> m2 <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span>
<span id="cb135-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste0</span>(<span class="st">&quot;RSE del Modelo con outlier: &quot;</span>,l1<span class="sc">$</span>r.squared, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>RSE del Modelo con outlier: 0.154956818679954</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste0</span>(<span class="st">&quot;RSE del Modelo sin outlier: &quot;</span>,l2<span class="sc">$</span>r.squared))</span></code></pre></div>
<pre><code>RSE del Modelo sin outlier: 0.15805697613396</code></pre>
<p>Si bien el impacto en este caso no se ve tan drástico, este lo puede ser en otros modelos y hay que recordar que el RSE es utilizado para calcular los intervalos de confianza y los <span class="math inline">\(p-values\)</span>, por lo que un cambio tan fuerte para un sólo punto puede tener graves problemas de interpretación.</p>
<p>Para identificar outliers, podemos visualizar dos gráficas: residuales vs valores ajustados y residuales estudentizados vs valores ajustados</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-1" aria-hidden="true" tabindex="-1"></a>res_1 <span class="ot">&lt;-</span> (<span class="fu">tibble</span>(<span class="at">x =</span> m1<span class="sc">$</span>fitted.values, <span class="at">y =</span> <span class="fu">residuals</span>(m1)) <span class="sc">%&gt;%</span> </span>
<span id="cb139-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb139-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-3" aria-hidden="true" tabindex="-1"></a>    general_theme <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Valores ajustados&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuales&quot;</span>)) </span>
<span id="cb139-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-4" aria-hidden="true" tabindex="-1"></a>res_2 <span class="ot">&lt;-</span> (<span class="fu">tibble</span>(<span class="at">x =</span> m1<span class="sc">$</span>fitted.values, <span class="at">y =</span> <span class="fu">rstudent</span>(m1)) <span class="sc">%&gt;%</span> </span>
<span id="cb139-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb139-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-6" aria-hidden="true" tabindex="-1"></a>     <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="at">alpha =</span> <span class="fl">0.5</span>)<span class="sc">+</span> </span>
<span id="cb139-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-7" aria-hidden="true" tabindex="-1"></a>   general_theme <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Valores ajustados&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuales Estudentizados&quot;</span>))</span>
<span id="cb139-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb139-8" aria-hidden="true" tabindex="-1"></a>res_1 <span class="sc">+</span> res_2</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-102-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Aveces es difícil determinar si una observación debe ser considerado como outlier con la primera gráfica, pero en la segunda es más sencillo. De acuerdo al libro <span class="citation"><a href="#ref-james2013introduction" role="doc-biblioref">James et al.</a> (<a href="#ref-james2013introduction" role="doc-biblioref">2013</a>)</span>, observaciones con residuales estudentizados que son más grandes que 3, en valor absoluto, pueden ser considerados como posibles outliers. Lo cual tiene sentido, ya que al “normalizar” los residuales así, se esta haciendo que estos sigan una distribución <span class="math inline">\(t-student\)</span> con <span class="math inline">\(n-k-1\)</span> grados de libertad, en nuestro caso <span class="math inline">\(df = 1398-1-1 = 1396\)</span></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(latex2exp)</span>
<span id="cb140-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb140-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb140-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span> <span class="fu">stat_function</span>(<span class="at">fun =</span> <span class="sc">~</span><span class="fu">dt</span>(.x, <span class="dv">1396</span>)) <span class="sc">+</span></span>
<span id="cb140-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb140-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Densidad&quot;</span>) <span class="sc">+</span> </span>
<span id="cb140-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb140-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;t-student con 1396 grados de libertad&quot;</span>) <span class="sc">+</span> </span>
<span id="cb140-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb140-6" aria-hidden="true" tabindex="-1"></a>  general_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-103-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Entonces, de acuerdo a lo anterior tenemos, en este caso, 9 outliers.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> data_vestido<span class="sc">$</span>vestido , <span class="at">y =</span> data_vestido<span class="sc">$</span>ing_cor, </span>
<span id="cb141-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb141-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">fitted_values =</span> m1<span class="sc">$</span>fitted.values, <span class="at">rStudent =</span> <span class="fu">rstudent</span>(m1)) <span class="sc">%&gt;%</span> </span>
<span id="cb141-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb141-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">index =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> <span class="fu">filter</span>(rStudent<span class="sc">&lt;</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">|</span> rStudent<span class="sc">&gt;</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code># A tibble: 9 × 5
      x       y fitted_values rStudent index
  &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;
1 2015. 580698.          11.0     3.69     2
2 1174. 364980.          10.8     3.21   333
3  822. 376175.          10.7     3.41   359
4 1272. 352488.          10.8     3.12   490
5 1256.   1874.          10.8    -5.37   759
6  235. 608965.          10.5     4.55   777
7  636. 479572.          10.7     3.89  1090
8 5350.  12138.          11.4    -3.30  1130
9  959.   4431.          10.8    -3.83  1318</code></pre>
<p>Veamos que sucede con nuestro último modelo</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-1" aria-hidden="true" tabindex="-1"></a>filter_data_ToOutliers <span class="ot">&lt;-</span> filter_data <span class="sc">%&gt;%</span> </span>
<span id="cb143-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">fitted_values =</span> third_model_BX<span class="sc">$</span>fitted.values, </span>
<span id="cb143-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">rStandar =</span> <span class="fu">residuals</span>(third_model_BX),</span>
<span id="cb143-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">rStudent =</span> <span class="fu">rstudent</span>(third_model_BX),</span>
<span id="cb143-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">fitted_values =</span> third_model_BX<span class="sc">$</span>fitted.values)</span>
<span id="cb143-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-6" aria-hidden="true" tabindex="-1"></a>r_third_model1 <span class="ot">&lt;-</span> filter_data_ToOutliers <span class="sc">%&gt;%</span> </span>
<span id="cb143-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> rStandar, <span class="at">x =</span> fitted_values)) <span class="sc">+</span> </span>
<span id="cb143-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Valores ajustados&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuales&quot;</span>) <span class="sc">+</span> </span>
<span id="cb143-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> general_theme</span>
<span id="cb143-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-11" aria-hidden="true" tabindex="-1"></a>r_third_model2 <span class="ot">&lt;-</span>filter_data_ToOutliers <span class="sc">%&gt;%</span> </span>
<span id="cb143-12"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> rStudent, <span class="at">x =</span> fitted_values)) <span class="sc">+</span> </span>
<span id="cb143-13"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb143-14"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Valores ajustados&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuales estudentizados&quot;</span>) <span class="sc">+</span></span>
<span id="cb143-15"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> general_theme</span>
<span id="cb143-16"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-17"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb143-17" aria-hidden="true" tabindex="-1"></a>r_third_model1 <span class="sc">+</span> r_third_model2</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-105-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Es decir, tenemos 20 outliers</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb144-1" aria-hidden="true" tabindex="-1"></a>filter_data_ToOutliers <span class="sc">%&gt;%</span> <span class="fu">filter</span>(rStudent<span class="sc">&lt;</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">|</span> rStudent<span class="sc">&gt;</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code># A tibble: 20 × 8
   ing_cor transporte alimentos limpieza personales fitted_values rStandar
     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;
 1 580698.     12326.    37594.   3318.      8520            13.3     2.12
 2 114342.       579.     4706.   1131.       805.           11.2     2.10
 3   3424.      4090.     8318.   1272.      1643.           12.0    -3.06
 4 278952.     10627.    10749.    116.      1050.           11.9     2.60
 5 439078.     24949.     9334.  17321.      5266.           13.1     1.99
 6   3522.      2765.     7399.    435.       822.           11.6    -2.61
 7 364980.      8725.     8421.  25218.      5336.           12.8     1.99
 8 654367.     31987.    10202.   9120.      7542.           13.1     2.47
 9 109957.      2483.    12722.    131.        26.1          10.9     2.34
10   8951.      2474.    17055.   2337.      2117.           12.2    -2.14
11   1874.       600      6313.   1900.      1322.           11.4    -3.23
12 608965.      2087.    15126.   1745.      3591.           12.2     3.32
13   9748.      8700     10067.   1503.      1943.           12.3    -2.09
14 144248.       600      8601.    618       1437            11.4     2.23
15 150376.      3484.     5484.     71.7      340.           11.1     2.53
16 479572.     16619.    20237.   2093.       615.           12.5     2.63
17  12138.      8890.     8916.   2081.      4420.           12.4    -1.98
18   1941.     15184.     9720.   1008.      7227.           12.6    -4.34
19  10715.     30803.    13526.  10147.       746.           12.8    -2.51
20   4431.      3267.    14908.   1304.      1710.           12.1    -2.89
# … with 1 more variable: rStudent &lt;dbl&gt;</code></pre>
<p>Si dicho outlier fue generado por un error de recolección de información, podemos eliminar la observación sin problemas, aunque hay que considerar que si no es el caso, entonces nuestro modelo simplemente no es lo suficientemente bueno para predecir valores que pueden suceder. En este caso vamos a eliminarlos considerando un limite en 3.5.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-1" aria-hidden="true" tabindex="-1"></a>filter_data_ToOutliers <span class="sc">%&gt;%</span> </span>
<span id="cb146-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> rStudent, <span class="at">x =</span> fitted_values)) <span class="sc">+</span> </span>
<span id="cb146-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb146-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Valores ajustados&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuales estudentizados&quot;</span>) <span class="sc">+</span></span>
<span id="cb146-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#447e9e&quot;</span>) <span class="sc">+</span> </span>
<span id="cb146-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>( </span>
<span id="cb146-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> filter_data_ToOutliers <span class="sc">%&gt;%</span> </span>
<span id="cb146-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">index =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb146-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-9" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">filter</span>(rStudent<span class="sc">&lt;</span> <span class="sc">-</span><span class="fl">3.5</span> <span class="sc">|</span> rStudent<span class="sc">&gt;</span><span class="fl">3.5</span>),</span>
<span id="cb146-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label=</span>index), <span class="at">nudge_x =</span> <span class="fl">0.2</span></span>
<span id="cb146-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb146-11" aria-hidden="true" tabindex="-1"></a>  )<span class="sc">+</span> general_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-107-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb147-1" aria-hidden="true" tabindex="-1"></a>fourth_model_1 <span class="ot">&lt;-</span> filter_data_ToOutliers <span class="sc">%&gt;%</span> <span class="fu">filter</span>(rStudent<span class="sc">&gt;=-</span><span class="fl">3.5</span> <span class="sc">&amp;</span> rStudent<span class="sc">&lt;=</span><span class="fl">3.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb147-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb147-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(((ing_cor<span class="sc">^</span>lambdaBCox_tM<span class="dv">-1</span>)<span class="sc">/</span>lambdaBCox_tM) <span class="sc">~</span> </span>
<span id="cb147-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb147-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">log</span>(transporte) <span class="sc">+</span> <span class="fu">sqrt</span>(alimentos) <span class="sc">+</span> <span class="fu">log</span>(limpieza) <span class="sc">+</span> <span class="fu">log</span>(personales), </span>
<span id="cb147-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb147-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">data =</span> .)</span>
<span id="cb147-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb147-5" aria-hidden="true" tabindex="-1"></a>fourth_model_1 <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>
Call:
lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + 
    sqrt(alimentos) + log(limpieza) + log(personales), data = .)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.1395 -0.4038 -0.0162  0.3866  2.2298 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     6.8496905  0.1292413  52.999   &lt;2e-16 ***
log(transporte) 0.2514986  0.0134651  18.678   &lt;2e-16 ***
sqrt(alimentos) 0.0057975  0.0004841  11.975   &lt;2e-16 ***
log(limpieza)   0.1593438  0.0171812   9.274   &lt;2e-16 ***
log(personales) 0.1849702  0.0177541  10.418   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.6049 on 2127 degrees of freedom
Multiple R-squared:  0.5193,    Adjusted R-squared:  0.5184 
F-statistic: 574.5 on 4 and 2127 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb149-1" aria-hidden="true" tabindex="-1"></a>fourth_model_1 <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>() <span class="sc">+</span> general_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-109-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-1" aria-hidden="true" tabindex="-1"></a>Comparison_models <span class="ot">&lt;-</span> third_model_BX <span class="sc">%&gt;%</span> <span class="fu">glance</span>() <span class="sc">%&gt;%</span>  </span>
<span id="cb150-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="st">&quot;Estadística&quot;</span>, <span class="st">&quot;Tercel Modelo&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb150-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.numeric, round, <span class="at">digits =</span> <span class="dv">8</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb150-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(</span>
<span id="cb150-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-5" aria-hidden="true" tabindex="-1"></a>    fourth_model_1 <span class="sc">%&gt;%</span> <span class="fu">glance</span>() <span class="sc">%&gt;%</span>  <span class="fu">gather</span>(<span class="st">&quot;Estadística&quot;</span>, <span class="st">&quot;Cuarto Modelo_1&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb150-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-6" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.numeric, round, <span class="at">digits =</span> <span class="dv">8</span>),</span>
<span id="cb150-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">by =</span> <span class="st">&quot;Estadística&quot;</span></span>
<span id="cb150-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb150-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb150-9" aria-hidden="true" tabindex="-1"></a>Comparison_models</span></code></pre></div>
<pre><code># A tibble: 12 × 3
   Estadística   `Tercel Modelo` `Cuarto Modelo_1`
   &lt;chr&gt;                   &lt;dbl&gt;             &lt;dbl&gt;
 1 r.squared               0.489             0.519
 2 adj.r.squared           0.488             0.518
 3 sigma                   0.642             0.605
 4 statistic             511.              575.   
 5 p.value                 0                 0    
 6 df                      4                 4    
 7 logLik              -2089.            -1951.   
 8 AIC                  4190.             3914.   
 9 BIC                  4224.             3948.   
10 deviance              881.              778.   
11 df.residual          2139              2127    
12 nobs                 2144              2132    </code></pre>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-1" aria-hidden="true" tabindex="-1"></a>tests_linearM <span class="ot">&lt;-</span> <span class="cf">function</span>(model){</span>
<span id="cb152-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-2" aria-hidden="true" tabindex="-1"></a>  shapiro <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">residuals</span>() <span class="sc">%&gt;%</span> <span class="fu">shapiro.test</span>()</span>
<span id="cb152-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-3" aria-hidden="true" tabindex="-1"></a>  breusch_pagan <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">bptest</span>()</span>
<span id="cb152-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-4" aria-hidden="true" tabindex="-1"></a>  durbin_watson <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">dwtest</span>()</span>
<span id="cb152-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-5" aria-hidden="true" tabindex="-1"></a>  breusch_godfrey <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">bgtest</span>()</span>
<span id="cb152-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-6" aria-hidden="true" tabindex="-1"></a>  p_values <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="st">&quot;Estadística&quot;</span> <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Shapiro-Wils&quot;</span>, <span class="st">&quot;Breusch-Pagan&quot;</span>,</span>
<span id="cb152-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-7" aria-hidden="true" tabindex="-1"></a>                                       <span class="st">&quot;Durbin-Watson&quot;</span>, <span class="st">&quot;Breusch_Godfrey&quot;</span>), </span>
<span id="cb152-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-8" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;P-value&quot;</span> <span class="ot">=</span> <span class="fu">c</span>(shapiro<span class="sc">$</span>p.value, breusch_pagan<span class="sc">$</span>p.value,</span>
<span id="cb152-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-9" aria-hidden="true" tabindex="-1"></a>                                   durbin_watson<span class="sc">$</span>p.value, breusch_godfrey<span class="sc">$</span>p.value)) <span class="sc">%&gt;%</span> </span>
<span id="cb152-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-10" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.numeric, round, <span class="at">digits =</span> <span class="dv">8</span>)</span>
<span id="cb152-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="st">&quot;p-values&quot;</span> <span class="ot">=</span> p_values, <span class="st">&quot;vif&quot;</span> <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">vif</span>()))</span>
<span id="cb152-12"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb152-13"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb152-13" aria-hidden="true" tabindex="-1"></a>fourth_model_1 <span class="sc">%&gt;%</span> <span class="fu">tests_linearM</span>()</span></code></pre></div>
<pre><code>$`p-values`
# A tibble: 4 × 2
  Estadística      `P-value`
  &lt;chr&gt;                &lt;dbl&gt;
1 Shapiro-Wils    0.0152    
2 Breusch-Pagan   0.137     
3 Durbin-Watson   0.00000008
4 Breusch_Godfrey 0.00000019

$vif
log(transporte) sqrt(alimentos)   log(limpieza) log(personales) 
       1.412725        1.476318        1.559839        1.655387 </code></pre>
<p>Véase que seguimos “manteniendo” el supuesto de homocedasticidad y obtuvimos un mejor p-value para la prueba de normalidad aunque no mejoramos nuestro modelo para las pruebas de independencia, además de que no tenemos problemas de multicolinearidad.</p>
<p>¿Que hubiera sucedido si hubiéramos aplicado estos filtros a los datos del segundo modelo? ¿Hubiéramos conseguido un mejor modelo?</p>
</div>
<div id="valores-con-alto-apalancamiento" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Valores con alto “apalancamiento”</h3>
<p>Mientras que en los outliers nos enfocamos a los valores inusuales de la variable respuesta para un <span class="math inline">\(x_i\)</span>, una observación con alto apalancamiento (<strong><em>leverage</em></strong>) tiene un valor inusual para <span class="math inline">\(x_i\)</span>. El <em>leverage</em> de una observación es una medida de la distancia entre los valores de sus variables explicativas y el promedio de los valores de las variables explicativas en todo el conjunto de datos. Observaciones con alto apalancamiento pueden tener una alta influencia en el modelo.</p>
<p>Cuando sólo tenemos una variable explicativa en nuestro modelo, la estadística de apalancamiento queda definida de la siguiente manera:</p>
<p><span class="math display">\[
h_i = \frac{1}{(n-1)}\left[\frac{x_i-\bar{x}}{s_x}\right]^2 + \frac{1}{n} = \frac{(x_i-\bar{x})^2}{\sum_{i^{&#39;} = 1}^n (x^{&#39;}-\bar{x})^2} + \frac{1}{n}
\]</span></p>
<p>En el caso de regresión lineal simple es sencillo detectar este tipo de observaciones, ya que su valor predicho estará fuera del rango normal de las demás predicciones.</p>
<p>Con la ecuación anterior podemos ver que <span class="math inline">\(h_i\)</span> incrementa con la distancia entre <span class="math inline">\(x_i\)</span> y <span class="math inline">\(\bar{x}\)</span>. Esta estadística siempre estará entre <span class="math inline">\(1/n\)</span> y 1 y el leverage promedio para todas las observaciones será igual a <span class="math inline">\((p+1)/n\)</span> con <span class="math inline">\(p\)</span> el número de coeficientes, por lo que si una observación tiene un leverage más grande que <span class="math inline">\((p+1)/n\)</span> podríamos decir que se tiene un alto apalancamiento aunque algunos estadísticos prefieren utilizar como umbral <span class="math inline">\(2*p/n\)</span>.</p>
<p>Ya habíamos retirado algunos outliers, vamos a ver si estas observaciones también son tienen un alto leverage. En R, podemos utilizar la función <code>stats::hatvalues()</code></p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-1" aria-hidden="true" tabindex="-1"></a>filter_data_ToLeverage <span class="ot">&lt;-</span> filter_data_ToOutliers <span class="sc">%&gt;%</span> </span>
<span id="cb154-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">leverage =</span> third_model_BX <span class="sc">%&gt;%</span> <span class="fu">hatvalues</span>(),<span class="at">index =</span> <span class="fu">row_number</span>())</span>
<span id="cb154-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-4" aria-hidden="true" tabindex="-1"></a>(Leverage_analysis <span class="ot">&lt;-</span> filter_data_ToLeverage <span class="sc">%&gt;%</span> </span>
<span id="cb154-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> leverage)) <span class="sc">+</span> </span>
<span id="cb154-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#447e9e&quot;</span>) <span class="sc">+</span> </span>
<span id="cb154-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> (<span class="dv">5</span><span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>third_model_BX<span class="sc">$</span>fitted.values <span class="sc">%&gt;%</span> <span class="fu">length</span>(), </span>
<span id="cb154-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;blue3&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb154-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">2</span><span class="sc">*</span><span class="dv">5</span><span class="sc">/</span>third_model_BX<span class="sc">$</span>fitted.values <span class="sc">%&gt;%</span> <span class="fu">length</span>(), </span>
<span id="cb154-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;red3&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb154-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.016</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb154-12"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>( </span>
<span id="cb154-13"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> filter_data_ToLeverage <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(leverage <span class="sc">&gt;=</span><span class="fl">0.016</span>), </span>
<span id="cb154-14"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label=</span>index), <span class="at">nudge_x =</span> <span class="dv">100</span></span>
<span id="cb154-15"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb154-15" aria-hidden="true" tabindex="-1"></a>  )<span class="sc">+</span> general_theme)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-112-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Las lineas de colores tan sólo son distintos umbrales o cortes para establecer que observación tiene un alto apalancamiento. En este caso no tenemos que algún outlier también tiene un alto apalancamiento, lo cual es lo ideal.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-1" aria-hidden="true" tabindex="-1"></a>fourth_model_2 <span class="ot">&lt;-</span> filter_data_ToLeverage <span class="sc">%&gt;%</span> </span>
<span id="cb155-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(rStudent<span class="sc">&gt;=-</span><span class="fl">3.5</span> <span class="sc">&amp;</span> rStudent<span class="sc">&lt;=</span><span class="fl">3.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb155-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(leverage<span class="sc">&lt;</span> <span class="fl">0.016</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb155-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(((ing_cor<span class="sc">^</span>lambdaBCox_tM<span class="dv">-1</span>)<span class="sc">/</span>lambdaBCox_tM) <span class="sc">~</span> </span>
<span id="cb155-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-5" aria-hidden="true" tabindex="-1"></a>       <span class="fu">log</span>(transporte) <span class="sc">+</span> <span class="fu">sqrt</span>(alimentos) <span class="sc">+</span> <span class="fu">log</span>(limpieza) <span class="sc">+</span> <span class="fu">log</span>(personales), </span>
<span id="cb155-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">data =</span> .)</span>
<span id="cb155-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb155-7" aria-hidden="true" tabindex="-1"></a>fourth_model_2 <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>
Call:
lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + 
    sqrt(alimentos) + log(limpieza) + log(personales), data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.14019 -0.40307 -0.01616  0.38674  2.22978 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     6.8470768  0.1293183  52.947   &lt;2e-16 ***
log(transporte) 0.2510460  0.0134731  18.633   &lt;2e-16 ***
sqrt(alimentos) 0.0058011  0.0004842  11.981   &lt;2e-16 ***
log(limpieza)   0.1602029  0.0174279   9.192   &lt;2e-16 ***
log(personales) 0.1849316  0.0179337  10.312   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.6049 on 2125 degrees of freedom
Multiple R-squared:  0.5196,    Adjusted R-squared:  0.5187 
F-statistic: 574.6 on 4 and 2125 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb157-1" aria-hidden="true" tabindex="-1"></a>fourth_model_2 <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>() <span class="sc">+</span> general_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-114-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-1" aria-hidden="true" tabindex="-1"></a>Comparison_models <span class="ot">&lt;-</span> Comparison_models <span class="sc">%&gt;%</span> </span>
<span id="cb158-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(</span>
<span id="cb158-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-3" aria-hidden="true" tabindex="-1"></a>    fourth_model_2 <span class="sc">%&gt;%</span> <span class="fu">glance</span>() <span class="sc">%&gt;%</span>  <span class="fu">gather</span>(<span class="st">&quot;Estadística&quot;</span>, <span class="st">&quot;Cuarto Modelo_2&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb158-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-4" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.numeric, round, <span class="at">digits =</span> <span class="dv">8</span>),</span>
<span id="cb158-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">by =</span> <span class="st">&quot;Estadística&quot;</span></span>
<span id="cb158-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb158-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb158-7" aria-hidden="true" tabindex="-1"></a>Comparison_models</span></code></pre></div>
<pre><code># A tibble: 12 × 4
   Estadística   `Tercel Modelo` `Cuarto Modelo_1` `Cuarto Modelo_2`
   &lt;chr&gt;                   &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;
 1 r.squared               0.489             0.519             0.520
 2 adj.r.squared           0.488             0.518             0.519
 3 sigma                   0.642             0.605             0.605
 4 statistic             511.              575.              575.   
 5 p.value                 0                 0                 0    
 6 df                      4                 4                 4    
 7 logLik              -2089.            -1951.            -1949.   
 8 AIC                  4190.             3914.             3910.   
 9 BIC                  4224.             3948.             3944.   
10 deviance              881.              778.              778.   
11 df.residual          2139              2127              2125    
12 nobs                 2144              2132              2130    </code></pre>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb160-1" aria-hidden="true" tabindex="-1"></a>fourth_model_2 <span class="sc">%&gt;%</span> <span class="fu">tests_linearM</span>()</span></code></pre></div>
<pre><code>$`p-values`
# A tibble: 4 × 2
  Estadística      `P-value`
  &lt;chr&gt;                &lt;dbl&gt;
1 Shapiro-Wils    0.0148    
2 Breusch-Pagan   0.136     
3 Durbin-Watson   0.00000007
4 Breusch_Godfrey 0.00000017

$vif
log(transporte) sqrt(alimentos)   log(limpieza) log(personales) 
       1.413477        1.476237        1.583429        1.680891 </code></pre>
<p>Véase que al retirar estas observaciones el modelo no mejoró aunque no empeoro drásticamente.</p>
<p>Otra estadística para determinar la influencia que tiene una observación en el modelo, es la estadística de Cook, la cual mide la influencia general de la observación, es decir que muestra el efecto que tiene omitir tal observación en el modelo de regresión.</p>
<p><span class="math display">\[
D_i = \sum_{j = 1}^n\frac{(\hat{y}_{j(i)}-\hat{y}_j)^2}{p\hat{\sigma}^2}
\]</span></p>
<p>donde <span class="math inline">\(\hat{y}_j\)</span> es el <span class="math inline">\(j-ésimo\)</span> valor en el ajuste usando todos las observaciones y <span class="math inline">\(\hat{y}_{j(i)}\)</span> es el valor de la regresión omitiendo la observación <span class="math inline">\(i\)</span>; <span class="math inline">\(p\)</span> es el número de coeficientes (se incluye <span class="math inline">\(\beta_0\)</span>) y <span class="math inline">\(\sigma\)</span> la varianza estimada desde el modelo. El calculo de esta estadística es costoso ya que se tienen que hacer diferentes modelos de regresión, por lo que se prefiere utilizar una equivalencia</p>
<p><span class="math display">\[
D_i = \frac{1}{p}(r_i)^2\left(\frac{h_i}{1-h_i}\right)
\]</span></p>
<p>donde <span class="math inline">\(r_i\)</span> es el residual estandarizado aunque algunos autores prefieren los residuales estudentizados.</p>
<p>El umbral que se establezca también dependerá de la información aunque algunos autores sugieren valores cercanos o más grandes que 1 son valores con gran influencia</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-1" aria-hidden="true" tabindex="-1"></a>filter_data_ToCook <span class="ot">&lt;-</span> filter_data_ToLeverage <span class="sc">%&gt;%</span> </span>
<span id="cb162-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Cook =</span> third_model_BX <span class="sc">%&gt;%</span> <span class="fu">cooks.distance</span>())</span>
<span id="cb162-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-4" aria-hidden="true" tabindex="-1"></a>(Cook_analysis <span class="ot">&lt;-</span> filter_data_ToCook <span class="sc">%&gt;%</span> </span>
<span id="cb162-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> Cook)) <span class="sc">+</span> </span>
<span id="cb162-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#447e9e&quot;</span>) <span class="sc">+</span> </span>
<span id="cb162-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.03</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb162-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>( </span>
<span id="cb162-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> filter_data_ToCook <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(Cook <span class="sc">&gt;=</span> <span class="fl">0.03</span>), </span>
<span id="cb162-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label=</span>index), <span class="at">nudge_x =</span> <span class="dv">100</span></span>
<span id="cb162-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb162-11" aria-hidden="true" tabindex="-1"></a>  )<span class="sc">+</span> general_theme)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-117-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Es preferible estudiar los valores influyentes y outliers en conjunto, para determinar aquellos valores que tengan más problemas en conjunto</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-1" aria-hidden="true" tabindex="-1"></a>without_axis_x <span class="ot">&lt;-</span>  <span class="fu">theme</span>(<span class="at">axis.ticks.x =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb163-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">axis.text.x =</span> <span class="fu">element_blank</span>())</span>
<span id="cb163-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-4" aria-hidden="true" tabindex="-1"></a>Cook_analysis <span class="ot">&lt;-</span> filter_data_ToCook <span class="sc">%&gt;%</span> </span>
<span id="cb163-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> Cook)) <span class="sc">+</span> </span>
<span id="cb163-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#447e9e&quot;</span>) <span class="sc">+</span> </span>
<span id="cb163-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.02</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb163-8"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>( </span>
<span id="cb163-9"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> filter_data_ToCook <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(Cook <span class="sc">&gt;=</span> <span class="fl">0.02</span>), </span>
<span id="cb163-10"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label=</span>index), <span class="at">nudge_x =</span> <span class="dv">100</span>, <span class="at">size =</span> <span class="dv">3</span></span>
<span id="cb163-11"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-11" aria-hidden="true" tabindex="-1"></a>  )<span class="sc">+</span> general_theme <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>)) <span class="sc">+</span> </span>
<span id="cb163-12"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span> <span class="cn">NULL</span>) <span class="sc">+</span> without_axis_x</span>
<span id="cb163-13"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-14"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-14" aria-hidden="true" tabindex="-1"></a>Leverage_analysis <span class="ot">&lt;-</span> filter_data_ToLeverage <span class="sc">%&gt;%</span> </span>
<span id="cb163-15"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> leverage)) <span class="sc">+</span> </span>
<span id="cb163-16"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#447e9e&quot;</span>) <span class="sc">+</span> </span>
<span id="cb163-17"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.014</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb163-18"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>( </span>
<span id="cb163-19"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> filter_data_ToLeverage <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(leverage <span class="sc">&gt;=</span> <span class="fl">0.014</span>), </span>
<span id="cb163-20"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label=</span>index), <span class="at">nudge_x =</span> <span class="dv">100</span>, <span class="at">size =</span> <span class="dv">3</span></span>
<span id="cb163-21"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-21" aria-hidden="true" tabindex="-1"></a>  )<span class="sc">+</span> general_theme <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x=</span> <span class="cn">NULL</span>) <span class="sc">+</span> without_axis_x</span>
<span id="cb163-22"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-23"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-23" aria-hidden="true" tabindex="-1"></a>ResidualSt_analysis <span class="ot">&lt;-</span> filter_data_ToCook <span class="sc">%&gt;%</span> </span>
<span id="cb163-24"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> rStudent)) <span class="sc">+</span> </span>
<span id="cb163-25"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#447e9e&quot;</span>) <span class="sc">+</span> </span>
<span id="cb163-26"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-26" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb163-27"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Index&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuales</span><span class="sc">\n</span><span class="st">estudentizados&quot;</span>) <span class="sc">+</span></span>
<span id="cb163-28"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>( </span>
<span id="cb163-29"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> filter_data_ToOutliers <span class="sc">%&gt;%</span> </span>
<span id="cb163-30"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-30" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">index =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb163-31"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-31" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">filter</span>(rStudent<span class="sc">&lt;</span> <span class="sc">-</span><span class="fl">3.5</span> <span class="sc">|</span> rStudent<span class="sc">&gt;</span><span class="fl">3.5</span>), <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb163-32"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label=</span>index), <span class="at">nudge_x =</span> <span class="dv">100</span>)<span class="sc">+</span> general_theme</span>
<span id="cb163-33"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-34"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-35"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-36"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb163-36" aria-hidden="true" tabindex="-1"></a>Cook_analysis <span class="sc">/</span> Leverage_analysis <span class="sc">/</span> ResidualSt_analysis</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-118-1.png" width="960" style="display: block; margin: auto;" /></p>
<!-- <embed src="InfluencialPoints.pdf" width="800px" height="2100px" /> -->
<!-- <iframe src="InfluencialPoints.pdf" width="100%" height="500px"></iframe> -->
<!-- ![](InflentianPoints.png) -->
<p>Finalmente, vamos a retirar aquellas observaciones que causan mayores problemas y veamos como se comporta nuestro modelo</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb164-1" aria-hidden="true" tabindex="-1"></a>fourth_model_3 <span class="ot">&lt;-</span> filter_data_ToCook <span class="sc">%&gt;%</span> </span>
<span id="cb164-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb164-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>(index <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">881</span>, <span class="dv">190</span>, <span class="dv">1858</span>, <span class="dv">2023</span>, <span class="dv">703</span>, <span class="dv">515</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb164-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb164-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(((ing_cor<span class="sc">^</span>lambdaBCox_tM<span class="dv">-1</span>)<span class="sc">/</span>lambdaBCox_tM) <span class="sc">~</span> </span>
<span id="cb164-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb164-4" aria-hidden="true" tabindex="-1"></a>       <span class="fu">log</span>(transporte) <span class="sc">+</span> <span class="fu">sqrt</span>(alimentos) <span class="sc">+</span> <span class="fu">log</span>(limpieza) <span class="sc">+</span> <span class="fu">log</span>(personales), </span>
<span id="cb164-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb164-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">data =</span> .)</span>
<span id="cb164-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb164-6" aria-hidden="true" tabindex="-1"></a>fourth_model_3 <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>
Call:
lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + 
    sqrt(alimentos) + log(limpieza) + log(personales), data = .)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.2355 -0.4063 -0.0165  0.3863  3.3151 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     6.8390940  0.1334738  51.239   &lt;2e-16 ***
log(transporte) 0.2571418  0.0139177  18.476   &lt;2e-16 ***
sqrt(alimentos) 0.0058523  0.0005006  11.690   &lt;2e-16 ***
log(limpieza)   0.1543125  0.0178434   8.648   &lt;2e-16 ***
log(personales) 0.1843599  0.0184677   9.983   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.6263 on 2133 degrees of freedom
Multiple R-squared:  0.5043,    Adjusted R-squared:  0.5034 
F-statistic: 542.5 on 4 and 2133 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb166-1" aria-hidden="true" tabindex="-1"></a>fourth_model_3 <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>() <span class="sc">+</span> general_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-120-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-1" aria-hidden="true" tabindex="-1"></a>Comparison_models <span class="ot">&lt;-</span> Comparison_models <span class="sc">%&gt;%</span> </span>
<span id="cb167-2"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(</span>
<span id="cb167-3"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-3" aria-hidden="true" tabindex="-1"></a>    fourth_model_3 <span class="sc">%&gt;%</span> <span class="fu">glance</span>() <span class="sc">%&gt;%</span>  <span class="fu">gather</span>(<span class="st">&quot;Estadística&quot;</span>, <span class="st">&quot;Cuarto Modelo_3&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb167-4"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-4" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.numeric, round, <span class="at">digits =</span> <span class="dv">8</span>),</span>
<span id="cb167-5"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">by =</span> <span class="st">&quot;Estadística&quot;</span></span>
<span id="cb167-6"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb167-7"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb167-7" aria-hidden="true" tabindex="-1"></a>Comparison_models</span></code></pre></div>
<pre><code># A tibble: 12 × 5
   Estadística   `Tercel Modelo` `Cuarto Modelo_1` `Cuarto Modelo_2`
   &lt;chr&gt;                   &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;
 1 r.squared               0.489             0.519             0.520
 2 adj.r.squared           0.488             0.518             0.519
 3 sigma                   0.642             0.605             0.605
 4 statistic             511.              575.              575.   
 5 p.value                 0                 0                 0    
 6 df                      4                 4                 4    
 7 logLik              -2089.            -1951.            -1949.   
 8 AIC                  4190.             3914.             3910.   
 9 BIC                  4224.             3948.             3944.   
10 deviance              881.              778.              778.   
11 df.residual          2139              2127              2125    
12 nobs                 2144              2132              2130    
# … with 1 more variable: Cuarto Modelo_3 &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="otros-puntos-importantes-de-regresión-lineal.html#cb169-1" aria-hidden="true" tabindex="-1"></a>fourth_model_3 <span class="sc">%&gt;%</span> <span class="fu">tests_linearM</span>()</span></code></pre></div>
<pre><code>$`p-values`
# A tibble: 4 × 2
  Estadística     `P-value`
  &lt;chr&gt;               &lt;dbl&gt;
1 Shapiro-Wils    0        
2 Breusch-Pagan   0.513    
3 Durbin-Watson   0.0000346
4 Breusch_Godfrey 0.0000818

$vif
log(transporte) sqrt(alimentos)   log(limpieza) log(personales) 
       1.412271        1.474882        1.573000        1.670096 </code></pre>
<p>Con nuestro último modelo ya omitimos valores influyentes, retiramos outliers, tampoco estamos considerando algunos ceros y aplicamos diversas transformaciones, tanto para la variable dependiente y las variables independientes. Con este modelo logramos un modelo con homocedasticidad, al estar considerando una cantidad importante de datos podemos omitir el supuesto de normalidad aunque ay que tener precaución con las predicciones que se lleguen a dar. Otro punto a considerar es que el supuesto de independencia no se logro mejorar y para solucionar dicho problema tendríamos que considerar otro tipo de modelo.</p>
<ul>
<li><a href="https://daviddalpiaz.github.io/appliedstats/model-diagnostics.html#unusual-observations">Unusual Observations</a></li>
<li><a href="https://fhernanb.github.io/libro_regresion/diag2.html#dffits">Modelos de Regresión con R: Diagnósticos parte II</a></li>
<li><a href="https://daviddalpiaz.github.io/appliedstats/model-diagnostics.html#influence">Applied Statistics with R: Chapter 13 Model Diagnostics</a></li>
<li><a href="https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression7.html">Regression Diagnostics</a></li>
<li><a href="https://www.aptech.com/resources/tutorials/econometrics/ols-diagnostics-influential-data-tests/">OLS diagnostics: Influential data tests</a></li>
</ul>
<p>Aquí se dejan más enlaces:</p>
<ul>
<li><a href="https://rc2e.com/linearregressionandanova">R Cookbook, 2nd Edition: 11 Linear Regression and ANOVA</a></li>
<li><a href="https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html#">Beyond Multiple Linear Regression: Chapter 1 Review of Multiple Linear Regression</a></li>
<li><a href="http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf">Math 261A - Spring 2012: Multiple Linear Regression</a></li>
<li><a href="https://thomaselove.github.io/431-notes/multiple-regression-introduction.html">Data Science for Biological, Medical and Health Research: Notes for PQHS/CRSP/MPHP 431: Chapter 29 Multiple Regression: Introduction</a></li>
</ul>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-james2013introduction" class="csl-entry">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regresión-lineal-múltiple.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="anova-vs-regresión-lineal.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "github", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

[["index.html", "Statistical Notes Prefacio", " Statistical Notes Carlos Fernando Vásquez Guerra Prefacio Versión provisional de un conjunto de notas personales sobre temas estadísticos. Objetivos Aquí se encontrarán todas las notas que se han publicado directamente en RPubs durante la enseñanza de temas estadísticos en la facultad de ciencias, UNAM. Estructura Pendiente. Detalles técnicos Para la creación de este material se hizo uso de varios sistemas de software como LaTeX y CSS para el diseño de ciertos elementos. Todos los cálculos y gráficas fue creado con el lenguaje de programación R ya sea con el uso del paquete base o algún otro de los paquetes que se mencionan a continuación. .scroll-300 { max-height: 300px; } pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } R version 3.6.2 (2019-12-12) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS 10.16 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] knitr_1.31 visdat_0.5.3 actuar_2.3-3 [4] futurevisions_0.1.1 moments_0.14 fitdistrplus_1.0-14 [7] npsurv_0.4-0 lsei_1.2-0 survival_3.1-8 [10] scales_1.1.1 janitor_2.1.0 shape_1.4.4 [13] kableExtra_1.2.1 ggbrace_0.1.0 MASS_7.3-51.5 [16] GGally_2.1.2 readxl_1.3.1 cowplot_1.1.1 [19] patchwork_1.0.0 broom_0.7.9 latex2exp_0.4.0 [22] devtools_2.3.2 usethis_2.0.1 forcats_0.5.1 [25] stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 [28] readr_1.4.0 tidyr_1.1.3 tibble_3.1.3 [31] ggplot2_3.3.5 tidyverse_1.3.0 loaded via a namespace (and not attached): [1] fs_1.5.0 lubridate_1.7.10 webshot_0.5.2 [4] RColorBrewer_1.1-2 httr_1.4.2 rprojroot_2.0.2 [7] repr_1.1.0 tools_3.6.2 backports_1.2.1 [10] bslib_0.2.4.9002 utf8_1.2.2 R6_2.5.0 [13] DBI_1.1.0 colorspace_2.0-2 withr_2.4.2 [16] tidyselect_1.1.0 prettyunits_1.1.1 processx_3.4.5 [19] compiler_3.6.2 cli_3.0.1 rvest_1.0.1 [22] xml2_1.3.2 desc_1.2.0 bookdown_0.21.6 [25] sass_0.3.1.9000 callr_3.5.1 digest_0.6.27 [28] rmarkdown_2.7.3 base64enc_0.1-3 pkgconfig_2.0.3 [31] htmltools_0.5.1.9000 sessioninfo_1.1.1 dbplyr_1.4.2 [34] rlang_0.4.11 rstudioapi_0.13 jquerylib_0.1.3 [37] generics_0.1.0 jsonlite_1.7.2 magrittr_2.0.1 [40] Matrix_1.2-18 Rcpp_1.0.6 munsell_0.5.0 [43] fansi_0.5.0 lifecycle_1.0.0 stringi_1.5.3 [46] yaml_2.2.1 snakecase_0.11.0 expint_0.1-6 [49] pkgbuild_1.2.0 plyr_1.8.6 grid_3.6.2 [52] crayon_1.4.1 lattice_0.20-40 splines_3.6.2 [55] haven_2.3.1 hms_1.0.0 ps_1.6.0 [58] pillar_1.6.2 pkgload_1.2.0 reprex_0.3.0 [61] glue_1.4.2 evaluate_0.14 remotes_2.2.0 [64] modelr_0.1.6 vctrs_0.3.8 testthat_3.0.2 [67] cellranger_1.1.0 gtable_0.3.0 reshape_0.8.8 [70] assertthat_0.2.1 xfun_0.21 skimr_2.1.3 [73] viridisLite_0.4.0 memoise_1.1.0 ellipsis_0.3.2 Este libro fue escrito con bookdown usando RStudio. Esta versión fue escrita con: Finding R package dependencies ... Done! setting value version R version 3.6.2 (2019-12-12) os macOS 10.16 system x86_64, darwin15.6.0 ui X11 language (EN) collate en_US.UTF-8 ctype en_US.UTF-8 tz America/Mexico_City date 2021-08-15 Licencia This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["pruebas-de-hipótesis.html", "Capítulo 1 Pruebas de hipótesis 1.1 ¿Qué es una Prueba de Hipótesis? 1.2 Dos tipos de hipótesis 1.3 Significancia y confianza 1.4 Rechazar \\(H_0\\) 1.5 Tipos de pruebas 1.6 \\(P-value\\) 1.7 Algunos puntos a recordar", " Capítulo 1 Pruebas de hipótesis pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } Una de las técnicas más poderosas con las que contamos para determinar la certeza de una idea, son las pruebas de hipótesis y en este apartado veremos un resumen acerca de todo lo que implica hacer una prueba de hipótesis junto con los puntos más importantes a tener en cuenta sobre las conclusiones y las decisiones que se toman a la hora de realizar esta técnica estadística. 1.1 ¿Qué es una Prueba de Hipótesis? Vamos a ver muchas definiciones a lo largo de esta sección y en esta en particular vamos a citar diferentes versiones que se pueden encontrar en varias fuentes populares. “Una prueba de hipótesis es el proceso de inferir desde una muestra si una afirmación determinada sobre la población parece ser cierta o no.” Conover (1999) “Una hipótesis estadística es una sentencia sobre la naturaleza de una población. Por lo general, se formula en términos de un determinado parámetro de la población.” Ross (2005) “La prueba de hipótesis en estadística es una forma de probar los resultados de una encuesta o experimento para ver si tiene resultados significativos. Básicamente, está probando si sus resultados son válidos calculando las probabilidades de que sus resultados hayan ocurrido por casualidad. Si sus resultados pueden haber ocurrido por casualidad, el experimento no será repetible y, por lo tanto, tiene poca utilidad.” (StatisticsHowTo.com) “Testing in a nutshell: “Does our evidence make the null hypothesis look ridiculous?”\". (Cassie Kozyrkov, 2018 desde Towards Data Science) Como vemos, hay muchas fuentes interesantes y de cada una obtenemos cosas muy interesantes, así que vamos a dar algunas otras definiciones importantes con diferentes interpretaciones. 1.2 Dos tipos de hipótesis Es cierto que los tipos de hipótesis que existen son simple y compuesta, pero eso no nos interesa por el momento. Para realizar una prueba de hipótesis necesitamos una sentencia con la que vamos a determinar un comportamiento sobre la muestra (los datos) y su complemento. A la primera la llamaremos Hipótesis nula y a la complementaria Hipótesis alternativa; estas quedan codificadas como \\(H_0\\) y \\(H_1\\) respectivamente y lo que siempre vamos a hacer en una prueba de hipótesis es determinar si podemos rechazar con la información disponible la hipótesis nula y esto siempre se expresará de la siguiente manera: \\[ \\begin{array}{lcr} H_0 &amp; vs &amp; H_1 \\end{array} \\] A lo anterior se le llama regla de decisión. En este proceso vamos a tener que aplicar algunos supuestos sobre la población (vamos a considerar que son ciertos), como la distribución de la muestra, y estos van a recaer sobre la hipótesis nula y en base a esto, la prueba determinará algo sobre esta hipótesis: “La podemos rechazamos o no podemos.” Para que suceda lo primero, vamos a necesitar evidencia de que nuestros datos se comportan en mayor medida como lo que establece la hipótesis alternativa y si esto pasa diremos que se rechaza la hipótesis nula. En caso contrario, sólo podremos decir que No existe información suficiente para rechazar la hipótesis nula. Véase que todo es sobre la \\(H_0\\). Algunos ejemplos sencillos que podemos pensar sobre las pruebas de hipótesis son: \\[ \\begin{array}{ccc} H_0: \\mbox{Plutón es un planeta} &amp; vs &amp; H_1: \\mbox{Plutón no es un planeta}\\\\ H_0: \\mbox{La nueva droga no tuvo algún efecto} &amp; vs &amp; H_1: \\mbox{La nueva droga tuvo algún efecto}\\\\ H_0: \\mbox{Existe la relación #Cigueñas-#Bebes} &amp; vs &amp; H_1: \\mbox{No existe la relación #Cigueñas-#Bebes}\\\\ \\end{array} \\] ¿Ya se noto que en los ejemplos lo que se desea es rechazar la hipótesis nula? Esto no es coincidencia ya que es lo que buscamos generalmente (¡esto no sucede siempre!), ¿Por qué? Piensa lo siguiente: No podemos afirmar la certeza de algo cuando suponemos que es cierta Si no encontramos como rechazar algo, ¿Significa que entonces es cierto o que simplemente no encontramos la forma de rechazarlo? De hecho, históricamente Fisher consideró a la hipótesis nula como aquella que uno intenta desacreditar. En la palabras de Fisher: “Puede decirse que todos los experimentos se diseñan para poder asignar una probabilidad al hecho de que los resultados se opongan a la hipótesis nula” Ross (2005) Aquí se deja otro tipo de hipótesis comunes en términos matemáticos \\[ \\begin{array}{ccc} H_0: p = p^* &amp; vs &amp; H_1: p \\neq p^*\\\\ H_0: X\\sim N(0,1) &amp; vs &amp; H_1: X\\nsim N(0,1)\\\\ H_0: \\mathbb{P}(X, Y) = \\mathbb{P}(X)\\mathbb{P}(Y) &amp; vs &amp; H_1: \\mathbb{P}(X, Y) \\neq \\mathbb{P}(X)\\mathbb{P}(Y)\\\\ \\end{array} \\] Ahora, para cada prueba se tendrá algo específico que cambiará la forma de llevar a cabo esta, y esto se consigue con el Estadístico, que no hay que olvidar que un estadístico no es más que un valor que se determina con la información obtenida (los datos, la muestra) y como al inicio pensamos que la hipótesis nula se cumple, este tendrá alguna distribución específica, ya sea una distribución \\(N(\\mu, \\sigma)\\), \\(t_{(m)}\\), \\(\\chi^2_{(m)}\\), \\(F_{(m)}\\), etc. Véase que este entonces tendrá toda la información de la muestra y no tendrá alguna preferencia sobre la hipótesis \\(H_0\\) o \\(H_1\\). Sí bien es cierto que esta pensado en que la hipótesis nula es cierta, la propia muestra nos puede dar evidencia estadística de que nuestra muestra tiene un comportamiento diferente. No hay que olvidar que estamos tratando de hacer inferencia con datos que bien pudieron ser alterados por el propio azar, por lo que siempre damos una tolerancia a que sucedan eventos no significativos. 1.3 Significancia y confianza ¿Qué significa que un evento sea estadísticamente significante o que no lo sea? Creo que esta referencia nos puede ser de utilidad “La importancia estadística ayuda a cuantificar si un resultado es probable que se deba al azar o a algún factor de interés” (Amy Gallo, A Refresher on Statistical Significance). Es decir, si tenemos que una observación no es estadísticamente significante, es porque fue generada por factores externos que la mayoría de la población no tuvo, o bien pudo ser debido simplemente por el azar. Con esto, nosotros podemos establecer un margen para permitir este tipo de observaciones, ya que establecer que nuestras conclusiones en base a nuestra prueba sucederán siempre es algo atrevido; no hay que olvidar que en primera estamos trabajando bajo variables aleatorias, por lo que siempre se pueden tener distintos valores en diferentes ocasiones bajo una distribución establecida y que estamos trabajando con una muestra que suponemos significativa de la población, por lo que alguien en la población podría ser especial y comportarse de manera distinta. La significancia de la prueba la denotaremos como \\(\\alpha\\) y es común ver en libros de texto que esta se establece con un nivel del 5%, es decir que \\(\\alpha = 0.05\\). Esto hace que permitamos que el 5% de nuestros resultados no sigan a la hipótesis \\(H_0\\). Al complemento, \\(1-\\alpha\\), le llamaremos nivel de confianza y este nos dirá, valga la redundancia, cuando debemos confiar en nuestra prueba. Si por ejemplo tenemos un \\(\\alpha = 0.1\\), significa que estamos permitiendo que 1/10 observaciones no obedezca a la hipótesis nula, por lo que 9/10 (\\(1-\\alpha\\)) observaciones sí lo hacen. ¿Qué tan pequeña o grande debe ser la significancia de la prueba? Eso depende de cada investigador o quien este realizando la prueba. Es común dar un nivel del 5% pero aveces se prefiere más exactitud y se toma un 1%; tal vez no se desea ser tan riguroso o asumir un riesgo grande, así tal vez \\(\\alpha\\) podría valer 10%, etc. Un caso interesante es la exactitud con la que algunos estudios en física se realizan, por ejemplo para determinar que tan extraño es encontrar un boson de Higgs se considera una probabilidad de \\(3X10^-{7}\\), a lo cual lo llaman 5 sigmas y esto queda claro cuando se entiende que \\(\\sigma\\) representa la cantidad de desviaciones estandar de una normal. 1.4 Rechazar \\(H_0\\) Vale, ya quedo establecido que en general nos enfocamos en contradecir la hipótesis nula ¿Cómo lo hacemos? Esto va a depender de que tipo de prueba estemos realizando pero esto lo veremos más adelante, por el momento vamos a considerar un ejemplo sencillo. Supongamos que tenemos una muestra aleatoria y representativa de tamaño 20 que se distribuye \\(N(\\mu,\\sigma = 4)\\), tal vez esta represente las mediciones sobre la fuerza de una máquina, la intensidad de una señal o la edad en un cierto cohorte. Lo que se quiere determinar es si la media (la esperanza) es mayor a cierto valor, supongamos 10. Con esta información podemos determinar que estamos ante un contraste de medias de una población normal y formular lo siguiente \\[ \\begin{array}{ccc} H_0: \\mu \\leq \\mu_0 = 10 &amp; vs &amp; H_1: \\mu &gt; \\mu_0 = 10 \\end{array} \\] donde el estadístico \\(T = \\frac{|\\bar{X}-\\mu_{0}|}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\). En este caso tendríamos que \\(T = 1.79\\). ¿Cómo se ve esto gráficamente? tibble(x = c(-4.5, 4.5)) %&gt;% #Creamos unos datos que nos ayuden a colocar los ejes ggplot(aes(x = x)) + #Creamos una hoja para agregar elementos gráficos stat_function(fun = ~dnorm(x = .x)) + # Dibujamos una curva de acuerdo a una función geom_segment(x = 1.79, xend = 1.79, y = 0, yend = dnorm(1.79), color = &quot;#1854a8&quot;) + #Estadístico labs(y = &quot;Densidad&quot;) + ggtitle(TeX(&quot;N(0,1)&quot;)) + general_theme #Sólo agregamos más diseño Y ¿Cómo se vería representado el nivel de significancia? Para responder esto tenemos que recurrir al cuantil en el cual se alcanza la significancia establecida, en este caso es del 5% por lo que \\(Z_q = Z_{1- 0.05} =\\) 1.6448536. cuantil &lt;- qnorm(0.05, lower.tail = F) tibble(x = c(-4.5, 4.5)) %&gt;% #Creamos unos datos que nos ayuden a colocar los ejes ggplot(aes(x = x)) + #Creamos una hoja para agregar elementos gráficos stat_function(fun = ~dnorm(x = .x)) + # Dibujamos una curva de acuerdo a una función geom_segment(x = 1.79, xend = 1.79, y = 0, yend = dnorm(1.79), color = &quot;#1854a8&quot;) + #Estadístico geom_segment(x = cuantil, xend = cuantil, y = 0, yend = dnorm(cuantil), color = &quot;red&quot;) + #Cuantil stat_function(fun = ~under_curve(type = &quot;greater&quot;, 0.05 ,.x, dnorm, qnorm), geom = &#39;area&#39;, fill = &quot;red&quot;, alpha = 0.2) + #Una función para rellenar la zona de rechazo labs(y = &quot;Densidad&quot;) + ggtitle(TeX(&quot;N(0,1)&quot;)) + general_theme #Sólo agregamos más diseño Lo que nos esta diciendo la gráfica anterior es, que nuestro estadístico, el cual se distribuye de la misma forma que la gráfica de densidad, cae dentro de una zona en la cual nosotros establecimos que deberían estar los eventos no significantes, y como nuestro estadístico es un resumen completo de la muestra, tenemos evidencia para rechazar la hipótesis nula, en nuestro contexto se tendría que los datos muestran que la media sí es más grande que 10. A la zona coloreada en rojo se le llama región de rechazo o región crítica y queda definida como los valores con los cuales se debe rechazar la hipótesis nula. ¿Qué hubiera sucedido si se hubiera definido \\(\\alpha = 0.01\\)? Aquí te dejo la gráfica NOTA: Estos conceptos son análogos para otras distribuciones que requieren otro tipo de pruebas. 1.5 Tipos de pruebas Como ya vimos que debemos comparar un cuantil contra nuestro estadístico para determinar si rechazamos una hipótesis nula, es útil considerar no sólo el enfoque anterior, si no los otros casos. En la siguiente lista se dejan los otros tipos de prueba junto con el nombre que reciben. \\(H_0: \\mu = \\mu_0 = 10 \\ vs \\ H_1: \\mu \\neq \\mu_0 = 10\\). Prueba de dos colas \\(H_0: \\mu \\geq \\mu_0 = 10 \\ vs \\ H_1: \\mu &lt; \\mu_0 = 10\\). Prueba de cola izquierda \\(H_0: \\mu \\leq \\mu_0 = 10 \\ vs \\ H_1: \\mu &gt; \\mu_0 = 10\\). Prueba de cola derecha Creo que la mejor explicación para los nombres de estas pruebas es ver como se comportarían gráficamente las regiones de rechazo Ahora, aquí hay algo importante. En una prueba donde se esta asumiendo una distribución simétrica y se desea contrastar una hipótesis del estilo \\(H_0: \\mu = \\mu_0 \\ vs \\ H_1: \\mu \\neq \\mu_0\\), por lo que se debe considerar tal cual el efecto de simetría, ya que realmente las pruebas están diseñadas para medir la separación entre un valor teórico y un valor dado, en este caso hablamos de las medias, por lo que la simetría afectará a este tipo de pruebas y más en este caso en que la simetría hace que existan valores negativos que, al tomar el valor absoluto entre la diferencia de las medias, duplicarán la probabilidad. De hecho, lo que se hace en estos casos es considerar \\(\\alpha = \\alpha_1 + \\alpha_2\\), en general se considera \\(\\alpha/2\\). En este caso en particular se tiene que los cuantiles para una normal estandar se calculan con \\(\\alpha/2\\) (\\(Z_{\\alpha/2}\\)) de probabilidad para cada cola correspondiente. Entonces, sin ver las regiones de rechazo ¿Qué concluirías de los siguientes resultas gráficos si estos representan una prueba de hipótesis como las que se han visto en este material? 1.6 \\(P-value\\) Una de las herramientas más comunes de utilizar en una prueba de hipótesis es algo llamado \\(p-value\\), que no es más que una probabilidad que cambia de acuerdo a la prueba: Dos colas: \\(2*min(\\mathbb{P}(X&lt;T), \\mathbb{P}(X&gt;T))\\) Cola izquierda: \\(\\mathbb{P}(X&lt;T)\\) Cola derecha: \\(\\mathbb{P}(X&gt;T)\\) Como se puede apreciar, el \\(p-value\\), dando que la hipótesis nula es cierta, es la probabilidad de obtener valores “más grandes” o mejor dicho una diferencia igual o más extrema que nuestro estadístico, es decir que la observada; en el caso de una prueba de dos colas se aplica un criterio similar a la razón del porque se considera un \\(\\alpha /2\\). ¿Cómo se vería este gráficamente en la prueba anterior? cuantil &lt;- qnorm(0.05, lower.tail = F) tibble(x = c(-4.5, 4.5)) %&gt;% #Creamos unos datos que nos ayuden a colocar los ejes ggplot(aes(x = x)) + #Creamos una hoja para agregar elementos gráficos stat_function(fun = ~dnorm(x = .x)) + # Dibujamos una curva de acuerdo a una función geom_segment(x = 1.79, xend = 1.79, y = 0, yend = dnorm(1.79), color = &quot;#1854a8&quot;) + #Estadístico geom_segment(x = cuantil, xend = cuantil, y = 0, yend = dnorm(cuantil), color = &quot;red&quot;) + #Cuantil stat_function(fun = ~under_curve(type = &quot;greater&quot;, 0.05 ,.x, dnorm, qnorm), geom = &#39;area&#39;, fill = &quot;red&quot;, alpha = 0.2) + #Una función para rellenar la zona de rechazo stat_function(fun = ~under_curve(type = &quot;greater&quot;, 1-pnorm(1.79),.x, dnorm, qnorm), geom = &#39;area&#39;, fill = &quot;blue&quot;, alpha = 0.2) + #Una función para el p-value labs(y = &quot;Densidad&quot;) + ggtitle(TeX(&quot;N(0,1)&quot;)) + general_theme #Sólo agregamos más diseño La zona coloreada con azul es el \\(p-value\\). Observase que esta probabilidad esta ligada al estadístico y que, con el nivel de confianza establecido, se tiene que \\(p-value&lt;\\alpha\\). De hecho, otra manera muy común de rechazar las pruebas de hipótesis es con el siguiente criterio. \\[ Se \\ \\ rechaza \\ \\ H_0 \\ \\ cuando \\ \\ p-value&lt;\\alpha \\] Ya que el \\(p-value\\) es calculado de diferente manera para cada prueba, no es necesario saber si es de cola izquierda, derecha o de dos colas para sólo rechazar la hipótesis nula. De hecho, es común que en las funciones que tiene R para hacer pruebas de hipótesis como t.test(), binom.test(), chisq.test(), etc. no se muestre el cuantil con el cual se debe contrastar el estadístico. ¿Esto esta bien? Aquí hay un gran debate sobre el mal uso de los \\(p-values\\), ya que hay que mencionar que una prueba de hipótesis debería rechazarse con el contraste de un estadístico contra un cuantil, ya que el \\(p-value\\) sólo es una herramienta que aveces puede hacernos tomar decisiones equivocadas. ¿Conoces algún ejemplo donde el \\(p-value\\) indique una conclusión errónea? Además, ¿Qué pasas si nuestro \\(p-value\\) resulto un valor muy cercano a \\(\\alpha\\) como \\(0.049\\)? ¿Rechazamos? ¿No habrá sido este resultado tan pequeño debido al azar en algunas observaciones? Es real que existe una tendencia a darle preferencia al \\(p-value\\) en un sentido muy estricto en lugar de pensar en la creación y entendimiento de la prueba. De hecho The American Statistical Association aconseja y pide evitar los \\(p-values\\), ya que se han dado muchos casos en los que los experimentos se han modificado obteniendo más muestras (o tal vez hasta sesgando el experimento) hasta conseguir un \\(p-value&lt;0.05\\), lo cual le quita toda validez a uno de los principales supuestos que tiene una prueba de hipótesis: La muestra es aleatoria y representativa de la población. Si quieres ver un poco más de todo lo que he mencionado sobre los \\(p-values\\), puedes consultar los siguientes enlaces: Science world’s p-value controversy: Little number, big problem +The problem with p-values Statisticians Found One Thing They Can Agree On: It’s Time To Stop Misusing P-Values We’re All ‘P-Hacking’ Now Data dredging Otras preguntas interesantes sobre la prueba de hipótesis son las siguientes: Si hacemos muchas pruebas de hipótesis a la vez, ¿No se altera de alguna forma la exactitud de la prueba? Si sabemos que rechazamos una prueba tomamos otra que por complemento no se rechaza ¿Esta bien hacerlo? Hay propuestas diferentes para evitar el \\(p-value\\) Si tengo dos pruebas de hipótesis que me dan conclusiones similares ¿Cuál debería elegir para mostrar con mayor exactitud los valores del estadístico y el cuantil? ¿El tamaño de muestra afecta a la prueba de hipótesis? Y para finalizar, aquí se dejan algunos enlaces interesantes Basic concepts of hypothesis testing Failing to Reject the Null Hypothesis A Refresher on Statistical Significance 5 Sigma What’s That? Statistics for people in a hurry Explaining p-values with puppies A lot of bad statistics in one picture 1.7 Algunos puntos a recordar No rechazar la hipótesis nula no significa aceptarla. Recordemos que sólo podemos asegurar un hecho cuando la muestra tenga información tal que con la hipótesis nula tomada como cierta, el estadística caiga en la región de rechazo o cuando el \\(p-value\\) sea pequeño. En caso contrario sólo podemos decir que “NO HAY INFORMACIÓN SUFICIENTE PARA RECHAZAR LA HIPÓTESIS \\(H_0\\)”, lo cual es muy diferente a aceptarla. En general, deseamos rechazar la hipótesis nula. Si la única forma viable de aceptar algo con una prueba de hipótesis es rechazando la hipótesis nula, lo ideal es formular la prueba para que esto suceda; claro, siempre que se pueda. La formulación de la prueba puede llevar un proceso. Muchas veces, por la información o los enunciados que se desean verificar, se puede formular la hipótesis de manera errónea, ya que hay que recordar bien como están construidas las hipótesis \\(H_0\\) y \\(H_1\\) para aplicar dicha prueba. Aveces tener una solución equivalente será lo ideal. Datos para realizar la prueba. Depende de cada prueba, se necesitarán distintos elementos numéricos. Asegurarte de tener al menos los necesarios para realizar la prueba sin tener que crear más suposiciones. La significancia y la muestra pueden afectar la prueba. Recordemos que por convención se da un \\(\\alpha = 0.05\\), lo cual no es estrictamente necesario. Otro punto importante es que si la muestra es muy pequeña, puede afectar el resultado de la prueba así como el \\(p-value\\) y peor aún si la muestra no es representativa. En este último caso las pruebas no tendrán validez alguna. References "],["pruebas-binomiales-y-de-rangos.html", "Capítulo 2 Pruebas binomiales y de rangos", " Capítulo 2 Pruebas binomiales y de rangos pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } Mann-Whitney U Test in R Probability, Statistics, and Data: A fresh approach using R: Rank Based Tests JABSTB: Statistical Design and Analysis of Experiments with R: Sign Rank "],["tablas-de-contingencia.html", "Capítulo 3 Tablas de contingencia", " Capítulo 3 Tablas de contingencia pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } Además de la manera clásica que tenemos en R para generar una tabla de contingencia con base::table() podemos nosotros generar esta. Aquí se dejan tres maneras de hacer una tabla de contingencia apropiada con un data frame, para esto utilizaremos una bas da datos pública con datos de 120 años de las olimpiadas (Athens 1896-Rio 2016) obtenidos de este enlace. olympics &lt;- read_csv(&quot;athlete_events.csv&quot;) %&gt;% distinct() olympics # A tibble: 269,731 × 15 ID Name Sex Age Height Weight Team NOC Games Year Season City &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 A Diji… M 24 180 80 China CHN 1992… 1992 Summer Barc… 2 2 A Lamu… M 23 170 60 China CHN 2012… 2012 Summer Lond… 3 3 Gunnar… M 24 NA NA Denma… DEN 1920… 1920 Summer Antw… 4 4 Edgar … M 34 NA NA Denma… DEN 1900… 1900 Summer Paris 5 5 Christ… F 21 185 82 Nethe… NED 1988… 1988 Winter Calg… 6 5 Christ… F 21 185 82 Nethe… NED 1988… 1988 Winter Calg… 7 5 Christ… F 25 185 82 Nethe… NED 1992… 1992 Winter Albe… 8 5 Christ… F 25 185 82 Nethe… NED 1992… 1992 Winter Albe… 9 5 Christ… F 27 185 82 Nethe… NED 1994… 1994 Winter Lill… 10 5 Christ… F 27 185 82 Nethe… NED 1994… 1994 Winter Lill… # … with 269,721 more rows, and 3 more variables: Sport &lt;chr&gt;, Event &lt;chr&gt;, # Medal &lt;chr&gt; Nuestro objetivo por el momento es obtener una tabla de contingencia donde se relacionen el tipo de medallas y la edad de los que obtuvieron una medalla. La primera propuesta es determinar cuantas medallas se obtuvieron por edad para después separar los datos por las distintas edades y con cada uno de esos datos obtener cuantas medallas de bronce, plata y oro se obtuvieron incluyendo los casos donde no se tiene registro de algún participante con cierta edad que haya ganado algún tipo específico de medalla. get_n &lt;- function(df, Medalla){ nu &lt;- df %&gt;% filter(Medal == Medalla) if(dim(nu)[1] == 0){ return(integer(1)) }else{ return(nu$n) } } medals_by_age &lt;- olympics %&gt;% filter(!is.na(Medal) &amp; !is.na(Age)) %&gt;% group_by(Age) %&gt;% count(Medal) %&gt;% nest() %&gt;% mutate(Bronze = map_int(data, ~get_n(.x,&quot;Bronze&quot;)), Silver = map_int(data, ~get_n(.x,&quot;Silver&quot;)), Gold = map_int(data, ~get_n(.x,&quot;Gold&quot;))) %&gt;% dplyr::select(-data) medals_by_age # A tibble: 61 × 4 # Groups: Age [61] Age Bronze Silver Gold &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 10 1 0 0 2 11 0 1 0 3 12 3 3 0 4 13 2 7 7 5 14 18 30 27 6 15 54 67 75 7 16 105 129 116 8 17 172 163 199 9 18 286 294 280 10 19 469 441 459 # … with 51 more rows En la siguiente propuesta se obtendrá el resultado anterior de una manera sencilla, así que primero se muestra antes de crear la tabla de contingencia. medals_by_age &lt;- olympics %&gt;% filter(!is.na(Medal) &amp; !is.na(Age)) %&gt;% group_by(Age, Medal) %&gt;% tally() %&gt;% spread(Medal, n, fill = 0) medals_by_age # A tibble: 61 × 4 # Groups: Age [61] Age Bronze Gold Silver &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 10 1 0 0 2 11 0 0 1 3 12 3 0 3 4 13 2 7 7 5 14 18 27 30 6 15 54 75 67 7 16 105 116 129 8 17 172 199 163 9 18 286 280 294 10 19 469 459 441 # … with 51 more rows Ya con este resultado, es sencillo crear nuestra tabla de contingencia table_medals &lt;- matrix(c(medals_by_age$Bronze, medals_by_age$Silver, medals_by_age$Gold), nrow = 3, byrow = T) colnames(table_medals) &lt;- medals_by_age$Age rownames(table_medals) &lt;- c(&quot;Bronze&quot;, &quot;Silver&quot;, &quot;Gold&quot;) table_medals 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Bronze 1 0 3 2 18 54 105 172 286 469 692 899 1081 1116 1106 1045 1001 890 Silver 0 1 3 7 30 67 129 163 294 441 638 860 982 1142 1048 1023 947 937 Gold 0 0 0 7 27 75 116 199 280 459 674 925 1096 1136 1135 1056 976 863 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 Bronze 747 704 523 424 307 236 217 153 115 103 75 62 63 47 41 37 34 28 26 17 16 Silver 785 588 549 425 347 264 205 156 135 93 81 61 66 51 38 32 44 27 28 13 15 Gold 802 649 527 400 357 292 217 176 133 81 89 65 74 43 42 32 38 20 24 24 21 49 50 51 52 53 54 55 56 57 58 59 60 61 63 64 65 66 68 69 71 72 73 Bronze 19 8 11 15 5 4 5 4 4 3 1 2 2 3 1 1 0 0 1 1 1 0 Silver 15 13 7 8 8 7 7 6 1 6 3 6 4 2 0 0 2 2 1 1 1 1 Gold 15 12 4 12 6 15 1 10 2 3 2 4 0 4 2 0 0 0 0 0 0 0 Lo anterior puede ser obtenido con la función tabyl(). Esta función nos permitirá crear tablas de conteo a partir de una data frame con mencionar las variables que deseamos; se recomienda leer el siguiente vingette para obtener más información; además de revisar el paquete janitor del cual proviene dicha función. table_medals_efficent &lt;- olympics %&gt;% filter(!is.na(Medal) &amp; !is.na(Age)) %&gt;% tabyl(Medal, Age) table_medals_efficent Medal 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Bronze 1 0 3 2 18 54 105 172 286 469 692 899 1081 1116 1106 1045 1001 890 Gold 0 0 0 7 27 75 116 199 280 459 674 925 1096 1136 1135 1056 976 863 Silver 0 1 3 7 30 67 129 163 294 441 638 860 982 1142 1048 1023 947 937 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 747 704 523 424 307 236 217 153 115 103 75 62 63 47 41 37 34 28 26 17 16 19 8 802 649 527 400 357 292 217 176 133 81 89 65 74 43 42 32 38 20 24 24 21 15 12 785 588 549 425 347 264 205 156 135 93 81 61 66 51 38 32 44 27 28 13 15 15 13 51 52 53 54 55 56 57 58 59 60 61 63 64 65 66 68 69 71 72 73 11 15 5 4 5 4 4 3 1 2 2 3 1 1 0 0 1 1 1 0 4 12 6 15 1 10 2 3 2 4 0 4 2 0 0 0 0 0 0 0 7 8 8 7 7 6 1 6 3 6 4 2 0 0 2 2 1 1 1 1 Con esta tabla, ya podemos realizar una prueba \\(\\chi^2\\). ¿Que se interpreta del siguiente resultado? ¿Necesitaríamos modificar algo en la tabla anterior para tener una mayor confianza de los resultados? chisq.test(table_medals_efficent) Pearson&#39;s Chi-squared test data: table_medals_efficent X-squared = 132.57, df = 120, p-value = 0.204 "],["pruebas-de-bondad-y-ajuste.html", "Capítulo 4 Pruebas de bondad y ajuste 4.1 Funciones de distribución comunes y técnicas de identificación 4.2 Programación y evaluación 4.3 Comparación de pruebas 4.4 El caso de la normal", " Capítulo 4 Pruebas de bondad y ajuste De acuerdo al libro “Futbol y Matemáticas, Aventuras matemáticas del deporte rey”, la distribución de los máximos goleadores desde la temporada 1968/1987 a la temporada 2013/2014 queda representada con el siguiente histograma; ademas se menciona que dichos datos quedan modelados por una distribución Gumbel con parámetros \\(a = 5.44\\) y \\(b = 26.9\\). Esto resulta muy interesante pero, ¿Cómo podemos tener certeza estadística de esta declaración? Bueno, esté será el objetivo de las pruebas de bondad de ajuste; determinan si cierta información queda ajustada bajo alguna distribución que propongamos. Bajo esta idea, es importante que utilicemos todas nuestras herramientas estadísticas y probabilísticas para determinan si una distribución \\(F\\) es adecuada para mi información o no. Más adelante hablaremos un poco acerca del ejemplo anterior, por el momento es bueno tener en claro que deseamos probar la siguiente prueba de hipótesis: \\[ \\begin{array}{c} \\textbf{H}_0: \\ \\mbox{Los datos siguen una distribución} \\ \\ F_{0}(x) \\\\ vs\\\\ \\textbf{H}_a: \\ \\mbox{Los datos no siguen una distribución} \\ \\ F_{0}(x) \\end{array} \\] 4.1 Funciones de distribución comunes y técnicas de identificación Consideremos que tenemos los datos correspondientes al tiempo en segundos que tardar en pasar 100 automóviles por un cierto punto en una carretera. ¿Que distribución propondríamos como una primera idea? Aquí se muestran un poco los datos. # A tibble: 50 × 2 Observación Tiempos &lt;int&gt; &lt;dbl&gt; 1 1 32.4 2 2 28.8 3 3 36.1 4 4 28.7 5 5 28.5 6 6 34.1 7 7 28.3 8 8 33.0 9 9 39.3 10 10 37.1 # … with 40 more rows Para dar un conjunto de candidatos, sería buena práctica responder las siguientes preguntas: ¿Que tipo de datos tengo, continuos o discretos? Sí son discretos, ¿Mis datos provienen de una variable de conteo? ¿Que rango ocupan mis datos? ¿Hay algún indicio de tratar con alguna cola pesada? ¿Que tan simétrica es mi distribución? ¿Cómo se comportan las estadísticas básicas en mis datos? Además de esto, no olvidemos que estamos tratando con datos que pueden tener diversos problemas, por lo que podríamos primero hacer aplicar una limpieza para después continuar con el verdadero proceso. En este caso podemos ver los valores perdidos de esta manera: visdat::vis_miss(data_autos, sort_miss=TRUE, warn_large_data = FALSE) De acuerdo a nuestro problema podemos tratar de diferente manera los valores perdidos, pero lo más sencillo (lo cual debe ser el último recurso para evitar pérdida de información) es eliminar estos registros para no tener problemas posteriores data_autos &lt;- drop_na(data_autos) Ahora, para este caso es evidente que nuestros datos son continuos, por lo que podemos descartar todas las distribuciones discretas como la son la poisson (ideal para conteos), binomial (combinaciones \\(k\\) en \\(n\\)), geométrica (número de fallos antes del primer éxito), bernoulli (eventos con solo dos opciones de resultado), hipergeométrica (\\(k\\) éxitos en \\(n\\) ensayos sin remplazo), alguna distribución de la familia (\\(a\\),\\(b\\)), etc. Genial, ahora sólo nos falta infinidad de distribuciones, ya que podría hacer alguna función que cumpla las propiedades de una función de distribución de probabilidad. Bajo este caso, vamos a considerar distribuciones conocidas. Véase que por la naturaleza de nuestro problema no es ideal considerar una distribución Normal (¿Por qué?). Vamos a ver algunas gráficas que podrían ser de ayuda. boxplot_autos &lt;- data_autos %&gt;% ggplot(aes(x = Tiempos)) + geom_boxplot() + coord_flip() + general_theme + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) density_autos &lt;- data_autos %&gt;% ggplot(aes(x = Tiempos)) + geom_density() + labs(y = &quot;Densidad&quot;) + geom_histogram(aes(y = ..density..), fill = NA, color = &quot;black&quot;) + general_theme ecdf_autos &lt;- data_autos %&gt;% ggplot(aes(x = Tiempos)) + stat_ecdf(geom = &quot;step&quot;) + labs(y = &quot;Distribución empírica&quot;) + general_theme (density_autos + ecdf_autos)/boxplot_autos ¿Qué podemos decir ahora? Tal vez necesitamos más ayuda; véamos las estadísticas básicas sobre estos datos Media = 33.3707742 Mediana = 33.1964279 Moda = 31.64 Varianza = 10.8163016 Desviación estandar = 3.2888146 Rango intercuantil = [30.9755958, 35.6583154] Coeficiente de variación = 10% Coeficiente de asimetría (Skewness) = 0.2075035 Curtosis = 2.8698984 Recordemos que tanto la media, la mediana y la moda son estadísticas de tendencias centrales; la varianza y la desviación estandar nos dan una cantidad asociada a lo disperso que son nuestros datos; el rango intercuantil es la diferencia entre el tercer y el primer cuartil y podemos pensarlo como una medida de variabilidad de acuerdo a la mediana. Por otro lado, el coeficiente de variación (\\(C_V = \\frac{\\sigma}{\\bar{x}}\\)) es la relación entre el tamaño de la media y la variabilidad de la variable que podemos interpretar como un grado de variabilidad independiente de la escala de la variable. El coeficiente de asimetría nos indica que tan simétrica es nuestra distribución, la normal tiene un coeficiente igual a 0. Finalmente la curtosis nos da una idea de la forma de nuestra distribución por la concentración de datos. Entonces, ya con las gráficas, la metadata de los datos y las estadísticas anteriores, podemos dar posibles distribuciones candidatas para nuestos datos. • Al ser los valores mayores a cero y continuos, descartamos cualquier variable discreta y cualquiera que tenga un rango con valores negativos (como la normal y la t de Student), además de que se tienen valores mayores a uno por lo que se descartan otras distribuciones como la beta. • La densidad no tiene colas pesadas, en todo caso, la cola donde tiene la mayor cantidad de outliers es la cola derecha; por lo que descartamos distribuciones con colas muy pesadas (como la log-Normal, Pareto y Burr) • Al tener una curtosis positiva, indica la presencia de un pico y al tener un coeficiente de variación del 10%; los valores no varían demasiado de la media, lo cual, generalmente no cumple la distribución exponencial, por lo que podemos pensar en una función gamma o weibull para ajustar estos datos. • Si bien la distribución Weibull nos ayuda a modelar el tiempo en algunos escenarios, con sólo leer el enunciado es natural proponer una distribución gamma, ya que esta variable nos ayuda a modelar el tiempo; además de que podemos pensar a una distribución gamma como una suma de \\(n\\) variables independientes con distribución exponencial, la cual nos ayuda a modelar el tiempo de espera a que suceda un evento, en este caso el tiempo en que cada auto pasa por dicho punto en la carretera. Por si faltaba alguna otra razón si pensamos en su parámetro de escala como la cantidad de autos que estamos considerando (100), su coeficiente de simétrica teórica será \\(2/\\sqrt{100} = 0.2\\) la cual no es muy diferente al que obtuvimos. Ya tenemos propuesto uno de los parámetros para la distribución gamma, pero falta el parámetro de escala; (si conocieramoos cada cuanto pasan los coches de manera individual podríamos proponer este parámetro) por lo que podemos obtenerlo con alguna de las distintas técnicas que tenemos para calcular parámetros, ya sea por el método de momentos, máxima verosimilitud, con el UMVUE, etc. Para este caso, una propuesta para estos estimadores basados en la función de máxima verosimilitud son las siguientes: \\[ \\hat{k} = \\frac{N\\sum_{i = 1}^Nx_i}{N\\sum_{i = 1}^Nx_i\\ln(x_i)-\\sum_{i = 1}^N\\ln(x_i)\\sum_{i = 1}^Nx_i}\\\\ \\hat{\\theta} = \\frac{1}{N^2}\\left(N\\sum_{i = 1}^Nx_i\\ln(x_i)-\\sum_{i = 1}^N\\ln(x_i)\\sum_{i =1}^Nx_i\\right) \\] 4.2 Programación y evaluación Otra manera que tenemos de calcular los parámetros es a través de métodos numéricos, como lo realiza la función fitdistrplus::fitdist() y nos da diferentes estadísticas como el AIC, BIC y Likehood ratio test en diferentes métodos de estimación para que nosotros decidamos cual elegir. En este caso podemos aplicarlo de la siguiente manera fitdist(data = data_autos$Tiempos, distr = &quot;gamma&quot;, method = &quot;mme&quot;) Fitting of the distribution &#39; gamma &#39; by matching moments Parameters: estimate shape 103.162826 rate 3.091412 Como vemos no estabamos tan alejados con nuestra propuesta, además, ya sabiendo que el parámetro de escala es \\(\\sim 3 = \\lambda\\), podemos pensar que la tasa a la que cada auto pasa, en promedio, es 3 segundos, es decir que cada .3 segundos un nuevo auto pasa. Vamos a ajustar con la prueba \\(\\chi^2\\) para determinar que tan bien se ajustan nuestros datos. #Observados corte&lt;- cut(data_autos$Tiempos, breaks = 5) observados&lt;-table(corte) #Levels: (24.7,28.8] (28.8,33] (33,37.2] (37.2,41.3] (41.3,45.5] #Esperados int1&lt;-pgamma(q = 28.8, rate = 3.091412, shape=103.162826)-pgamma(q = 0, rate = 3.091412, shape=103.162826) int2&lt;-pgamma(q = 33, rate = 3.091412, shape=103.162826)-pgamma(q = 28.8, rate = 3.091412, shape=103.162826) int3&lt;-pgamma(q = 37.2, rate = 3.091412, shape=103.162826)-pgamma(q = 33, rate = 3.091412, shape=103.162826) int4&lt;-pgamma(q = 41.3, rate = 3.091412, shape=103.162826)-pgamma(q = 37.2, rate = 3.091412, shape=103.162826) int5&lt;-pgamma(q = Inf, rate = 3.091412, shape=103.162826)-pgamma(q = 41.3, rate = 3.091412, shape=103.162826) esperados&lt;-c(int1,int2,int3,int4,int5) #Hacemos la prueba chisq.test(x = observados, p = esperados) Chi-squared test for given probabilities data: observados X-squared = 1.3959, df = 4, p-value = 0.8449 ¿Qué podemos concluir? Vamos a colocar unos parámetros con menos decimales para ver que sucede #Esperados int1&lt;-pgamma(q = 28.8, rate = 3, shape=100)-pgamma(q = 0, rate = 3, shape=100) int2&lt;-pgamma(q = 33, rate = 3, shape=100)-pgamma(q = 28.8, rate = 3, shape=100) int3&lt;-pgamma(q = 37.2, rate = 3, shape=100)-pgamma(q = 33, rate = 3, shape=100) int4&lt;-pgamma(q = 41.3, rate = 3, shape=100)-pgamma(q = 37.2, rate = 3, shape=100) int5&lt;-pgamma(q = Inf, rate = 3, shape=100)-pgamma(q = 41.3, rate = 3, shape=100) esperados&lt;-c(int1,int2,int3,int4,int5) #Hacemos la prueba chisq.test(x = observados, p = esperados) Chi-squared test for given probabilities data: observados X-squared = 0.9446, df = 4, p-value = 0.9181 Mejoró considerablemente el ajuste. En cuanto al ejemplo de la cantidad de goles, la distribución que se ajusta es una distribución ideal para modelar datos extremos (Gumbel). Esta se utiliza para mostrar que en la temporada 2011/2012, Messi anotó en total 50 goles y que esto fue un hecho excepcional ya que con esta modelación, la probabilidad acumulada de que alguien vuelva a anotar más de 50 goles es de 1.36% o 1/73, lo que quiere decir que “deberíamos esperar una actuación como la de Messi una vez cada 73 años. La esperanza media de vida en Argentina es de 75 años. Visto desde esa perspectiva, Messi es realmente un acontecimiento que ocurre una vez en la vida”[2] Una manera de proponer de manera inicial una distribución para un conjunto de datos es haciendo una gráfica donde se compare la curtosis (la forma de la distribución) contra el coeficiente de asimetría (comunmente tomado al cuadrado) en una gráfica conocida como Cullen and Frey graph. La idea es comparar estas dos estadísticas que ayudan a tener una idea de la forma de la distribución entre diversas distribuciones comunes y los datos muestrales. Esta gráfica la podemos obtener mediante la función fitdistrplus::descdist() proposals_timeCar &lt;- fitdistrplus::descdist(data_autos$Tiempos[!is.na(data_autos$Tiempos)]) Como se aprecia, la gráfica anterior nos sugiere diferentes distribuciones (normal, beta, gamma y log-normal) de la cual sabemos que la gamma es la que tiene una mejor justificación en este caso. Otro punto a considerar es que la función anterior otorga sugerencias en el ámbito continuo y discreto. Las distribuciones sugeridas sólo son propuestas, por lo que se recomienda siempre hacer pruebas de bondad y ajuste con dichas distribuciones. set.seed(50) example_d &lt;- fitdistrplus::descdist(rpois(100,lambda=2),discrete=TRUE) 4.3 Comparación de pruebas Además de la prueba Ji-cuadrada tenemos distintas pruebas que tienen ciertas ventajas y desventajas. Algunas de ellas son las siguientes y las referencias se pueden consultar al final de este capítulo: .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} .tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top} .tg .tg-0lax{text-align:left;vertical-align:top} Prueba Ventajas Desventajas \\(\\chi^2\\) \\(\\bullet\\) Utilizado para variables discretas y continuas. \\(\\bullet\\) Depende de un adecuado tamaño muestras.\\(\\bullet\\) No adecuado para muestras pequeñas.\\(\\bullet\\) Sensible a las agrupaciones propuestas (the choice of the bins).\\(\\bullet\\) El punto anterior afectará el poder de la prueba Kolmogorov-Smirnov \\(\\bullet\\) No depende de la función de distribución subyacente.\\(\\bullet\\) Test exacto.\\(\\bullet\\) Existen modificaciones para datos discretos y censurados. \\(\\bullet\\) Sólo para variables continuas.\\(\\bullet\\) Tiende a ser más sensible cerca del centro de la distribución que en las colas.\\(\\bullet\\) La distribución subyacente debe ser completamente especificada.\\(\\bullet\\) Se reduce la potencia cuando se estiman los parámetros de la distribución subyacente. Lilliefors \\(\\bullet\\) Prueba con mayor potencia que en K-S.\\(\\bullet\\) Prueba adecuada para normalidad cuando se estiman los parámetros desde la muestra. \\(\\bullet\\) Sólo para variables continuas. Anderson Darling \\(\\bullet\\) No depende de la función de distribución subyacente.\\(\\bullet\\) Da un mayor peso a las colas de la distribución\\(\\bullet\\) Existen ponderaciones para el estadístico que aumentan la potencia de la prueba.\\(\\bullet\\) Prueba con mayor potencia que en K-S. \\(\\bullet\\) La distribución subyacente debe ser completamente especificada.\\(\\bullet\\) Sólo para variables continuas. Cramér-von-Mises \\(\\bullet\\) Prueba con mayor potencia que en K-S \\(\\bullet\\) Sólo para variables continuas. También consideremos lo siguiente: Las últimas tres pruebas son mejoras sobre la prueba de Kolmogorov-Smirnov. La pruebas Anderson–Darling es una estadística EDF (empirical distribution function) cuadrática. Otra prueba con un estadístico EDF cuadrático es la prueba de Cramér–von Mises. La prueba de Watson es una modificación de la prueba de Cramér–von Mises. Aquí se enlistan algunas opciones para realizar dichas pruebas en R: Prueba Chi-square: stats::chisq.test() Prueba Kolmogorov-Smirnov: stats::ks.test() Prueba Lilliefors: KScorrect::LcKS() Prueba Anderson-Darling: goftest::ad.test() Prueba Cramer-von Mises: goftest::cvm.test() Otro punto interesante cuando se esta probando la pertenencia de una distribución sobre una muestra es el uso de gráficos para comprobar tal hipótesis. Esto se puede realizar mediante las gráficas Q-Q Plot y P-P Plot, donde básicamente se comparan los cuantiles o probabilidades muestrales contra los teóricos. La idea es que si la muestra sigue una cierta distribución \\(F^*\\), entonces los cuantiles de dicha distribución (por ejemplo) deberán ser muy cercanos a los cuantiles muestrales; si estos se comparan en una gráfica de dispersión y si lo anterior sucede, entonces las observaciones deberán caer sobre una recta con pendiente 1 e intercepto 0 como en los siguientes ejemplos. La pregunta natural que surge aquí es: Si tenemos pruebas estadísticas, ¿Para qué utilizar gráficas? Bueno, resulta que las pruebas aveces nos pueden dar resultados no adecuados a la realidad, ya sea por las mismas deficiencias de la prueba o por algunos otros factores como el tamaño muestral. En el siguiente artículo se puede ver como el tamaño muestral puede alterar los resultados de las pruebas (como en general sucede con pruebas de hipótesis) por lo que nunca estará demás (y es necesario) dar una justificación gráfica para reforzar nuestra conclusión sobre la hipótesis nula. 4.4 El caso de la normal Los puntos anteriores son consideraciones en general que se deben tener en cuenta al tratar con una distribución \\(F^*\\). El caso de la distribución es muy especial ya que es uno de las distribución más utilizadas en este tipo de pruebas al ser una distribución muy común que se utiliza como supuestos en distintas aplicaciones como en la regresión lineal y el análisis de discriminante. En el paper Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling Tests se hace una comparación entre diversas pruebas de bondad y ajuste sobre una distribución normal utilizando la potencia (simulada a través del método Monte Carlo) de cada una de las pruebas. Aquí se dejan algunas gráficas que se mencionan en dicho paper. Imagenes tomadas del artículo Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling Tests Es clara la superioridad de la prueba Shapiro Wilks ya que de hecho esta fue diseñada para probar la normalidad en una muestra aleatoria, muchas veces esta es conocida como la prueba más potente para probar normalidad. Sea cual sea la prueba que se desee realizar, estas pueden ser efectuadas en R con las siguientes funciones: Prueba Shapiro-Wilk: stats::shapiro.test Prueba Anderson-Darling: nortest::ad.test() Prueba Cramer-von Mises: nortest::cvm.test() Prueba Lilliefors: nortest::lillie.test() Prueba Pearson chi-square: nortest::pearson.test(). Prueba Shapiro-Francia: nortest::sf.test(). El contenido visto hasta ahora es llamado no paramétrico porque no es necesario algún supuesto relacionado con alguna distribución. Aunque es importante siempre tener en cuenta que las técnicas basadas en supuestos distributivos específicos son en general más poderosas. Por lo que si se puede confirmar el supuesto distributivo, generalmente se prefieren las técnicas paramétricas. Finalmente, aquí se dejan ciertos enlaces que pueden ser de interés: Kolmogorov-Smirnov Goodness-of-Fit Test Kolmogorov-Smirnov and related tests: Use &amp; misuse -Chi-Square Goodness-of-Fit Test -Chi-square goodness-of-fit test, more details Anderson-Darling Test Anderson-Darling and Shapiro-Wilk tests Test for Normality 6 ways to test for a Normal Distribution — which one to use? Shapiro-Wilk Test: What it is and How to Run it Use of the package fitdistrplus to specify a distribution from non-censored or censored data "],["regresión-lineal-simple.html", "Capítulo 5 Regresión lineal simple 5.1 Correlación y linearidad 5.2 Variables categóricas 5.3 Paquetes y funciones útiles 5.4 Ejemplo 5.5 Otros puntos importantes", " Capítulo 5 Regresión lineal simple pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } Nuestro objetivo en un modelo de regresión será ajustar o asociar una variable, a la que llamaremos variable respuesta o dependediente, con una o más variables distintas llamadas variables predictoras o independientes, con el fin de poder predecir valores futuros de la variable respuesta y analizar los efectos que tendría esta variable con alteraciones en el resto de variables del modelo. En este caso, el de la regresión lineal simple consideramos dos variables (1 predictora (\\(x\\)) y otra como respuesta (\\(y\\))) y establecemos este modelo de la siguiente manera: \\[ y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i} \\] Por algunas propiedades interesantes, podemos ver que \\(\\mathbf{E}[y_{i}]=\\beta_{0}+\\beta_{1}x_{i}\\) por lo que este modelo considera que la media de \\(y\\) cambia a razón constante mediante los cambios de \\(x\\). Como estamos obteniendo una recta, \\(\\beta_{1}\\) es la pendiente de dicha recta y es lo que nos indica la importancia de \\(x\\) sobre \\(y\\) (que tan drásticos son los cambios de una sobre otra). \\(\\beta_0\\) nos indica el valor de \\(y\\) cuando la variable predictora esta ausente. \\(\\epsilon\\) es simplemente un error aleatorio que nos dará ciertos problemas para la validación de este modelo mediante pruebas de hipótesis pero nos otorga la flexibilidad probabilística para la predicción. Es sencillo realizar una regresión lineal en R, simplemente podemos utilizar el comando base::lm() agregando una fórmula (~) como parámetro. Por ejemplo, consideremos la base de datos datasets::cars donde se tiene la velocidad y la distancia que recorrió un coche para detenerse; estos datos fueron recolectados en 1920. cars %&gt;% ggplot(aes(x = speed, y = dist)) + labs(x = &quot;Velocidad&quot;, y = &quot;Distancia&quot;) + geom_point(colour = &quot;purple3&quot;) + general_theme Ahora, realizaremos nuestro modelo lineal simple: car_regression &lt;- lm(dist~speed, data = cars) summary(car_regression) Call: lm(formula = dist ~ speed, data = cars) Residuals: Min 1Q Median 3Q Max -29.069 -9.525 -2.272 9.215 43.201 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.5791 6.7584 -2.601 0.0123 * speed 3.9324 0.4155 9.464 1.49e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 15.38 on 48 degrees of freedom Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Véase que se nos da mucha información: Estadísticas sobre los residuales. Información sobre los coeficientes de la regresión mediante mínimos cuadrados. Información sobre el ajuste del modelo con los datos. Estos puntos se verán después, algunos de ellos se verán hasta regresión lineal múltiple ya que tienen una interpretación similar, pero por el momento basta con saber que, de acuerdo a los datos obtenidos, nuestro modelo queda expresado de la siguiente manera: \\[ y_i = -17.5791 + 3.9324x_i \\] Que es lo mismo al siguiente modelo: \\[ \\mbox{Distancia del auto }i = -17.5791 + 3.9324\\times\\mbox{Velocidad del auto }i \\] Y ahora podemos generar la siguiente gráfica cars %&gt;% ggplot(aes(x = speed, y = dist)) + labs(x = &quot;Velocidad&quot;, y = &quot;Distancia&quot;) + general_theme + geom_point(colour = &quot;purple3&quot;) + geom_abline(intercept = car_regression$coefficients[1], slope = car_regression$coefficients[2], colour = &quot;red&quot;) Hay cosas importantes que ver en este tema y aquí se comenzará con un pequeño análisis de correlación. 5.1 Correlación y linearidad La manera más sencilla de proponer un conjunto de variables predictoras es ver la relación lineal que tienen estas con la variable respuesta. Esto lo podemos obtener mediante diferentes estadísticas, por ejemplo: Coeficiente de correlación de pearson: \\(\\rho_{xy} = r_{xy} = \\frac{\\sum_{i = 1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i = 1^n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i = 1}^n(y_i-\\bar{y})^2}}\\) Coeficiente de correlación de spearman: \\(r_s = 1- \\frac{6\\sum d_i^2}{n(n^2-1)}\\) donde \\(d_i = rango(X_i)-rango(Y_i)\\) Coeficiente de correlación de Kendall: \\(\\tau = \\frac{(\\mbox{número de pares concordantes})-(\\mbox{número de pares discordantes})}{n \\choose 2}\\) Cada uno de las estadísticas anteriores tiene un propósito diferente, \\(\\rho\\) mide la relación lineal entre las variables que se están comparando, el coeficiente de correlación de spearman mide relaciones monótonas (una relación lineal lo es) y el coeficiente de Kendall mide la semejanza en el ordenamiento de los datos cuando se clasifican en rangos por cada una de las cantidades. Todas estas se pueden obtener mediante la función base::cor(x, y, method = c(\"pearson\", \"kendall\", \"spearman\")). Es importante no sólo considerar el primero de los coeficientes de correlación mencionados, ya que puede existir una clara relación entre las variables (como en un conjunto de puntos que dibujen una parábola) y el coeficiente tener un valor de 0. Vamos a tomar los datos dplyr::starwars donde se tienen datos descriptivos de los personajes de la saga Star Wars. starwars # A tibble: 87 × 14 name height mass hair_color skin_color eye_color birth_year sex gender &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Luke S… 172 77 blond fair blue 19 male mascu… 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… 4 Darth … 202 136 none white yellow 41.9 male mascu… 5 Leia O… 150 49 brown light brown 19 fema… femin… 6 Owen L… 178 120 brown, grey light blue 52 male mascu… 7 Beru W… 165 75 brown light blue 47 fema… femin… 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… 9 Biggs … 183 84 black light brown 24 male mascu… 10 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Sólo para estos fines, eliminaremos un outlier sobre el peso y consideraremos sólo esta y la altura para ver si existe algún tipo de relación entre estas variables. new_starwars &lt;- starwars %&gt;% filter(mass &lt; max(starwars$mass, na.rm = T)) pearson_star &lt;- cor(new_starwars$height, new_starwars$mass) spearman_star &lt;- cor(new_starwars$height, new_starwars$mass, method = &quot;spearman&quot;) kendall_star &lt;- cor(new_starwars$height, new_starwars$mass, method = &quot;kendall&quot;) new_starwars %&gt;% ggplot(aes(x = height, y = mass)) + geom_point(color = &quot;royalblue4&quot;) + labs(x = &quot;Altura (cm)&quot;, y = &quot;Peso (kg)&quot;) + annotate(&quot;text&quot;, x = 100, y = 150, label = paste(&quot;Pearson: &quot;, pearson_star, collapse = &quot;&quot;)) + annotate(&quot;text&quot;, x = 100, y = 140, label = paste(&quot;Spearman: &quot;, spearman_star, collapse = &quot;&quot;)) + annotate(&quot;text&quot;, x = 100, y = 130, label = paste(&quot;Kendall: &quot;, kendall_star, collapse = &quot;&quot;)) + general_theme Como vemos, el coeficiente de correlación de pearson tiene un valor cercano al de Spearman, aunque este último nos inidica que existe una relación monótona, por lo que podríamos aplicar una transformación para otorgar un comportamiento más cercano al lineal con una función monóntona como lo es el logaritmo. pearson_star &lt;- cor(new_starwars$height, log(new_starwars$mass)) spearman_star &lt;- cor(new_starwars$height, log(new_starwars$mass), method = &quot;spearman&quot;) kendall_star &lt;- cor(new_starwars$height, log(new_starwars$mass), method = &quot;kendall&quot;) new_starwars %&gt;% ggplot(aes(x = height, y = log(mass))) + geom_point(color = &quot;royalblue4&quot;) + labs(x = &quot;Altura (cm)&quot;, y = &quot;Peso (kg)&quot;) + annotate(&quot;text&quot;, x = 100, y = 4.8, label = paste(&quot;Pearson: &quot;, pearson_star, collapse = &quot;&quot;)) + annotate(&quot;text&quot;, x = 100, y = 4.6, label = paste(&quot;Spearman: &quot;, spearman_star, collapse = &quot;&quot;)) + annotate(&quot;text&quot;, x = 100, y = 4.4, label = paste(&quot;Kendall: &quot;, kendall_star, collapse = &quot;&quot;)) + general_theme Nuestro coeficiente de correlación lineal aumente significativamente y surge la siguiente pregunta: ¿Cuándo utilizar alguna transformación y cuál? En general, cualquier función que sólo cambie la escala y este definida para los datos que se estén considerando se puede utilizar, como un logaritmo (que nos ayuda a reducir dispersión y dar valores pequeños), una raíz cuadrada, una función inversa, etc; aunque hay que tener cuidado. Por ejemplo ¿Qué pasaría con si tengo valores iguales a cero y aplico un logaritmo? ¿La interpretación sobre los coeficientes cambiaría o no? Este último punto se verá después. Se recomienda revisar el libro The Statistical Sleuth, tercera edición donde se muestran algunos patrones comunes que podrían impedir realizar alguna relación lineal entre variables. Ahora, si bien es razonable pensar que la altura y el peso estén relacionados de manera lineal ¿Podemos confiar siempre en el resultado de una correlación? No, ya que una correlación no implica causalidad (una relación verdadera entre dos variables). Veamos los siguientes ejemplos del siguiente enlace: Es difícil pensar que realmente existe una relación entre dichas variables en cada caso, así como considerar que existe una relación entre la cantidad de IPhones comprados y la cantidad de personas que mueren al caer de las escaleras o pensar que la cantidad de cigueñas esta relacionado con la cantidad de nacimientos en Europa aunque se tenga un \\(r = 0.62\\) y un \\(p-value = 0.008\\) en la prueba de hipótesis correspondiente a \\(\\beta_i\\). Otro punto interesante es, ¿Qué tanto podemos confiar sólo en nuestras estadísticas? Consideremos la siguiente información obtenida del paquete datasauRus. library(datasauRus) dino_data &lt;- datasauRus::datasaurus_dozen %&gt;% filter(dataset == &quot;dino&quot;) datasaurus_dozen %&gt;% group_by(dataset) %&gt;% summarize( mean_x = mean(x), mean_y = mean(y), std_dev_x = sd(x), std_dev_y = sd(y), corr_x_y = cor(x, y) ) # A tibble: 13 × 6 dataset mean_x mean_y std_dev_x std_dev_y corr_x_y &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 away 54.3 47.8 16.8 26.9 -0.0641 2 bullseye 54.3 47.8 16.8 26.9 -0.0686 3 circle 54.3 47.8 16.8 26.9 -0.0683 4 dino 54.3 47.8 16.8 26.9 -0.0645 5 dots 54.3 47.8 16.8 26.9 -0.0603 6 h_lines 54.3 47.8 16.8 26.9 -0.0617 7 high_lines 54.3 47.8 16.8 26.9 -0.0685 8 slant_down 54.3 47.8 16.8 26.9 -0.0690 9 slant_up 54.3 47.8 16.8 26.9 -0.0686 10 star 54.3 47.8 16.8 26.9 -0.0630 11 v_lines 54.3 47.8 16.8 26.9 -0.0694 12 wide_lines 54.3 47.8 16.8 26.9 -0.0666 13 x_shape 54.3 47.8 16.8 26.9 -0.0656 Como vemos, las estadísticas son bastante similares entre todos los conjuntos de datos, incluso la correlación; por lo que tal vez pensaríamos en tener datos similares; bueno veámos como se ven algunos de estos datos. ggplot(datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;h_lines&quot;)), aes(x=x, y=y, colour=dataset))+ geom_point()+ theme_void()+ theme(legend.position = &quot;none&quot;)+ general_theme+ facet_wrap(~dataset, ncol=3) Aquí se dejan algunos enlaces sobre estos ejemplos en particular: The Datasaurus data package Download the Datasaurus: Never trust summary statistics alone; always visualize your data Same Stats, Different Graphs ShinySaurus Y algunos con temas relacionados a la correlación: Correlation Pearson correlation coefficient Coeficiente de correlación de rango de Kendall Tau de Kendall Spearman’s rank correlation coefficient Coefficient of colligation A comparison of correlation measures 5.2 Variables categóricas Se puede trabajar con variables categóricas en un modelo de regresión para determinar la importante que tiene una cierta segregación en nuestra variable a modelar. Si bien se podría remplazar todos los valores categóricos por valores numéricos, esto nos ayudaría sólo con valores ordinales. Otra técnica común es la creación de variables dummys a lo cual se le conoce como One-Hot-Encoding. Para esta sección se utilizarán algunas variables de los tabulados de la encuesta Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). 2018 Nueva serie, específicamente para el estado de Aguascalientes. Nos interesan ciertas variables que se puedan relacionar con el ingreso corriente de los hogares de dicha región y en esta sección nos interesa determinar la importancia del sexo de los jefes del hogar. # A tibble: 2,306 × 15 edad_jefe sexo_jefe tot_integ percep_ing ing_cor alimentos vestido calzado &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 74 Hombre 3 3 76404. 5618. 0 0 2 48 Hombre 5 5 42988. 20930. 225. 176. 3 39 Hombre 2 2 580698. 37594. 2015. 0 4 70 Mujer 2 2 46253. 2893. 97.8 0 5 51 Mujer 4 2 53837. 7367. 0 0 6 41 Mujer 4 2 237743. 0 0 0 7 57 Mujer 1 1 32607. 11456. 0 0 8 53 Hombre 2 2 169918. 30986. 293. 1272. 9 30 Hombre 3 2 17311. 5773. 533. 0 10 69 Hombre 4 4 120488. 3986. 1174. 0 # … with 2,296 more rows, and 7 more variables: vivienda &lt;dbl&gt;, limpieza &lt;dbl&gt;, # salud &lt;dbl&gt;, transporte &lt;dbl&gt;, personales &lt;dbl&gt;, educacion &lt;dbl&gt;, # esparcimiento &lt;dbl&gt; Vamos a aplicar una regresión lineal entre el ingreso corriente y el sexo del jefe de familia para ver como se comporta la función lm(). data_income_sexo %&gt;% lm(ing_cor~sexo_jefe, .) %&gt;% tidy() # A tibble: 2 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 60072. 1672. 35.9 7.55e-225 2 sexo_jefeMujer -9823. 3348. -2.93 3.38e- 3 Al parecer trabaja bien dicha función con datos categóricos. Lo que sucedio es que tal función realizo una codificación sobre el factor sexo_jefe, de hecho aplico esta configuración contrasts(data_income_sexo$sexo_jefe) Mujer Hombre 0 Mujer 1 Es decir, la variable dummy que se esta considerando en el modelo tiene la siguiente codificación: 1 si una persona es mujer 0 si una persona es hombre Lo que otorgaría una ponderación sobre las variables del modelo cuando estos son diferentes de cero: \\(\\beta_0 + \\beta_1\\) si una persona es mujer. \\(\\beta_0\\) si una persona es hombre. Estos pueden ser interpretados de la siguiente manera: \\(\\beta_0 = 60071.50\\) es el promedio del ingreso entre los hombres. \\(\\beta_0 + \\beta_1 = 60071.50 - 9822.53 = 50248.97\\) es el promedio del ingreso entre las mujeres \\(beta_1 = -9822.53\\) es el la diferencia promedio en el ingreso entre los hombres y mujeres El coeficiente negativo para las mujeres en la regresión sólo indica que la categoría “mujer” está asociado con una disminución en el salario (en relación con los hombres). Si cambiamos la configuración de la variable dummy, la interpretación y los valores “se conservan” di_nc &lt;- data_income_sexo %&gt;% mutate(sexo_jefe = relevel(sexo_jefe, ref = &quot;Mujer&quot;)) contrasts(di_nc$sexo_jefe) Hombre Mujer 0 Hombre 1 Véase que el ingreso promedio entre las mujeres sigue siendo 50248.97 y la diferencia promedio en el ingreso entre los hombres y mujeres sigue siendo de 9822.53 pesos, aunque al tomar a los hombres como el factor con valor 1, indica un aumento en la variable respuesta. di_nc %&gt;% lm(ing_cor~sexo_jefe, .) %&gt;% tidy() # A tibble: 2 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 50249. 2901. 17.3 2.78e-63 2 sexo_jefeHombre 9823. 3348. 2.93 3.38e- 3 R Companion to Real Econometrics: Chapter 7 Dummy Variables: Smarter than You Think 5.3 Paquetes y funciones útiles 5.3.1 Correlación Al utilizar la función cor() con un conjunto de datos numéricos obtenemos una matriz de correlaciones entre las distintas variables; esta representación no es la mejor ayuda visual para identificar correlaciones fuertes, por lo que podemos utilizar un correlograma con dicha matriz. Vamos a tomar los datos de gasto-ingreso vistos anteriormente poniendo especial atención sobre el ingreso corriente de los hogares de dicha región. Aquí se ve una aplicación de la función corrplot::corrplot() library(corrplot) corrplot(cor(data_income), type = &quot;lower&quot;, diag = F) Tenemos otras alternativas que nos permiten tener un comportamiento similar al paquete ggplot, como lo es la función ggcorrplot::ggcorrplot(). library(ggcorrplot) ggcorrplot(cor(data_income), #method = &quot;circle&quot; #Método de visualización, &quot;square&quot; por default. hc.order = TRUE, #Orden jerárquico sobre los valores obtenidos. outline.col = &quot;white&quot;, #Color del margen de los cuadrádos o círculos. type = &quot;lower&quot; #Elementos a desplegar en la gráfica #lab = TRUE # Anotaciones de los valores obtenidos en cada celda de la matriz. #colors = c(&quot;green&quot;, &quot;black&quot;, &quot;red&quot;) #Colores ) Además de un correlograma, podemos obtener las correlaciones más importantes en otro tipo de gráfico, esto es lo que hace el paquete lares con las funciones corr_cross() y corr_var(). Para más información se sugiere revisar el siguiente post library(lares) corr_cross(data_income, max_pvalue = 0.05, # Correlaciones significantes a mostrar (a un nivel del 5%) top = 10 # Cantidad de variables a mostrar con mayor significancia ) + corr_var(data_income, ing_cor, # Nombre de la variable a comparar top = 5 # Cantidad de variables a mostrar con mayor significancia ) Igual podríamos elegir una visualización más tradicional pero con un formato más elegante, como el proporcionado con la función modelsummary::datasummary_correlation() library(modelsummary) datasummary_correlation(data_income, method = &quot;pearspear&quot; # &quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;, or &quot;pearspear&quot; (Pearson correlations above and Spearman correlations below the diagonal) ) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) %&gt;% scroll_box(width = &quot;100%&quot;) edad_jefe tot_integ percep_ing ing_cor alimentos vestido calzado vivienda limpieza salud transporte personales educacion esparcimiento edad_jefe 1 -.10 .09 .02 -.10 -.10 -.14 -.05 -.03 .08 -.02 -.02 -.09 -.09 tot_integ -.14 1 .67 .12 .24 .10 .22 .00 .03 -.02 .08 .13 .18 -.02 percep_ing .11 .61 1 .18 .23 .13 .18 .02 .06 .00 .10 .13 .15 .05 ing_cor .01 .30 .40 1 .37 .42 .28 .18 .41 .13 .28 .33 .22 .37 alimentos -.14 .32 .27 .52 1 .30 .32 .23 .40 .10 .34 .34 .28 .35 vestido -.18 .18 .21 .38 .37 1 .60 .14 .37 .10 .24 .35 .26 .34 calzado -.22 .31 .23 .30 .34 .54 1 .11 .30 .09 .22 .29 .30 .29 vivienda -.02 .11 .12 .25 .23 .14 .08 1 .23 .11 .15 .17 .15 .21 limpieza -.10 .19 .19 .48 .46 .38 .32 .23 1 .10 .39 .38 .31 .40 salud -.01 .09 .14 .29 .28 .25 .20 .12 .28 1 .07 .12 .07 .05 transporte -.08 .20 .23 .62 .47 .31 .27 .24 .44 .26 1 .31 .17 .28 personales -.16 .34 .26 .52 .50 .45 .40 .26 .58 .30 .47 1 .22 .30 educacion -.23 .45 .28 .30 .31 .25 .36 .11 .25 .15 .29 .32 1 .31 esparcimiento -.13 .04 .11 .41 .36 .32 .24 .14 .34 .20 .37 .33 .21 1 ggcorr::correlation matrixes with ggplot2 5.3.2 Regresión Con ggplot podemos añadir de manera rápida una recta de regresión a nuestros datos; véase como se hace uso de la base de datos de las olimpiadas y de la función ggplot::geom_smooth() para agregar una capa con la recta de regresión. olympics &lt;- read_csv(&quot;athlete_events.csv&quot;) %&gt;% distinct() olympics %&gt;% filter(!is.na(Medal) &amp; !is.na(Age)) %&gt;% filter(!(Year %in% c(1994, 1998, 2002, 2006, 2010, 2014))) %&gt;% #Eliminamos algunos valores inluyentes group_by(Year) %&gt;% count(Medal) %&gt;% ggplot(aes(x = Year, y = n)) + geom_point() + general_theme + geom_smooth(formula = y~x, method = &quot;lm&quot;) + labs(x = &quot;Año&quot;, y = &quot;Cantidad de medallas&quot;) 5.3.3 TidyModels::Broom library(broom) glance(car_regression) # A tibble: 1 × 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.651 0.644 15.4 89.6 1.49e-12 1 -207. 419. 425. # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; tidy(car_regression) # A tibble: 2 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -17.6 6.76 -2.60 1.23e- 2 2 speed 3.93 0.416 9.46 1.49e-12 5.4 Ejemplo Consideremos la base de datos de la encuesta ingreso-gasto del 2018 en Aguascalientes. .main-container { max-width: 300px !important; } library(skimr) fancy_summary &lt;- skim_with( numeric = sfl( Min = min, Max = max, #Q1 = ~ quantile(., probs = .25), Median = ~quantile(., probs = .50), #Q3 = ~ quantile(., probs = .75), Mean = mean, Sd = sd, hist = ~ inline_hist(., 5) ), append = FALSE ) fancy_summary(data_income) Table 5.1: Data summary Name data_income Number of rows 2306 Number of columns 14 _______________________ Column type frequency: numeric 14 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate Min Max Median Mean Sd hist edad_jefe 0 1 16.00 96.00 47.00 48.65 15.63 ▃▇▆▃▁ tot_integ 0 1 1.00 16.00 4.00 3.91 1.97 ▇▃▁▁▁ percep_ing 0 1 0.00 12.00 2.00 2.47 1.38 ▇▅▁▁▁ ing_cor 0 1 1873.77 2310762.00 43175.40 57622.26 69677.79 ▇▁▁▁▁ alimentos 0 1 0.00 76602.70 9681.26 11298.88 7755.85 ▇▂▁▁▁ vestido 0 1 0.00 26421.70 332.60 1044.05 2026.59 ▇▁▁▁▁ calzado 0 1 0.00 11739.12 273.91 681.48 1115.71 ▇▁▁▁▁ vivienda 0 1 0.00 70664.50 1759.72 2661.50 3317.12 ▇▁▁▁▁ limpieza 0 1 0.00 40309.56 1277.64 2196.77 2874.19 ▇▁▁▁▁ salud 0 1 0.00 184800.25 63.58 1257.52 6207.59 ▇▁▁▁▁ transporte 0 1 0.00 353941.46 4447.32 7505.52 12313.27 ▇▁▁▁▁ personales 0 1 0.00 113246.10 1749.66 2787.67 4087.78 ▇▁▁▁▁ educacion 0 1 0.00 160801.44 0.00 2788.35 6848.96 ▇▁▁▁▁ esparcimiento 0 1 0.00 40726.22 170.43 1074.29 2473.48 ▇▁▁▁▁ Aquí consideramos un modelo lineal por cada variable y mostramos el resultado de la regresión entre el ingreso y la edad del jefe de familia. all_linear_models &lt;- data_income %&gt;% dplyr::select(-ing_cor) %&gt;% map(~summary(lm(data_income$ing_cor~.x))) all_linear_models$edad_jefe Call: lm(formula = data_income$ing_cor ~ .x) Residuals: Min 1Q Median 3Q Max -56861 -30361 -14048 9919 2253282 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 53422.63 4745.03 11.26 &lt;2e-16 *** .x 86.33 92.86 0.93 0.353 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 69680 on 2304 degrees of freedom Multiple R-squared: 0.0003749, Adjusted R-squared: -5.895e-05 F-statistic: 0.8641 on 1 and 2304 DF, p-value: 0.3527 Para comparar los distintos modelos, podemos hacerlo bajo diferentes criterios; por ejemplo con el \\(R^2\\) all_linear_models %&gt;% map_df(~`[[`(.x, &quot;r.squared&quot;)) %&gt;% gather(&quot;Variable predictora&quot;, &quot;r&quot;, 1:13) %&gt;% arrange(desc(r)) # A tibble: 13 × 2 `Variable predictora` r &lt;chr&gt; &lt;dbl&gt; 1 vestido 0.177 2 limpieza 0.168 3 alimentos 0.140 4 esparcimiento 0.135 5 personales 0.109 6 calzado 0.0806 7 transporte 0.0804 8 educacion 0.0474 9 vivienda 0.0329 10 percep_ing 0.0316 11 salud 0.0175 12 tot_integ 0.0139 13 edad_jefe 0.000375 O por el p-value de la variable independiente all_linear_models_tidy &lt;- data_income %&gt;% dplyr::select(-ing_cor) %&gt;% map(~tidy(lm(data_income$ing_cor~.x))) all_linear_models_tidy %&gt;% map_df(~.x[2,&quot;p.value&quot;]) %&gt;% mutate(&quot;Variable predictora&quot; = names(all_linear_models_tidy)) %&gt;% dplyr::select(&quot;Variable predictora&quot;, &quot;p.value&quot;) %&gt;% arrange(p.value) # A tibble: 13 × 2 `Variable predictora` p.value &lt;chr&gt; &lt;dbl&gt; 1 vestido 2.72e-99 2 limpieza 7.31e-94 3 alimentos 1.50e-77 4 esparcimiento 1.17e-74 5 personales 6.35e-60 6 calzado 5.36e-44 7 transporte 7.09e-44 8 educacion 4.03e-26 9 vivienda 1.66e-18 10 percep_ing 7.67e-18 11 salud 1.87e-10 12 tot_integ 1.37e- 8 13 edad_jefe 3.53e- 1 … … Finalmente, aquí se dejan más enlaces útiles The Discovery of Statistical Regression Shiny: Sum of Squares Ordinary Least Squares Regression Correlación no implica causalidad How to apply and interpret linear regression in R 5.5 Otros puntos importantes Hasta este momento sólo se ha construido un modelo de manera sencilla sin haber considerado factores importantes como la validación de supuestos (lo cual determinará si nuestro modelo tiene veracidad o no), el uso de otro tipo de transformaciones, la comparación de modelos, etc. Todo lo antes mencionado se verá en la regresión múltiple ya que es completamente equivante el enfoque. "],["regresión-lineal-múltiple.html", "Capítulo 6 Regresión lineal múltiple 6.1 Pruebas de hipótesis y ANOVA e intervalos 6.2 Interacción y selección de variables 6.3 Supuestos y problemas potenciales 6.4 Predicción", " Capítulo 6 Regresión lineal múltiple pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } El mundo es complejo y aveces tratar de modelar el comportamiento de un evento sólo por una variable no es lo más conveniente; así que el camino natural para mejorar nuestro modelo de regresión simple es hacer múltiple agregando más variables. Así ya no estaríamos trabajando, geométricamente hablando, en una recta, si no en un hiperplano.Entonces, suponiendo \\(p\\) predictores, una regresión múltiple queda expresada matematicamente como sigue: \\[ f(X) = Y = \\beta_0 +\\beta_1X_1+\\beta_2X_2+\\dots + \\beta_pX_p+\\epsilon = \\beta_0 + \\sum_{i = 1}^pX_i\\beta_i + \\epsilon = X\\beta + \\epsilon \\] Donde \\(\\beta_i\\) son los parámetros del modelo representando el efecto promedio en \\(Y\\) de un incremento de una unidad en \\(X_i\\), manteniendo todos los otros predictores fijos y \\(\\epsilon\\sim N(0,\\sigma^2)\\). De igual manera, la forma habitual de estimar los parámetros es mediante el uso de mínimos cuadrados para minimizar la suma de los residuales al cuadrado: \\[ \\begin{split} RSS &amp;= \\sum_{i = 1}^n(y_i-f(x_i))^2 = \\sum_{i = 1}^n(y_i-\\hat y_i)^2 = \\sum_{i = 1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_{i1}-\\hat\\beta_2x_{i2}-\\cdots-\\hat\\beta_px_{ip})^2\\\\ &amp;= \\sum_{i = 1}^n\\left(y_i-\\beta_0-\\sum_{j = 1}^px_{ij}\\beta_j\\right)^2 \\end{split} \\] Vamos a seguir tomando el ejemplo de los ingresos en el estado de Aguascalientes de acuerdo a la Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). 2018 Nueva serie. Vamos a ajustar un modelo con las variables que tengan la mejor correlación con el ingreso eliminando previamente algunos outliers que se consiguen en ciertas variables. Para facilitarnos esto, veamos el siguiente gráfico de correlación con la correlación de spearman para ver relaciones monotonas y después diagramas de dispersión junto a la correlación de pearson que se consigue por pares. (data_income %&gt;% filter(ing_cor &lt; 2000000 &amp; transporte &lt; 350000 &amp; limpieza &lt; 40000 &amp; personales &lt; 113246) %&gt;% cor(method = &quot;spearman&quot;) %&gt;% ggcorrplot::ggcorrplot(hc.order = TRUE, outline.col = &quot;white&quot;, type = &quot;lower&quot;, tl.cex = 8#, lab = T )+ general_theme + theme(legend.text = element_text(size = 8, face = &quot;plain&quot;), legend.title = element_text(size = 8, face = &quot;plain&quot;), legend.position = c(0.30, 0.85), legend.direction = &quot;horizontal&quot;, legend.box = &quot;horizontal&quot;, panel.background = element_blank())) + (data_income %&gt;% filter(ing_cor &lt; 2000000 &amp; transporte &lt; 350000 &amp; limpieza &lt; 40000 &amp; personales &lt; 113246) %&gt;% dplyr::select(ing_cor, transporte, alimentos, limpieza, personales) %&gt;% GGally::ggscatmat() + general_theme + theme(axis.text = element_text(size = 8), axis.text.x = element_text(angle = 45))) Por lo que proponemos distintas transformaciones; el resultado de esto lo podemos ver en la siguiente matriz de dispersión data_income %&gt;% filter(ing_cor &lt; 2000000 &amp; transporte &lt; 350000 &amp; limpieza &lt; 40000 &amp; personales &lt; 113246) %&gt;% dplyr::select(ing_cor, transporte, alimentos, limpieza, personales) %&gt;% mutate(ing_cor = log(ing_cor), transporte = sqrt(transporte), alimentos = sqrt(alimentos), limpieza = sqrt(limpieza), personales = sqrt(personales)) %&gt;% GGally::ggscatmat() + general_theme Entonces, ajustando un modelo de regresión con los siguientes datos obtenemos el siguiente modelo junto con la implementación de dicho modelo en R: mutated_data_income &lt;- data_income %&gt;% filter(ing_cor &lt; 2000000 &amp; transporte &lt; 350000 &amp; limpieza &lt; 40000 &amp; personales &lt; 113246) #dplyr::select(ing_cor, transporte, alimentos, limpieza, personales) #%&gt;% dplyr::select(ing_cor_log, transporte_sqrt, alimentos_sqrt, limpieza_sqrt, personales_sqrt) first_m_model &lt;- mutated_data_income %&gt;% lm(log(ing_cor)~sqrt(transporte) + sqrt(alimentos) + sqrt(limpieza) + sqrt(personales), data = .) first_m_model %&gt;% summary() Call: lm(formula = log(ing_cor) ~ sqrt(transporte) + sqrt(alimentos) + sqrt(limpieza) + sqrt(personales), data = .) Residuals: Min 1Q Median 3Q Max -3.5269 -0.3109 -0.0043 0.3218 2.6055 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.3424687 0.0351277 265.957 &lt; 2e-16 *** sqrt(transporte) 0.0053498 0.0002970 18.010 &lt; 2e-16 *** sqrt(alimentos) 0.0049731 0.0003956 12.571 &lt; 2e-16 *** sqrt(limpieza) 0.0045630 0.0006180 7.383 2.15e-13 *** sqrt(personales) 0.0054216 0.0005851 9.266 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5266 on 2297 degrees of freedom Multiple R-squared: 0.479, Adjusted R-squared: 0.4781 F-statistic: 528.1 on 4 and 2297 DF, p-value: &lt; 2.2e-16 library(equatiomatic) extract_eq(first_m_model) \\[ \\operatorname{log(ing\\_cor)} = \\alpha + \\beta_{1}(\\operatorname{sqrt(transporte)}) + \\beta_{2}(\\operatorname{sqrt(alimentos)}) + \\beta_{3}(\\operatorname{sqrt(limpieza)}) + \\beta_{4}(\\operatorname{sqrt(personales)}) + \\epsilon \\] O mejor dicho \\[ \\log(ing\\_cor) = \\alpha + \\beta_{1}\\sqrt{transporte} + \\beta_{2}\\sqrt{alimentos} + \\beta_{3}\\sqrt{limpieza} + \\beta_{4}\\sqrt{personales} + \\epsilon \\] Como vemos, con todas las variables se rechaza la hipótesis \\(H_0: \\beta_i = 0\\), además de que también se rechaza la hipótesis \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\\) con la prueba ANOVA y tenemos un \\(R^2\\) ajustado del 0.4781, por lo que el modelo recupera un 47% de la variabilidad de los datos, lo cual no significa que sea un mal modelo. Veamos un poco más a detalle que significa todo esto, además de hacer ver posibles problemas que hagan que nuestro modelo no sea adecuado. 6.1 Pruebas de hipótesis y ANOVA e intervalos Lo primero que nos otorga el summary del modelo anterior es lo siguiente: broom::tidy(first_m_model) # A tibble: 5 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 9.34 0.0351 266. 0 2 sqrt(transporte) 0.00535 0.000297 18.0 6.18e-68 3 sqrt(alimentos) 0.00497 0.000396 12.6 4.24e-35 4 sqrt(limpieza) 0.00456 0.000618 7.38 2.15e-13 5 sqrt(personales) 0.00542 0.000585 9.27 4.30e-20 Con esto obtenemos: Los coeficientes de nuestro modelo (columna estimate) El error estandar sobre para parámetro (columna std.error) El estadístico de la prueba sobre los coeficientes (columna statistic) El \\(p-value\\) sobre la prueba mencionada en el anterior punto (columna p.value) Hay que recordar que el error estandar nos ayuda a crear los intervalos de confianza sobre cada parámetro del modelo. Tomemos como ejemplo el termino \\(\\beta_1\\) asociado a la raíz cuadrada del gasto en transporte. El intervalo de confianza en este caso sería, aproximadamente, el siguiente: \\[ \\hat{\\beta_i}\\pm 2\\cdot SE(\\hat{\\beta_i}) = \\left[\\hat{\\beta_1}-2\\cdot SE(\\hat{\\beta_1}), \\hat{\\beta_1}+2\\cdot SE(\\hat{\\beta_1})\\right] = [0.004755661, 0.005943853] \\] Lo que significa que, hay aproximadamente un 95% de probabilidad de que el intervalo \\([0.004755661, 0.005943853]\\) contiene el verdadero valor de \\(\\beta_1\\). También podríamos haber utilizado el hecho de que suponemos en la construcción de intervalos de confianza que todos los coeficientes se distribuyen de manera normal (\\(\\hat{\\beta_i} = N(\\beta_i, \\sigma^2C_{(i+1)(i+1)})\\) donde \\(C_{(i+1)(i+1)})\\) es el i-ésimo coeficiente de la diagonal de la matriz \\((X&#39;X)^{-1}\\)) y haber calculado los intervalos como: \\[ \\hat{\\beta_i}\\pm 1.96\\cdot SE(\\hat{\\beta_i}) = \\left[\\hat{\\beta_1}-2\\cdot SE(\\hat{\\beta_1}), \\hat{\\beta_1}+1.96\\cdot SE(\\hat{\\beta_1})\\right] = [0.004755661, 0.005943853] \\] Aunque para ser más precisos, los intervalos de confianza, de manera general, están determinados por la siguiente ecuación: \\[ \\hat{\\beta_i}\\pm t_{n-k-1}^{\\alpha/2} \\sqrt{\\hat{\\sigma}^2C_{(i+1)(i+1)}} \\] Para nuestro caso podemos calcularlos de la siguiente manera: sfm &lt;- summary(first_m_model) interval_confidence_firstM &lt;- matrix( c(sfm$coefficients[,1] - qt(0.975, df = sfm$df[2]) * sfm$coefficients[, 2], sfm$coefficients[,1] + qt(0.975, df = sfm$df[2]) * sfm$coefficients[, 2]), ncol = 2 ) row.names(interval_confidence_firstM) &lt;- row.names(sfm$coefficients) colnames(interval_confidence_firstM) &lt;- c(&quot;lower&quot;, &quot;upper&quot;) interval_confidence_firstM lower upper (Intercept) 9.273583316 9.411354045 sqrt(transporte) 0.004767247 0.005932267 sqrt(alimentos) 0.004197295 0.005748825 sqrt(limpieza) 0.003351097 0.005774906 sqrt(personales) 0.004274251 0.006569003 Los cuales se pudieron haber obtenido con la función stats::confint.lm() confint(first_m_model) 2.5 % 97.5 % (Intercept) 9.273583316 9.411354045 sqrt(transporte) 0.004767247 0.005932267 sqrt(alimentos) 0.004197295 0.005748825 sqrt(limpieza) 0.003351097 0.005774906 sqrt(personales) 0.004274251 0.006569003 Una manera elegante que podemos utilizar para visualizar la anterior información no las proporciona la función modelsummary::modelplot() modelplot(first_m_model) + general_theme + theme(panel.background = element_blank()) Por la escala que tenemos en nuestro intercepto, tenemos poca apreciación del resto de coeficientes modelplot(first_m_model, coef_omit = &quot;Intercept&quot;) + general_theme + theme(panel.background = element_blank()) Retomando el ejemplo, podemos decir que en la ausencia de cualquiera de nuestras variables, el ingreso, en promedio, estará entre $10652.86 (=exp(9.273583316)) y 12226.41 pesos mexicanos. Y, por ejemplo, con un aumento de $1,000 pesos en el gasto de alimentos, el ingreso debe aumentar, en promedio para los ciudadanos de Aguascalientes, entre $1000.018 y $1000.033 pesos mexicanos. Respecto a los valores obtenidos en la columna statistic, estos corresponden a un estadístico \\(t\\) para determinar si hay una relación entre la variable asociada a dicho parámetro y la variable a predecir. Para tales fines nuestro estadístico mide el número de desviaciones estandar que nuestro coeficientes lejanas desde el 0, es decir: \\[ t = \\frac{\\hat{\\beta_i}-0}{SE(\\hat{\\beta_i})} \\] Y la prueba de hipótesis en la que se utiliza dicho estadístico queda determinada de la siguiente manera: \\[ \\begin{array}{c} H_0: \\mbox{No hay alguna relación entre }X \\mbox{ y }Y \\equiv \\beta_i = 0\\\\ H_a: \\mbox{Hay alguna relación entre }X \\mbox{ y }Y \\equiv \\beta_i \\neq 0 \\end{array} \\] Entonces lo que buscamos es que \\(\\hat{\\beta_i}\\) este lo más alejado del 0, es decir que si \\(SE(\\hat{\\beta_i})\\) es pequeño, \\(\\hat{\\beta_i}\\) puede ser pequeño y si \\(SE(hat{\\beta_i})\\) es grande, entonces \\(\\hat{\\beta_i}\\) debe ser lo suficientemente grande en valor absoluto. En la siguiente gráfica podemos ver que todas nuestras variables son relevantes para modelar el ingreso (gracias a que los \\(p-values\\) indican de la información no es compatible con la hipótesis nula), así como una comparación entre los valores de cada uno de los coeficientes. GGally::ggcoef_model(first_m_model) + general_theme Por lo que, al considerar que la raíz cuadrada es una función monótona y creciente, al igual que el logaritmo en el rango que estamos considerando (\\(\\mathbb{R^{+}}\\)), los cambios en los gastos personales, en promedio, son lo que más puede afectar en el ingreso si los demás gastos permanecen constantes; de hecho por cada $1,000 más en los gastos personales, se necesitará entre $1000.018 y $1000.043 pesos mexicanos que aumente el ingreso. Lo último que se visualiza en un summary de un modelo lineal son algunas estadísticas sobre el rendimiento del modelo e información sobre la prueba ANOVA, por el momento sólo se analizará esto último. glance(first_m_model) %&gt;% dplyr::select(statistic, p.value, df) # A tibble: 1 × 3 statistic p.value df &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 528. 3e-323 4 La prueba ANOVA (Análisis de Varianza) es un prueba que nos ayudará a determinar la significancia del modelo, es decir que se desea probar si la ecuación de regresión no explica una proporción considerable de la variabilidad en la variable respuesta, contra la hipótesis alternativa de que sí la explica, es decir: \\[ \\begin{array}{c} H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\\\ vs\\\\ H_1 : \\beta_i \\neq 0 \\mbox{ p.a }i, i\\in\\{1,2,\\dots, p\\} \\end{array} \\] El estadístico para esta prueba hace entender el nombre de la prueba y para obtenerlo necesitamos diferentes estadísticas. De acuerdo a la bibliografía se pueden encontrar diferentes nomenclaturas, aquí se colocan algunas ejemplos: \\(TSS = SC_{total} = \\sum(y_i-\\bar{y_i})^2\\): Total sum of squares o la suma de cuadrados total. \\(SC_{reg} = SSM = SSR = \\sum(\\hat{y}_i-\\bar{y})^2\\): La suma de cuadrados de la regresión o del modelo. \\(RSS = SC_{error} = SSE = \\sum(y_i-\\hat{y_i})^2\\): La suma de cuadrados del error o de los residuales La segunda estadística la podemos interpretar como la cantidad de varianza explicada por la regresión, por el modelo o por las variables; mientras la tercera se puede interpretar como la cantidad de varianza que no se explica por la regresión. \\(CM_{reg} = SC_{reg}/p; MSM = MSR = SSM/p\\): Cuadrado medio de la regresión o Mean Square of model/regression \\(CM_{error} = SC_{error}/n-p-1; MSE = SSE/n-p-1\\): Cuadrado medio del error Y con esto ya podemos obtener nuestro estadístico \\(F\\): \\(F = \\frac{CM_{reg}}{CM_{error}}\\) \\(F = \\frac{MSM}{MSE}\\) \\[ F = \\frac{MSR}{MSE} = \\frac{CM_{reg}}{CM_{error}} = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)} \\] Este estadístico tiene una distribución \\(F_{p, n-p-1}\\) y rechazaremos la hipótesis nula cuando \\(F&gt;F^{(\\alpha)}_{k, n-k-1}\\). Si los supuestos del modelo de regresión lineal son validos, se puede ver que \\(\\mathbb{E}[RSS/(n-p-1)] = \\sigma^2\\), es decir la \\(\\sigma^2\\) estimada para todo el modelo, y bajo la hipótesis nula \\(\\mathbb{E}[(TSS-RSS)/p] = \\sigma^2\\). Por lo que cuando NO hay relación entre la variable dependiente y los predictores, se esperaría que el estadístico \\(F\\) sea cercano a 1. Si \\(H_a\\) es cierta \\(\\mathbb{E}[(TSS-RSS)/p] &gt; \\sigma^2\\), por lo que tendríamos valores más grandes que 1 en el estadístico. Entonces podemos interpretar este estadístico como la razón entre la variabilidad explicada por los regresores entre la variabilidad no explicada por el modelo, ponderando con los respectivos grados de libertad que contiene cada estadístico. Por lo que buscamos que nuestro modelo contenga la mayor cantidad de información proporcionada por los datos. Sólo para aclarar, \\(TSS-RSS = SC_{total}-SC_{error} = SC_{reg}\\) y esto es gracias a la siguiente igualdad \\[ SC_{total} = SC_{reg} + SC_{error} \\] La cual proviene de la ley total de la varianza donde los sumandos igual pueden ser interpretados como la cantidad de varianza explicada y no explicada: \\[ Var(Y) = \\mathbb{E}[Var(Y|X)] + Var(\\mathbb{E}[Y|X]) \\] Esto puede ser entendido de manera sencilla con la siguiente gráfica Para resumir la información de dicha prueba tenemos la tabla ANOVA: \\[ \\begin{array}{|c| c| c| c| c|} \\hline &amp;Grados\\ de\\ libertad &amp; Suma\\ de\\ Cuadrados &amp; Cuadrado\\ Medio &amp; Prueba\\ F \\\\ \\hline \\hline Regresión &amp; k &amp; SSR = \\hat{\\beta}&#39;X&#39;\\underline{Y}-n\\overline{y}^2 &amp; MSR = \\frac{SC_{reg}}{k} &amp; \\frac{CM_{reg}}{CM_{error}} \\\\ \\hline Error &amp; n-k-1&amp; SSE = \\underline{Y}&#39;(I-H)\\underline{Y} &amp; MSE = \\frac{SC_{error}}{n-k-1} &amp; -\\\\ \\hline Total &amp; n-1 &amp; TSS = \\underline{Y}&#39;\\underline{Y}-n\\overline{y}^2 &amp; - &amp; - \\\\ \\hline \\end{array} \\] Y en R podemos utilizar la siguiente función con nuestro modelo previamente ajustado anova(first_m_model) Analysis of Variance Table Response: log(ing_cor) Df Sum Sq Mean Sq F value Pr(&gt;F) sqrt(transporte) 1 416.14 416.14 1500.362 &lt; 2.2e-16 *** sqrt(alimentos) 1 108.51 108.51 391.233 &lt; 2.2e-16 *** sqrt(limpieza) 1 37.38 37.38 134.757 &lt; 2.2e-16 *** sqrt(personales) 1 23.81 23.81 85.862 &lt; 2.2e-16 *** Residuals 2297 637.09 0.28 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 m1 &lt;- mutated_data_income %&gt;% lm(ing_cor_log~transporte_sqrt + alimentos_sqrt +limpieza_sqrt + personales_sqrt, data = .) m2 &lt;- mutated_data_income %&gt;% lm(ing_cor_log~ alimentos_sqrt +limpieza_sqrt + personales_sqrt, data = .) anova(m2, m1) Aquí se dejan algunos enlaces que pueden ayudar a aclarar todos los conceptos Lab notes for Statistics for Social Sciences II: Multivariate Techniques :ANOVA and model fit 6.2 Interacción y selección de variables Entre todos los factores que tenemos que considerar en nuestro modelo lineal múltiple, tenemos dos que pueden causar graves problemas: Aditividad y linearidad entre los predictores y la variable respuesta. El concepto (y supuesto) de Linearidad es claro para este momento (1 cambio en X implica un cambio constante en Y), por otro lado hemos utilizado la aditividad en nuestras interpretaciones de los coeficientes. La aditividad significa que el efecto que tiene una variable predictora sobre el modelo es independiente de otra regresora sobre la variable respuesta lo cual podría no suceder. Véamos un ejemplo con la misma base de los ingresos en Aguascalientes. Para esto vamos a considerar un modelo más grande, tomando en cuenta las variables de calzado y vestido. Primero veamos la siguiente gráfica en la cual se plasman los datos ya transformados, considerando los filtros anteriores, agregando la significancia de la variable regresora sobre modelos lineales simples m_calzado &lt;- lm(log(ing_cor)~sqrt(calzado) , data = mutated_data_income) %&gt;% summary() m_vestido &lt;- lm(log(ing_cor)~sqrt(vestido) , data = mutated_data_income) %&gt;% summary() (mutated_data_income %&gt;% ggplot(aes(x = sqrt(calzado), y = log(ing_cor))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + annotate(&quot;text&quot;, x = 70, y = 9, family=&quot;Dosis&quot;, label = paste0(&quot;p-value: &quot;, m_calzado$coefficients[2,4])) + general_theme) + (mutated_data_income %&gt;% ggplot(aes(x = sqrt(vestido), y = log(ing_cor))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(y = NULL) + annotate(&quot;text&quot;, x = 100, y = 9, family=&quot;Dosis&quot;, label = paste0(&quot;p-value: &quot;, m_vestido$coefficients[2,4])) + general_theme) Como vemos, ambas variables variables tiene un p-value muy pequeño para no considerarlas importantes en nuestro modelo, veamos si mejora nuestro modelo con dichas variables y de paso veamos el uso de la función stats::update() update(first_m_model, . ~ .+ sqrt(calzado) + sqrt(vestido), data = mutated_data_income) %&gt;% summary() Call: lm(formula = log(ing_cor) ~ sqrt(transporte) + sqrt(alimentos) + sqrt(limpieza) + sqrt(personales) + sqrt(calzado) + sqrt(vestido), data = mutated_data_income) Residuals: Min 1Q Median 3Q Max -3.4248 -0.3146 -0.0112 0.3175 2.6405 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.3719803 0.0352492 265.878 &lt; 2e-16 *** sqrt(transporte) 0.0052574 0.0002955 17.789 &lt; 2e-16 *** sqrt(alimentos) 0.0046875 0.0003982 11.773 &lt; 2e-16 *** sqrt(limpieza) 0.0040612 0.0006197 6.554 6.90e-11 *** sqrt(personales) 0.0045262 0.0006040 7.493 9.52e-14 *** sqrt(calzado) -0.0001498 0.0007305 -0.205 0.838 sqrt(vestido) 0.0032308 0.0006085 5.309 1.21e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.523 on 2295 degrees of freedom Multiple R-squared: 0.4867, Adjusted R-squared: 0.4854 F-statistic: 362.7 on 6 and 2295 DF, p-value: &lt; 2.2e-16 Comparado con el anterior modelo, nuestra \\(R^2\\) ajustada aumento y sigue siendo válida la prueba ANOVA, pero ¿Qué notamos ahora en nuestro modelo? Al considerar la variable \\(\\sqrt{calzado}\\), esta ya no es importante para el modelo de acuerdo a la prueba \\(H_0: \\beta_i = 0\\). ¿Esto por qué sucede? Al parecer el efecto que tenía la raíz cuadrado del calzado sobre el logaritmo de los ingresos ya no es relevante y esto es porque de alguna manera esta relación queda explicada por la otra variable que agregamos, es decir: \\(\\sqrt{vestido}\\). Lo cual tiene sentido ya que si se hace un gasto en el vestido, es muy probable que sea haga un gasto en el calzado y si se hace un gasto en calzado, seguramente los encuestados consideraron esto en la variable vestido. De hecho, veamos que tan correlacionadas están estas variables mutated_data_income %&gt;% dplyr::select(vestido, calzado, ing_cor) %&gt;% cor() %&gt;% ggcorrplot(type = &quot;lower&quot;, lab = T) + general_theme + theme(legend.text = element_text(size = 8, face = &quot;plain&quot;), legend.title = element_text(size = 8, face = &quot;plain&quot;), legend.direction = &quot;vertical&quot;,legend.position = &quot;right&quot;, panel.background = element_blank()) Entoces, al tener que los aumentos en el calzado aumentaran los del vestido y veceversa, esto viola el supuesto de aditividad en el modelo lineal. Para solucionar esto agregamos una interacción al modelo con estas variables, es decir que agregaremos el término \\(\\beta_iX_{calzado}X_{vestido}\\) al modelo, entonces ahora nuestro modelo sería el siguiente: \\[ \\begin{split} \\log(ing\\_cor) = &amp; \\beta_{0} + \\beta_{1}\\sqrt{transporte} + \\beta_{2}\\sqrt{alimentos} + \\beta_{3}\\sqrt{limpieza} + \\beta_{4}\\sqrt{personales} \\\\&amp;+ \\beta_5\\sqrt{calzado} + \\beta_6\\sqrt{vestido} + \\beta_7\\sqrt{calzado}\\times \\sqrt{vestido} + \\epsilon\\\\ =&amp; \\beta_{0} + \\beta_{1}\\sqrt{transporte} + \\beta_{2}\\sqrt{alimentos} + \\beta_{3}\\sqrt{limpieza} + \\beta_{4}\\sqrt{personales} +\\\\&amp; \\widetilde{\\beta_5}\\sqrt{calzado} + \\beta_6\\sqrt{vestido} + \\epsilon \\end{split} \\] En la última expresión suponemos que \\(\\widetilde{\\beta_5} = \\beta_5+\\beta_7\\sqrt{X_{vestido}}\\) y así podemos seguir viendo nuestro modelo “sin interacciones,” sólo que ahora un cambio en el calzado tendrá un efecto en el vestido y veceversa Para agregar una interacción entre variables, utilizamos var1:var2 en la formula. Veamos el summary que obtenemos de nuestro nuevo modelo second_m_model &lt;- update(first_m_model, . ~ .+ sqrt(calzado) + sqrt(vestido) + sqrt(calzado):sqrt(vestido), data = mutated_data_income) second_m_model %&gt;% summary() Call: lm(formula = log(ing_cor) ~ sqrt(transporte) + sqrt(alimentos) + sqrt(limpieza) + sqrt(personales) + sqrt(calzado) + sqrt(vestido) + sqrt(calzado):sqrt(vestido), data = mutated_data_income) Residuals: Min 1Q Median 3Q Max -3.3987 -0.3134 -0.0097 0.3153 2.6469 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.351e+00 3.592e-02 260.359 &lt; 2e-16 *** sqrt(transporte) 5.253e-03 2.951e-04 17.802 &lt; 2e-16 *** sqrt(alimentos) 4.568e-03 3.997e-04 11.431 &lt; 2e-16 *** sqrt(limpieza) 4.136e-03 6.192e-04 6.680 3.00e-11 *** sqrt(personales) 4.578e-03 6.033e-04 7.588 4.68e-14 *** sqrt(calzado) 1.467e-03 9.206e-04 1.594 0.11108 sqrt(vestido) 4.718e-03 7.974e-04 5.916 3.79e-09 *** sqrt(calzado):sqrt(vestido) -5.275e-05 1.832e-05 -2.879 0.00403 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5221 on 2294 degrees of freedom Multiple R-squared: 0.4886, Adjusted R-squared: 0.487 F-statistic: 313.1 on 7 and 2294 DF, p-value: &lt; 2.2e-16 Es evidente, bajo una significancia del 5% que la ponderación de la interacción es estadísticamente diferente de cero, dándole un peso relevante a nuestro modelo. Con esto podemos decir que por cada $100 pesos de aumento en el calzado, tendremos un aumento de \\((\\beta_5+\\beta_7\\sqrt{X_{vestido}})\\times 10 = (0.001467 + 0.004718\\times X_{vestido})\\times 10 = 0.01467 + 0.04718\\times X_{vestido}\\) unidades sobre el logaritmo de los ingresos. Véase que seguimos viendo que \\(\\sqrt{calzado}\\) sigue sin ser relevante.entonces ¿Por qué no consideramos un modelo sin esta variable? Para evitar inconsistencias en los modelos, si se realiza una interacción, en el modelo se debe tener presente las variables de la interacción, a esto se le llama principio jerárquico, independientemente si el coeficiente de alguna de las variables es estadísticamente 0 o no. ¿Qué pasaría por ejemplo si omitimos el calzado en nuestro modelo? en tal caso, si hay un aumento en calzado pero no hay presencial del gasto en vestido \\(\\beta_7\\sqrt{calzado\\times vestido} = 0\\) cuando debe ser así ya que realmente hubo un aumento en al menos una de las variables. Además de cumplir ahora el supuesto de aditividad, véase que nuestro modelo mejoro en comparación del anterior. second_m_model %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Nuevo modelo&quot;) %&gt;% inner_join( first_m_model %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Anterior modelo&quot;), by = &quot;Estadística&quot; ) # A tibble: 12 × 3 Estadística `Nuevo modelo` `Anterior modelo` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 r.squared 0.489 4.79e- 1 2 adj.r.squared 0.487 4.78e- 1 3 sigma 0.522 5.27e- 1 4 statistic 313. 5.28e+ 2 5 p.value 0 3 e-323 6 df 7 4 e+ 0 7 logLik -1767. -1.79e+ 3 8 AIC 3551. 3.59e+ 3 9 BIC 3603. 3.62e+ 3 10 deviance 625. 6.37e+ 2 11 df.residual 2294 2.30e+ 3 12 nobs 2302 2.30e+ 3 Con este nuevo modelo aumento el \\(R^2\\) ajustada, el \\(p-value\\) de la prueba ANOVA indica que nuestras variables son significativas y las estadísticas AIC y BIC disminuyeron. La interacción con datos categóricos cambia un poco en el sentido de que puede ser más fácil identificar dichas interacciones. Considerando el AIC, por ejemplo, podemos utilizar algunas técnicas para elegir el mejor modelo, de acuerdo a una estadística como esta, omitiendo diferentes variables al modelo o agregándolas. Estas técnicas son llamadas Forward Selection, Backward Selection y Mixed Selection en los cuales simplemente se comienza con un modelo base y se van agregando o eliminando variables de acuerdo a la significancia que se vaya obteniendo con los coeficientes o con otro criterio como el AIC. Veamos como aplicarlo a nuestro modelo best_second_model &lt;- second_m_model %&gt;% MASS::stepAIC(direction = &quot;backward&quot;) Start: AIC=-2983.78 log(ing_cor) ~ sqrt(transporte) + sqrt(alimentos) + sqrt(limpieza) + sqrt(personales) + sqrt(calzado) + sqrt(vestido) + sqrt(calzado):sqrt(vestido) Df Sum of Sq RSS AIC &lt;none&gt; 625.42 -2983.8 - sqrt(calzado):sqrt(vestido) 1 2.260 627.68 -2977.5 - sqrt(limpieza) 1 12.164 637.58 -2941.4 - sqrt(personales) 1 15.699 641.11 -2928.7 - sqrt(alimentos) 1 35.621 661.04 -2858.3 - sqrt(transporte) 1 86.397 711.81 -2687.9 summary(best_second_model) Call: lm(formula = log(ing_cor) ~ sqrt(transporte) + sqrt(alimentos) + sqrt(limpieza) + sqrt(personales) + sqrt(calzado) + sqrt(vestido) + sqrt(calzado):sqrt(vestido), data = mutated_data_income) Residuals: Min 1Q Median 3Q Max -3.3987 -0.3134 -0.0097 0.3153 2.6469 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.351e+00 3.592e-02 260.359 &lt; 2e-16 *** sqrt(transporte) 5.253e-03 2.951e-04 17.802 &lt; 2e-16 *** sqrt(alimentos) 4.568e-03 3.997e-04 11.431 &lt; 2e-16 *** sqrt(limpieza) 4.136e-03 6.192e-04 6.680 3.00e-11 *** sqrt(personales) 4.578e-03 6.033e-04 7.588 4.68e-14 *** sqrt(calzado) 1.467e-03 9.206e-04 1.594 0.11108 sqrt(vestido) 4.718e-03 7.974e-04 5.916 3.79e-09 *** sqrt(calzado):sqrt(vestido) -5.275e-05 1.832e-05 -2.879 0.00403 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5221 on 2294 degrees of freedom Multiple R-squared: 0.4886, Adjusted R-squared: 0.487 F-statistic: 313.1 on 7 and 2294 DF, p-value: &lt; 2.2e-16 En este caso tenemos un bueno modelo ya que algorítmicamente si eliminamos alguna de las variables, perderíamos información con el subsequente modelo (esto mediante el AIC). Sólo para ejemplificar, veáse que pasaría con nuestros datos originales model_zero &lt;- lm(ing_cor~., data = mutated_data_income) %&gt;% stepAIC(direction = &quot;backward&quot;) Start: AIC=48539.95 ing_cor ~ edad_jefe + tot_integ + percep_ing + alimentos + vestido + calzado + vivienda + limpieza + salud + transporte + personales + educacion + esparcimiento Df Sum of Sq RSS AIC - tot_integ 1 1.8169e+09 3.2703e+12 48539 &lt;none&gt; 3.2685e+12 48540 - calzado 1 4.8737e+09 3.2734e+12 48541 - educacion 1 8.1884e+09 3.2767e+12 48544 - vestido 1 8.6011e+09 3.2771e+12 48544 - vivienda 1 1.0226e+10 3.2787e+12 48545 - edad_jefe 1 2.4454e+10 3.2930e+12 48555 - salud 1 3.5137e+10 3.3037e+12 48563 - percep_ing 1 5.0741e+10 3.3193e+12 48573 - personales 1 8.5543e+10 3.3541e+12 48597 - limpieza 1 8.8024e+10 3.3565e+12 48599 - esparcimiento 1 8.9726e+10 3.3582e+12 48600 - transporte 1 1.5210e+11 3.4206e+12 48643 - alimentos 1 1.7074e+11 3.4393e+12 48655 Step: AIC=48539.23 ing_cor ~ edad_jefe + percep_ing + alimentos + vestido + calzado + vivienda + limpieza + salud + transporte + personales + educacion + esparcimiento Df Sum of Sq RSS AIC &lt;none&gt; 3.2703e+12 48539 - calzado 1 5.6047e+09 3.2759e+12 48541 - educacion 1 7.5215e+09 3.2779e+12 48543 - vestido 1 9.2882e+09 3.2796e+12 48544 - vivienda 1 1.0559e+10 3.2809e+12 48545 - edad_jefe 1 2.8199e+10 3.2985e+12 48557 - salud 1 3.5594e+10 3.3059e+12 48562 - percep_ing 1 6.8639e+10 3.3390e+12 48585 - personales 1 8.4152e+10 3.3545e+12 48596 - limpieza 1 8.9924e+10 3.3603e+12 48600 - esparcimiento 1 9.5439e+10 3.3658e+12 48603 - transporte 1 1.5196e+11 3.4223e+12 48642 - alimentos 1 1.6893e+11 3.4393e+12 48653 Véase que con todos los datos y sin ninguno tipo de limpieza el AIC determinado por el segundo modelo (se elimino la variable tot_integ) es mejor. Un punto importante que hay que remarcar es que este modelo no significa que sea el mejor que podamos obtener, de hecho nuestro último modelo (second_model) es hasta ahora el mejor. Por razones que se explorarán posteriormente, vamos a eliminar la interacción y las variables calzada y vestido ya que el modelo es mejor sin estas variables (es mejor en los supuestos, lo cual nos importará más que obtener el mejor AIC o mejor \\(R^2\\)). 6.3 Supuestos y problemas potenciales Antes de pasar con los supuestos del modelo lineal, véamos un poco más a detalle las transformaciones más comunes para obtener un comportamiento lineal entre nuestros predictores y la variable respuesta. 6.3.1 +Transformaciones Hasta el momento se han mencionado algunas transformaciones que nos pueden ayudar a conseguir linealidad entre dos variables, por ejemplo cuando deseamos reducir la variabilidad de la variable a predecir o cambiar la escala de la variable predictora: \\(log()\\): Función monótona creciente definida para los positivos e indefinida para el 0. \\(sqrt()\\): Función monótona creciente definida para los positivos y el 0. \\(1/x\\): Función monótona creciente definida para los positivos e indefinida para el 0. De acuerdo al comportamiento de los datos podemos aplicar cualquiera de estas variables siempre que este bien definida la función. ¿Qué podemos hacer cuando tenemos ceros? ¿Qué otras propuestas comunes existen? Para el primer punto hay que pensar que los ceros pueden tener diferentes significados en nuestras variables, en una pregunta que se realizo en StackExchange: How should I transform non-negative data including zeros? se porponen diferentes puntos de vista validos que hay que pensar sobre los ceros en nuestros datos: Truncamiento: Puede ser que un modelaje para los datos sea un modelo de mezclas, un modelo de supervivencia, etc. Valores perdidos: Es natural que esto puedan ser valores perdidos y no un valor obtenido de la variable, por lo que se puede imputar información o eliminarla si se considera apropiado. Punto cero natural: Por ejemplo con niveles de ingreso (puede tratarse de una situación de desemepleo) y en tal caso una transformación sería lo indicado. Sensibilidad del instrumento de medición: Una solución podría ser agregar un pequeño valor a la información. Sea cual sea el caso, podemos eliminarlos si no perdemos una gran cantidad de información o usar dichos datos como un segregador de la información. También podemos utilizar una transformación donde no existan problemas en este valor específico y una de las más populares es una de las parametrizaciones de la familia de transformaciones Box-Cox: \\[ g_{\\lambda}(y) = \\Bigg\\{\\begin{array}{rl}\\frac{y^{\\lambda}-1}{\\lambda} &amp; \\lambda \\neq 0\\\\ \\log(y) &amp; \\lambda = 0\\end{array} \\] Donde la función inversa es: \\[ y = \\Bigg\\{\\begin{array}{cc}\\exp\\left(\\frac{\\log(1+\\lambda g_{\\lambda}(y))}{\\lambda}\\right) &amp; \\lambda \\neq 0 \\\\ \\exp(g_{\\lambda}(y) &amp; \\lambda = 0\\end{array} \\] Y para diferentes valores de \\(\\lambda\\) tenemos diversas funciones asociadas: \\(\\lambda = 1\\): No se requiere alguna transformación ya que se producen resultados idénticos a los originales y no se alteraría la variabilidad \\(\\lambda = 0.50\\): Raíz cuadrada \\(\\lambda = 0.33\\): Raíz cúbica \\(\\lambda = 0.25\\): Raíz cuarta \\(\\lambda = 0\\): Logaritmo natural \\(\\lambda = -0.50\\): raíz cuadrada recíproca \\(\\lambda = -1\\): Transformación inversa La obtención de \\(\\lambda\\) es mediante el método de máxima verosimilitud para obtener el valor más probable que haya dado lugar a la distribución observada de los residuos del modelo \\[ \\log\\mathbb{L} = -\\frac{n}{2}\\log(2\\pi) - n\\log \\sigma - \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n\\left[g_{\\lambda}(y)-(\\beta_0+\\beta X)\\right]^2 + (\\lambda-1)\\sum_{i = 1}^n \\log Y_i \\] Este es obtenido de manera numérica y en general se elige un valor en el intervalo de confianza para dicho parámetro \\[ \\left\\{\\lambda: \\mathbb{L}(\\lambda) &gt; \\mathbb{L}(\\hat{\\lambda}-\\frac{1}{2}\\chi_{1,\\alpha}^2)\\right\\} \\] Tal información la podemos obtener de la función MASS::boxcox() la cual proporciona un gráfica donde podemos apreciar el intervalo de confianza y la verosimilitud para diferentes valores de \\(\\lambda\\). Como argumento le pasamos a la función el modelo de regresión que hemos creado. Es común que esta transformación es aplicada a la variable respuesta, ya que al hacer esto generalmente se reduce la no normalidad de los residuales, al igual que la no linearidad (relacionado con la independencia de estos) y se mejoran problemas de homocedasticidad. Con nuestros datos, se decidió eliminar las observaciones donde se tengan ceros en las variables predictoras (ya que estos representan menos del 10% de información y, se puede comprobar, dejando dichas observaciones se obtienen diversos problemas con las pruebas de hipótesis y el análisis de valores influyentes). También se cambiaron algunas transformaciones ya que, al no tener valores que afecten el logaritmo se mejoró la relación lineal de algunos predictores con la variable respuesta filter_data &lt;- data_income_sexo %&gt;% filter(ing_cor &lt; 2000000 &amp; transporte &lt; 350000 &amp; limpieza &lt; 40000 &amp; personales &lt; 113246) %&gt;% filter(transporte != 0 &amp; limpieza != 0 &amp; personales != 0 &amp; alimentos != 0) %&gt;% dplyr::select(ing_cor, transporte, alimentos, limpieza, personales) third_model &lt;- lm(ing_cor~log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = filter_data) boxCox_third_model &lt;- boxcox(third_model, plotit = TRUE, #Deseamos ver la gráfica lambda = seq(-0.2, 0.2, by = 0.1) # Con esto modificamos el rango ) Entonces no estábamos mal al proponer al logaritmo en el ingreso como una transformación que ayudaría a nuestro modelo lineal. Podemos obtener el valor exacto en el que se maximiza la verosimilitud (lambdaBCox_tM &lt;- boxCox_third_model$x[which.max(boxCox_third_model$y)]) [1] 0.02222222 Es decir, esto nos sugiere hacer un modelo con la siguiente variable repuesta \\[ \\frac{y^{\\lambda}-1}{\\lambda} = \\frac{y^{0.022}-1}{0.022} \\] Y con esto tendríamos el siguiente summary del correspondiente modelo: third_model_BX &lt;- lm(((ing_cor^lambdaBCox_tM-1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = filter_data) third_model_BX %&gt;% summary() Call: lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = filter_data) Residuals: Min 1Q Median 3Q Max -4.3384 -0.4054 -0.0169 0.3843 3.3192 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.9124443 0.1361592 50.767 &lt;2e-16 *** log(transporte) 0.2564575 0.0142296 18.023 &lt;2e-16 *** sqrt(alimentos) 0.0060496 0.0005121 11.813 &lt;2e-16 *** log(limpieza) 0.1503380 0.0180963 8.308 &lt;2e-16 *** log(personales) 0.1762014 0.0186708 9.437 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6418 on 2139 degrees of freedom Multiple R-squared: 0.4886, Adjusted R-squared: 0.4876 F-statistic: 510.9 on 4 and 2139 DF, p-value: &lt; 2.2e-16 ¿Cómo se compara con nuestro anterior mejor modelo? second_m_model %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Viejo modelo&quot;) %&gt;% inner_join( third_model_BX %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Nuevo modelo&quot;) %&gt;% dplyr::mutate_if(is.numeric, round, digits = 8), by = &quot;Estadística&quot; ) # A tibble: 12 × 3 Estadística `Viejo modelo` `Nuevo modelo` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 r.squared 0.489 0.489 2 adj.r.squared 0.487 0.488 3 sigma 0.522 0.642 4 statistic 313. 511. 5 p.value 0 0 6 df 7 4 7 logLik -1767. -2089. 8 AIC 3551. 4190. 9 BIC 3603. 4224. 10 deviance 625. 881. 11 df.residual 2294 2139 12 nobs 2302 2144 Si bien parece ser mejor el nuevo modelo por centésimas en el \\(R^2\\) ajustado, otras estadísticas como el logLik, IAC y BIC indican que nuestro modelo no mejoro y pero es relevante ver si mejora este modelo omitiendo outliers y valores influyentes. Otras propuestas son las transformación Box-Cox que permiten un desplazamiento en los datos y la transformación inversa del seno hiperbólico. \\[ g_{\\lambda_1, \\lambda_2}(y) = \\Bigg\\{\\begin{array}{rl}\\frac{(y+\\lambda_2)^{\\lambda_1}-1}{\\lambda_1} &amp; \\lambda_1 \\neq 0\\\\ \\log(y + \\lambda_2) &amp; \\lambda_1 = 0\\end{array} \\] \\[ \\begin{array}{lr} f(y, \\theta) = \\sinh^{-1}(\\theta y)/\\theta = \\log\\left(\\theta y + (\\theta^2y^2+1)^{1/2}\\right)/\\theta &amp; ;\\theta&gt;0 \\end{array} \\] Aquí se dejan otros enlaces interesantes sobre la transformación Box-Cox Transforming data with zeros 14.1.2 Box-Cox Transformations Box-cox transformation recipes: Box-Cox Transformation for Non-Negative Data A new approach to the Box–Cox transformation Statistics How To: Box Cox Transformation OnlineStatBook Project Home: Box-Cox Transformations Box-Cox Transformations: An Overview 6.3.2 Interpretaciones Es importante hacer ver que al modificar alguna de las variables con alguna transformación, ya sea la variable respuesta o alguna de las variables independientes, también se modifica la interpretación de todo. Generalmente podemos aplicar transformaciones inversas para dar una interpretación de nuestras variables originales, hay que tener en cuenta ciertos detalles con algunas funciones. Logaritmo Raíz cuadrada 6.3.3 Validación Para que nuestros resultados tengan validez, necesitamos cumplir una serie de supuestos y es recomendable verificar algunos problemas potenciales. De lo principal que debemos verificar en cada modelo es lo siguiente: Linearidad: La respuesta puede ser escrita como una combinación lineal de los predictores: \\(\\mathbb{E}[Y|X_1 = x_1, \\dots, X_k = x_k] = \\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k\\). Homocedasticidad: La varianza de los errores es la misma en cualquier conjunto de los valores predichos: \\(\\mathbb{V}ar(\\epsilon_i) = \\sigma^2\\), con \\(\\sigma^2\\) constante para \\(i = 1, \\dots, n\\). Normalidad: La distribución de los errores deben seguir una distribución normal: \\(\\epsilon_i \\sim \\mathbb{N}(0,\\sigma^2)\\) para \\(i = 1, \\dots, n\\). Independencia: Los errores son independientes: \\(\\epsilon_1, \\dots, \\epsilon_n\\) son independientes o \\(\\mathbb{E}[\\epsilon_i\\epsilon_j] = 0, i\\neq j\\), es decir, que los errores no estan correlacionados ya que asumimos que son normales. Aditividad: El efecto que tiene una variable predictora sobre el modelo es independiente de otra regresora sobre la variable respuesta. En caso de tener problemas con esto, podríamos tener problemas de multicolinealidad. Este último es considerado como supuesto del modelo en los libros James et al. (2013) y Friedman et al. (2001) pero es común que sólo los primeros 4 sean considerados los supuestos del modelo. Si bien es cierto que es ideal que todos nuestros supuestos se cumplan, podemos dar cierta tolerancia a ellos por su Robustez. El libro Gelman and Hill (2006) enlista los anteriores supuestos en orden de importancia (de manera descendente) y el libro Ramsey and Schafer (2012) se dan algunos puntos importantes sobre esto. Validez: La información debe ser adecuada para responder tu pregunta objetivo Aditividad y linearidad: El supuesto matemático más importante del modelo de regresión es que su componente determinista es una función lineal de los predictores separados. Esto puede ser corregido con diferentes transformaciones. Este supuesto puede ser violado cuando la recta de ajuste entre los datos no es recta (una curva por ejemplo) o cuando la cantidad y fuerza de los valores atípicos no permiten que el modelo sea adecuado. Ambas violaciones pueden hacer que las estimaciones de mínimos cuadrados den respuestas engañosas a las preguntas de interés. Las medias y las predicciones estimadas pueden estar sesgadas (subestiman o sobrestiman sistemáticamente la cantidad prevista) y las pruebas y los intervalos de confianza pueden reflejar de forma inexacta la incertidumbre Independencia de los errores: La falta de independencia no provoca sesgos en las estimaciones de mínimos cuadrados de los coeficientes, pero los errores estándar se ven seriamente afectados. Este problema puede ser dado, por ejemplo, cuando los individuos en un estudio pertenecen a un mismo grupo o comparten características similares que no se consideraron como que tenga la misma dieta, sean de la misma familia o hayan sido expuestos a las mismas condiciones ambientales. Homocedasticidad: Si la varianza de los errores de regresión es desigual, la estimación se realiza de manera más eficiente utilizando mínimos cuadrados ponderados, donde cada punto se pondera inversamente proporcional a su varianza. En la mayoría de los casos, sin embargo, este problema es menor. La varianza desigual no afecta el aspecto más importante de un modelo de regresión, que es la forma del predictor \\(X\\beta\\). Las consecuencias de violar este supuesto son las mismas que las del análisis de varianza unidireccional. Aunque las estimaciones de mínimos cuadrados siguen siendo insesgadas incluso si la varianza no es constante, los errores estándar describen de manera inexacta la incertidumbre en las estimaciones. Las pruebas y los intervalos de confianza pueden ser engañosos. Normalidad: El supuesto de regresión que generalmente es menos importante es que los errores se distribuyen normalmente. De hecho, para estimar la línea de regresión (en comparación con la predicción de puntos de datos individuales), el supuesto de normalidad apenas tiene importancia. Las estimaciones de los coeficientes y sus errores estándar son robustas a distribuciones no normales. Aunque las pruebas y los intervalos de confianza se originan en distribuciones normales, las consecuencias de violar este supuesto suelen ser menores. La única situación de gran preocupación es cuando las distribuciones tienen colas largas (existen valores atípicos) y los tamaños de muestra son de moderados a pequeños. Si se utilizan intervalos de predicción, por otro lado, las desviaciones de la normalidad se vuelven importantes. Esto se debe a que los intervalos de predicción se basan directamente en la normalidad de las distribuciones de la población, mientras que las pruebas y los intervalos de confianza se basan en la normalidad de las distribuciones muestrales de las estimaciones (que pueden ser aproximadamente normales incluso cuando las distribuciones de la población no lo son). Por lo que en este punto, si tenemos una buena cantidad de datos, podemos ser flexibles con este último supuesto. La manera más sencilla de ver la mayoría de estos supuestos es mediante una gráfica de residuales vs los valores ajustados donde, para la linealidad deseamos ver que los residuales tengan un comportamiento aleatorio con una varianza constante y media cero, lo cual implicitamente también ayuda con la homocedasticidad y normalidad. Esta gráfica y otras no las proporciona el comando plot() para un modelo lineal par(mfrow = c(2, 2)) third_model_BX %&gt;% plot() O bien con la función ggfortify::autoplot() library(ggfortify) third_model_BX %&gt;% autoplot() Para más detalles de estas gráficas véase la sección de Análisis de valores influyentes. Finalmente aplicamos diversas pruebas para probar cada uno de los supuestos: Normalidad: Todas las pruebas que se vieron anteriormente. third_model_BX %&gt;% residuals() %&gt;% shapiro.test() Shapiro-Wilk normality test data: . W = 0.98185, p-value = 6.856e-16 Homocedasticidad: Prueba de Breusch-Pagan: lmtest::bptest() Prueba de White: lmtest::bptest() library(lmtest) third_model_BX %&gt;% bptest() studentized Breusch-Pagan test data: . BP = 6.2836, df = 4, p-value = 0.1789 Autocorrelación (independencia): Durbin Watson: lmtest::dwtest() Breusch-Godfrey Test: lmtest::bgtest() dwtest(third_model_BX) Durbin-Watson test data: third_model_BX DW = 1.8388, p-value = 9.232e-05 alternative hypothesis: true autocorrelation is greater than 0 bgtest(third_model_BX) Breusch-Godfrey test for serial correlation of order up to 1 data: third_model_BX LM test = 13.716, df = 1, p-value = 0.0002126 The Independence Assumption Pruebas de independencia de los errores Durbin Watson Test &amp; Test Statistic How to Perform a Breusch-Godfrey Test in R Colinearidad/Multicolinealidad Una manera sencilla para detecetar la colinearidad es ver un correlograma y tomar aquellas variables que tengan fuerte relación entre ellas. Así sabiendo que una variable explica a otra, podríamos omitirla del modelo y determinar si esto mejora nuestro rendimiento, o bien crear una nueva variable. Otra solución en caso de colinearidad podría ser una interacción como ya se vio previamente. El problema con lo anterior es que no todos los problemas de colinearidad pueden ser detectados mediante un correlograma, ya que puede existe una relación entre tres o más variables sin que sea visible tomándolo por pares. A lo anterior es lo que llamaremos multicolinearidad Lo recomendable, y más sencillo, es visualizar el factor de inflación de varianza: VIF el cual es la razón de la varianza de \\(\\beta_j\\) cuando \\[ VIF\\left(\\hat{\\beta}_i\\right) = \\frac{1}{1-R^2_{X_j|X_{-j}}} \\] Donde \\(R^2_{X_j|X_{-j}}\\) es la \\(R^2\\) de la regresión de \\(X_j\\) a todos los demás predictores; es decir, la proporción de variabilidad en \\(X_j\\) que es explicada por su relación lineal a las otras variables del modelo. Cuando \\(X_j\\) puede ser bien explicada por la adicción de las demas variables; es decir cuando \\(R^2_{X_j|X_{-j}}\\) es cercano a 1, entonces el valor \\(VIF\\) será grande e indicaría problas de colinearidad. Buscamos valores pequeños y lo más pequeño que se puede obtener para cada variable es un \\(VIF = 1\\) lo cual indica una completa ausencia de colinearidad. Por regla de dedo, un \\(VIF\\) mayor a 5 o 10 indica problemas de colinearidad. En R podemos obtener el \\(VIF\\) para cada variable con la función faraway::vif() library(faraway) third_model_BX %&gt;% vif() log(transporte) sqrt(alimentos) log(limpieza) log(personales) 1.409547 1.469732 1.560714 1.649865 No tenemos problemas de colinearidad. ¿Qué sucedió con nuestro segundo modelo? second_m_model %&gt;% residuals() %&gt;% shapiro.test() Shapiro-Wilk normality test data: . W = 0.98339, p-value = 9.574e-16 second_m_model %&gt;% bptest() studentized Breusch-Pagan test data: . BP = 51.355, df = 7, p-value = 7.823e-09 second_m_model %&gt;% bgtest() Breusch-Godfrey test for serial correlation of order up to 1 data: . LM test = 14.285, df = 1, p-value = 0.0001571 second_m_model %&gt;% vif() sqrt(transporte) sqrt(alimentos) 1.475002 1.546820 sqrt(limpieza) sqrt(personales) 1.631803 1.816380 sqrt(calzado) sqrt(vestido) 2.500522 2.971420 sqrt(calzado):sqrt(vestido) 4.387703 Invalidamos el supuesto de homocedasticidad que ya habíamos conseguido con nuestro tercer modelo, por lo que preferiremos el tercer modelo desde ahora. Introduction to Econometrics with R: OLS Assumptions in Multiple Regression Lab notes for Statistics for Social Sciences II: Multivariate Techniques: 3.3 Assumptions of the model Modelos de Regresión con R: 9. Diagnósticos parte I Applied Statistics with R: Chapter 13 Model Diagnostics Lab notes for Statistics for Social Sciences II: Multivariate Techniques: 3.8 Model diagnostics and multicollinearity Regression Model Diagnostics 6.4 Predicción Uno de los puntos más objetivos principales de crear un modelo de regresión es predecir un valor futuro de acuerdo a diferentes valores de nuestras variables predictoras. Esto es sencillo teniendo ya el modelo. Vamos a suponer que nuestro último modelo es adecuado (ya que aún no hemos tratado el tema de validación de supuestos, los cuales en predicción son vitales para su credibilidad) En R podemos utilizar el comando predict() para obtener fácilmente las predicciones de nueva información sobre un modelo. En esta caso vamos a realizar un ejemplo con regresión simple ya que es análogo para el modelo de regresión múltiple. new_data &lt;- data.frame(vestido = 100) simple_model &lt;- lm(log(ing_cor)~sqrt(vestido), data = mutated_data_income %&gt;% filter(vestido&lt;7500 &amp; vestido&gt;0)) simple_model %&gt;% predict(new_data) 1 10.42712 Véase que R se encarga de las transformaciones en las variables predictoras (en este caso el modelo queda expresado como \\(y = 10.26570 + 0.01614\\times \\sqrt{X_{vestido}}\\)) \\(10.26570 + 0.01614\\times100 = 11.8797\\) \\(10.26570 + 0.01614\\times\\sqrt{100} = 10.42712\\) Entonces, en este caso podemos decir que con un gasto de $100 mexicanos en vestido (con esta base de datos), un ciudadano de Aguascalientes, en promedio tiene que ganar \\(e^{10.26570}\\times e^{(0.01614\\times\\sqrt(100)} = exp(10.42712) = \\$33762.97\\) pesos mexicanos. La misma función predict() nos permite obtener los intervalos de confianza para la respuesta media y los intervalos de predicción con el argumento interval = c(\"confidence\", \"prediction\") ¿Cuál es la diferencia entre estos? Como sabemos, nuestro modelo sólo es una representación de la realidad ya que mediante algún método obtenemos estimaciones sobre los parámetro (los \\(\\beta_i\\)s); es decir, sólo estamos obteniendo una estimación del verdadero plano poblacional de regresión \\[ f(X) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_{p}X_p \\] con nuestro plano obtenido por el método de mínimos cuadrados \\[ \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta_1}X_1 + \\cdots + \\hat{\\beta}_{p}X_p \\] en el cual controlamos un error que se puede reducir. Sobre dicha respuesta promedio es donde tenemos nuestros intervalos de confianza. Por ejemplo, si nosotros recolectamos distintas bases de datos con las variables que tenemos en nuestro último modelo del ingreso en Aguascalientes, digamos 100, y obtenemos el intervalo de confianza sobre el promedio del gasto en cada una de esas bases (con las variables explicativas fijas), 95% de dichos intervalos contendrán el verdadero valor promedio del gasto (la respuesta media en este caso). Recordemos que este valor promedio es lo que tenemos con la evaluación de un vector X conocido. En el caso del intervalo de predicción, estamos haciendo inferencia sobre el error que no controlamos en el modelo (\\(\\epsilon\\)), por lo que es común que el intervalo de predicción sea más amplio que los intervalos de confianza al incorporar esta incertidumbre. Si bien, al igual que en el intervalo de confianza estaríamos dando un intervalo para un valor de la respuesta, en este caso es de manera específica y no sobre una aproximación hacia la población. Entonces, sí se desea dar un intervalo para un punto en particular, utilizaremos los intervalos de predicción y si deseamos hablar de los valores medios de predicción, usaríamos un intervalo de confianza. new_data &lt;- data.frame(vestido = 100, 200, 150) simple_model %&gt;% predict(new_data, interval = &quot;confidence&quot;) fit lwr upr 1 10.42712 10.37179 10.48246 simple_model %&gt;% predict(new_data, interval = &quot;prediction&quot;) fit lwr upr 1 10.42712 9.206192 11.64806 En este caso de una regresión lineal simple podemos visualizar mejor dichos intervalos pred_int &lt;- predict(simple_model, interval = &quot;prediction&quot;) mutated_data_income %&gt;% filter(vestido&lt;7500 &amp; vestido&gt;0) %&gt;% bind_cols(pred_int %&gt;% as_tibble()) %&gt;% ggplot(aes(x = sqrt(vestido), y = log(ing_cor))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + #Intervalos de confianza geom_line(aes(y = lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + #intervalos de predicción geom_line(aes(y = upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + general_theme References "],["otros-puntos-importantes-1.html", "Capítulo 7 Otros puntos importantes 7.1 Comparación de modelos 7.2 Análisis de valores influyentes", " Capítulo 7 Otros puntos importantes pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } Si se específica el tipo de regresión, ¿Hay otras técnicas de modelaje? ¡Claro! existen muchísimas técnicas de modelaje, en cuanto a la regresión tenemos otros tipos además de la lineal, como la regresión logística, poisson, polinómica, splines, etc. ¿El modelo es lineal en que sentido? Es lineal en sus coeficientes, es decir que los siguientes modelos también son lineales: \\[ \\begin{array}{c} \\mu\\left\\{Y|X_1, X_2\\right\\} = \\beta_0+\\beta_1X_1+\\beta_2X_2\\\\ \\mu\\left\\{Y|X_1\\right\\} = \\beta_0+\\beta_1X_1+\\beta_2X_1^2\\\\ \\mu\\left\\{Y|X_1, X_2\\right\\} = \\beta_0+\\beta_1X_1+\\beta_2X_1^2+\\beta_3X_1X_2\\\\ \\mu\\left\\{Y|X_1, X_2\\right\\} = \\beta_0+\\beta_1\\log(X_1)+\\beta_2\\log(X_2) \\end{array} \\] No hay que olvidar toda la teoría que esta por detrás de tal modelo, como la utilización del procedimiento de mínimos cuadrados para estimar los parámetros del modelo. ¿Es suficiente realizar una regresión con todos los datos “limpios?” En algunos casos no. Piensen en la paradoja de Simpson, la cual menciona que una tendencia que aparece en varios grupos de datos puede desaparece o invertirse cuando se combinan los grupos. Bajo este enfoque, siempre es bueno segregar la información si se considera que dicha segregación mejorará los resultados del modelo. ¿Hay otros métodos para crear un modelo de regresión sin utilizar el método de mínimos cuadrados? SÍ. 7.1 Comparación de modelos https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html modelsummary: regression tables with side-by-side models What is a Partial F-Test? StackExchange: ANOVA Table for Model In R StackExchange: What is residual standard error? 7.1.1 R^2 Standard Error of the Regression vs. R-squared 7.2 Análisis de valores influyentes 7.2.1 Outliers Como ya se ha mencionado, hay algunos datos que nos pueden generar problemas, y por lo mismo previamente se han eliminado algunos datos, como ciertos outliers y ceros en nuestras variables. Un outlier en nuestro modelo de regresión será aquel que sea lejano a su valor predicho. Para ver esto gráficamente vamos considerar nuestro ejemplo de regresión lineal simple visto en la sección de predicción (\\(\\log(ing\\_cor) = 10.265703 + 0.016142\\times \\sqrt{vestido}\\)) m1 &lt;- mutated_data_income %&gt;% filter(vestido&lt;7500 &amp; vestido&gt;0) %&gt;% ggplot(aes(x = sqrt(vestido) , y = log(ing_cor))) + geom_point() + #Hasta aquí los datos geom_abline(intercept = 10.265703, slope = 0.016142, color = &quot;red3&quot;) + geom_point(data = tibble(x = 35.440514, y = 7.535708), aes(x = x, y = y), color = &quot;red&quot;) + lims(y = c(7, 14)) + general_theme m2 &lt;- mutated_data_income %&gt;% filter(vestido&lt;7500 &amp; vestido&gt;0) %&gt;% filter(vestido!=1256.03) %&gt;% ggplot(aes(x = sqrt(vestido) , y = log(ing_cor))) + labs(y = &quot;&quot;) + lims(y = c(7, 14)) + geom_point() + #Hasta aquí los datos geom_abline(intercept = 10.267257, slope = 0.016167, color = &quot;red3&quot;) + general_theme plot_grid(m1, m2, ncol = 2, align = &quot;h&quot;) Le punto rojo es nuestro outlier en nuestro modelo inicial y en la misma subgráfica se aprecia la linea de regresión que se ajusta a tales datos. La segunda gráfica nos muestra la linea de regresión que resultad al omitir dicha observación. Como bien se ve, no se afecta realmente el comportamiento de la linea de regresión, pero veamos como cambiaron en otro aspecto los modelos data_vestido &lt;- mutated_data_income %&gt;% filter(vestido&lt;7500 &amp; vestido&gt;0) m1 &lt;- data_vestido %&gt;% lm(log(ing_cor)~sqrt(vestido), data = .) m2 &lt;- data_vestido %&gt;% filter(vestido!=1256.03) %&gt;% lm(log(ing_cor)~sqrt(vestido), data = .) l1 &lt;- m1 %&gt;% summary() l2 &lt;- m2 %&gt;% summary() cat(paste0(&quot;RSE del Modelo con outlier: &quot;,l1$r.squared, &quot;\\n&quot;)) RSE del Modelo con outlier: 0.154956818679954 cat(paste0(&quot;RSE del Modelo sin outlier: &quot;,l2$r.squared)) RSE del Modelo sin outlier: 0.15805697613396 Si bien el impacto en este caso no se ve tan drástico, este lo puede ser en otros modelos y hay que recordar que el RSE es utilizado para calcular los intervalos de confianza y los \\(p-values\\), por lo que un cambio tan fuerte para un sólo punto puede tener graves problemas de interpretación. Para identificar outliers, podemos visualizar dos gráficas: residuales vs valores ajustados y residuales estudentizados vs valores ajustados res_1 &lt;- (tibble(x = m1$fitted.values, y = residuals(m1)) %&gt;% ggplot(aes(x = x, y = y)) + geom_point(alpha = 0.5) + general_theme + labs(x = &quot;Valores ajustados&quot;, y = &quot;Residuales&quot;)) res_2 &lt;- (tibble(x = m1$fitted.values, y = rstudent(m1)) %&gt;% ggplot(aes(x = x, y = y)) + geom_point(alpha = 0.5) + geom_hline(yintercept = c(-3, 3), alpha = 0.5)+ general_theme + labs(x = &quot;Valores ajustados&quot;, y = &quot;Residuales Estudentizados&quot;)) res_1 + res_2 Aveces es difícil determinar si una observación debe ser considerado como outlier con la primera gráfica, pero en la segunda es más sencillo. De acuerdo al libro James et al. (2013), observaciones con residuales estudentizados que son más grandes que 3, en valor absoluto, pueden ser considerados como posibles outliers. Lo cual tiene sentido, ya que al “normalizar” los residuales así, se esta haciendo que estos sigan una distribución \\(t-student\\) con \\(n-k-1\\) grados de libertad, en nuestro caso \\(df = 1398-1-1 = 1396\\) library(latex2exp) tibble(x= c(-4, 4)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = ~dt(.x, 1396)) + labs(y = &quot;Densidad&quot;) + ggtitle(&quot;t-student con 1396 grados de libertad&quot;) + general_theme Entonces, de acuerdo a lo anterior tenemos, en este caso, 9 outliers. tibble(x = data_vestido$vestido , y = data_vestido$ing_cor, fitted_values = m1$fitted.values, rStudent = rstudent(m1)) %&gt;% mutate(index = row_number()) %&gt;% filter(rStudent&lt; -3 | rStudent&gt;3) # A tibble: 9 × 5 x y fitted_values rStudent index &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 2015. 580698. 11.0 3.69 2 2 1174. 364980. 10.8 3.21 333 3 822. 376175. 10.7 3.41 359 4 1272. 352488. 10.8 3.12 490 5 1256. 1874. 10.8 -5.37 759 6 235. 608965. 10.5 4.55 777 7 636. 479572. 10.7 3.89 1090 8 5350. 12138. 11.4 -3.30 1130 9 959. 4431. 10.8 -3.83 1318 Veamos que sucede con nuestro último modelo filter_data_ToOutliers &lt;- filter_data %&gt;% mutate(fitted_values = third_model_BX$fitted.values, rStandar = residuals(third_model_BX), rStudent = rstudent(third_model_BX), fitted_values = third_model_BX$fitted.values) r_third_model1 &lt;- filter_data_ToOutliers %&gt;% ggplot(aes(y = rStandar, x = fitted_values)) + labs(x = &quot;Valores ajustados&quot;, y = &quot;Residuales&quot;) + geom_point(alpha = 0.5) + general_theme r_third_model2 &lt;-filter_data_ToOutliers %&gt;% ggplot(aes(y = rStudent, x = fitted_values)) + geom_hline(yintercept = c(-3, 3), alpha = 0.5) + labs(x = &quot;Valores ajustados&quot;, y = &quot;Residuales estudentizados&quot;) + geom_point(alpha = 0.5) + general_theme r_third_model1 + r_third_model2 Es decir, tenemos 20 outliers filter_data_ToOutliers %&gt;% filter(rStudent&lt; -3 | rStudent&gt;3) # A tibble: 20 × 8 ing_cor transporte alimentos limpieza personales fitted_values rStandar &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 580698. 12326. 37594. 3318. 8520 13.3 2.12 2 114342. 579. 4706. 1131. 805. 11.2 2.10 3 3424. 4090. 8318. 1272. 1643. 12.0 -3.06 4 278952. 10627. 10749. 116. 1050. 11.9 2.60 5 439078. 24949. 9334. 17321. 5266. 13.1 1.99 6 3522. 2765. 7399. 435. 822. 11.6 -2.61 7 364980. 8725. 8421. 25218. 5336. 12.8 1.99 8 654367. 31987. 10202. 9120. 7542. 13.1 2.47 9 109957. 2483. 12722. 131. 26.1 10.9 2.34 10 8951. 2474. 17055. 2337. 2117. 12.2 -2.14 11 1874. 600 6313. 1900. 1322. 11.4 -3.23 12 608965. 2087. 15126. 1745. 3591. 12.2 3.32 13 9748. 8700 10067. 1503. 1943. 12.3 -2.09 14 144248. 600 8601. 618 1437 11.4 2.23 15 150376. 3484. 5484. 71.7 340. 11.1 2.53 16 479572. 16619. 20237. 2093. 615. 12.5 2.63 17 12138. 8890. 8916. 2081. 4420. 12.4 -1.98 18 1941. 15184. 9720. 1008. 7227. 12.6 -4.34 19 10715. 30803. 13526. 10147. 746. 12.8 -2.51 20 4431. 3267. 14908. 1304. 1710. 12.1 -2.89 # … with 1 more variable: rStudent &lt;dbl&gt; Si dicho outlier fue generado por un error de recolección de información, podemos eliminar la observación sin problemas, aunque hay que considerar que si no es el caso, entonces nuestro modelo simplemente no es lo suficientemente bueno para predecir valores que pueden suceder. En este caso vamos a eliminarlos considerando un limite en 3.5. filter_data_ToOutliers %&gt;% ggplot(aes(y = rStudent, x = fitted_values)) + geom_hline(yintercept = c(-3.5, 3.5), alpha = 0.5) + labs(x = &quot;Valores ajustados&quot;, y = &quot;Residuales estudentizados&quot;) + geom_point(alpha = 0.5, color = &quot;#447e9e&quot;) + geom_text( data = filter_data_ToOutliers %&gt;% mutate(index = row_number()) %&gt;% dplyr::filter(rStudent&lt; -3.5 | rStudent&gt;3.5), aes(label=index), nudge_x = 0.2 )+ general_theme fourth_model_1 &lt;- filter_data_ToOutliers %&gt;% filter(rStudent&gt;=-3.5 &amp; rStudent&lt;=3.5) %&gt;% lm(((ing_cor^lambdaBCox_tM-1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = .) fourth_model_1 %&gt;% summary() Call: lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = .) Residuals: Min 1Q Median 3Q Max -2.1395 -0.4038 -0.0162 0.3866 2.2298 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.8496905 0.1292413 52.999 &lt;2e-16 *** log(transporte) 0.2514986 0.0134651 18.678 &lt;2e-16 *** sqrt(alimentos) 0.0057975 0.0004841 11.975 &lt;2e-16 *** log(limpieza) 0.1593438 0.0171812 9.274 &lt;2e-16 *** log(personales) 0.1849702 0.0177541 10.418 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6049 on 2127 degrees of freedom Multiple R-squared: 0.5193, Adjusted R-squared: 0.5184 F-statistic: 574.5 on 4 and 2127 DF, p-value: &lt; 2.2e-16 fourth_model_1 %&gt;% autoplot() + general_theme Comparison_models &lt;- third_model_BX %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Tercel Modelo&quot;) %&gt;% dplyr::mutate_if(is.numeric, round, digits = 8) %&gt;% inner_join( fourth_model_1 %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Cuarto Modelo_1&quot;) %&gt;% dplyr::mutate_if(is.numeric, round, digits = 8), by = &quot;Estadística&quot; ) Comparison_models # A tibble: 12 × 3 Estadística `Tercel Modelo` `Cuarto Modelo_1` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 r.squared 0.489 0.519 2 adj.r.squared 0.488 0.518 3 sigma 0.642 0.605 4 statistic 511. 575. 5 p.value 0 0 6 df 4 4 7 logLik -2089. -1951. 8 AIC 4190. 3914. 9 BIC 4224. 3948. 10 deviance 881. 778. 11 df.residual 2139 2127 12 nobs 2144 2132 tests_linearM &lt;- function(model){ shapiro &lt;- model %&gt;% residuals() %&gt;% shapiro.test() breusch_pagan &lt;- model %&gt;% bptest() durbin_watson &lt;- model %&gt;% dwtest() breusch_godfrey &lt;- model %&gt;% bgtest() p_values &lt;- tibble(&quot;Estadística&quot; = c(&quot;Shapiro-Wils&quot;, &quot;Breusch-Pagan&quot;, &quot;Durbin-Watson&quot;, &quot;Breusch_Godfrey&quot;), &quot;P-value&quot; = c(shapiro$p.value, breusch_pagan$p.value, durbin_watson$p.value, breusch_godfrey$p.value)) %&gt;% dplyr::mutate_if(is.numeric, round, digits = 8) return(list(&quot;p-values&quot; = p_values, &quot;vif&quot; = model %&gt;% vif())) } fourth_model_1 %&gt;% tests_linearM() $`p-values` # A tibble: 4 × 2 Estadística `P-value` &lt;chr&gt; &lt;dbl&gt; 1 Shapiro-Wils 0.0152 2 Breusch-Pagan 0.137 3 Durbin-Watson 0.00000008 4 Breusch_Godfrey 0.00000019 $vif log(transporte) sqrt(alimentos) log(limpieza) log(personales) 1.412725 1.476318 1.559839 1.655387 Véase que seguimos “manteniendo” el supuesto de homocedasticidad y obtuvimos un mejor p-value para la prueba de normalidad aunque no mejoramos nuestro modelo para las pruebas de independencia, además de que no tenemos problemas de multicolinearidad. ¿Que hubiera sucedido si hubiéramos aplicado estos filtros a los datos del segundo modelo? ¿Hubiéramos conseguido un mejor modelo? 7.2.2 Valores con alto “apalancamiento” Mientras que en los outliers nos enfocamos a los valores inusuales de la variable respuesta para un \\(x_i\\), una observación con alto apalancamiento (leverage) tiene un valor inusual para \\(x_i\\). El leverage de una observación es una medida de la distancia entre los valores de sus variables explicativas y el promedio de los valores de las variables explicativas en todo el conjunto de datos. Observaciones con alto apalancamiento pueden tener una alta influencia en el modelo. Cuando sólo tenemos una variable explicativa en nuestro modelo, la estadística de apalancamiento queda definida de la siguiente manera: \\[ h_i = \\frac{1}{(n-1)}\\left[\\frac{x_i-\\bar{x}}{s_x}\\right]^2 + \\frac{1}{n} = \\frac{(x_i-\\bar{x})^2}{\\sum_{i^{&#39;} = 1}^n (x^{&#39;}-\\bar{x})^2} + \\frac{1}{n} \\] En el caso de regresión lineal simple es sencillo detectar este tipo de observaciones, ya que su valor predicho estará fuera del rango normal de las demás predicciones. Con la ecuación anterior podemos ver que \\(h_i\\) incrementa con la distancia entre \\(x_i\\) y \\(\\bar{x}\\). Esta estadística siempre estará entre \\(1/n\\) y 1 y el leverage promedio para todas las observaciones será igual a \\((p+1)/n\\) con \\(p\\) el número de coeficientes, por lo que si una observación tiene un leverage más grande que \\((p+1)/n\\) podríamos decir que se tiene un alto apalancamiento aunque algunos estadísticos prefieren utilizar como umbral \\(2*p/n\\). Ya habíamos retirado algunos outliers, vamos a ver si estas observaciones también son tienen un alto leverage. En R, podemos utilizar la función stats::hatvalues() filter_data_ToLeverage &lt;- filter_data_ToOutliers %&gt;% mutate(leverage = third_model_BX %&gt;% hatvalues(),index = row_number()) (Leverage_analysis &lt;- filter_data_ToLeverage %&gt;% ggplot(aes(x = index, y = leverage)) + geom_point(alpha = 0.5, color = &quot;#447e9e&quot;) + geom_hline(yintercept = (5+1)/third_model_BX$fitted.values %&gt;% length(), color = &quot;blue3&quot;, alpha = 0.5) + geom_hline(yintercept = 2*5/third_model_BX$fitted.values %&gt;% length(), color = &quot;red3&quot;, alpha = 0.5) + geom_hline(yintercept = 0.016, color = &quot;black&quot;, alpha = 0.5)+ geom_text( data = filter_data_ToLeverage %&gt;% dplyr::filter(leverage &gt;=0.016), aes(label=index), nudge_x = 100 )+ general_theme) Las lineas de colores tan sólo son distintos umbrales o cortes para establecer que observación tiene un alto apalancamiento. En este caso no tenemos que algún outlier también tiene un alto apalancamiento, lo cual es lo ideal. fourth_model_2 &lt;- filter_data_ToLeverage %&gt;% filter(rStudent&gt;=-3.5 &amp; rStudent&lt;=3.5) %&gt;% filter(leverage&lt; 0.016) %&gt;% lm(((ing_cor^lambdaBCox_tM-1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = .) fourth_model_2 %&gt;% summary() Call: lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = .) Residuals: Min 1Q Median 3Q Max -2.14019 -0.40307 -0.01616 0.38674 2.22978 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.8470768 0.1293183 52.947 &lt;2e-16 *** log(transporte) 0.2510460 0.0134731 18.633 &lt;2e-16 *** sqrt(alimentos) 0.0058011 0.0004842 11.981 &lt;2e-16 *** log(limpieza) 0.1602029 0.0174279 9.192 &lt;2e-16 *** log(personales) 0.1849316 0.0179337 10.312 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6049 on 2125 degrees of freedom Multiple R-squared: 0.5196, Adjusted R-squared: 0.5187 F-statistic: 574.6 on 4 and 2125 DF, p-value: &lt; 2.2e-16 fourth_model_2 %&gt;% autoplot() + general_theme Comparison_models &lt;- Comparison_models %&gt;% inner_join( fourth_model_2 %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Cuarto Modelo_2&quot;) %&gt;% dplyr::mutate_if(is.numeric, round, digits = 8), by = &quot;Estadística&quot; ) Comparison_models # A tibble: 12 × 4 Estadística `Tercel Modelo` `Cuarto Modelo_1` `Cuarto Modelo_2` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 r.squared 0.489 0.519 0.520 2 adj.r.squared 0.488 0.518 0.519 3 sigma 0.642 0.605 0.605 4 statistic 511. 575. 575. 5 p.value 0 0 0 6 df 4 4 4 7 logLik -2089. -1951. -1949. 8 AIC 4190. 3914. 3910. 9 BIC 4224. 3948. 3944. 10 deviance 881. 778. 778. 11 df.residual 2139 2127 2125 12 nobs 2144 2132 2130 fourth_model_2 %&gt;% tests_linearM() $`p-values` # A tibble: 4 × 2 Estadística `P-value` &lt;chr&gt; &lt;dbl&gt; 1 Shapiro-Wils 0.0148 2 Breusch-Pagan 0.136 3 Durbin-Watson 0.00000007 4 Breusch_Godfrey 0.00000017 $vif log(transporte) sqrt(alimentos) log(limpieza) log(personales) 1.413477 1.476237 1.583429 1.680891 Véase que al retirar estas observaciones el modelo no mejoró aunque no empeoro drásticamente. Otra estadística para determinar la influencia que tiene una observación en el modelo, es la estadística de Cook, la cual mide la influencia general de la observación, es decir que muestra el efecto que tiene omitir tal observación en el modelo de regresión. \\[ D_i = \\sum_{j = 1}^n\\frac{(\\hat{y}_{j(i)}-\\hat{y}_j)^2}{p\\hat{\\sigma}^2} \\] donde \\(\\hat{y}_j\\) es el \\(j-ésimo\\) valor en el ajuste usando todos las observaciones y \\(\\hat{y}_{j(i)}\\) es el valor de la regresión omitiendo la observación \\(i\\); \\(p\\) es el número de coeficientes (se incluye \\(\\beta_0\\)) y \\(\\sigma\\) la varianza estimada desde el modelo. El calculo de esta estadística es costoso ya que se tienen que hacer diferentes modelos de regresión, por lo que se prefiere utilizar una equivalencia \\[ D_i = \\frac{1}{p}(r_i)^2\\left(\\frac{h_i}{1-h_i}\\right) \\] donde \\(r_i\\) es el residual estandarizado aunque algunos autores prefieren los residuales estudentizados. El umbral que se establezca también dependerá de la información aunque algunos autores sugieren valores cercanos o más grandes que 1 son valores con gran influencia filter_data_ToCook &lt;- filter_data_ToLeverage %&gt;% mutate(Cook = third_model_BX %&gt;% cooks.distance()) (Cook_analysis &lt;- filter_data_ToCook %&gt;% ggplot(aes(x = index, y = Cook)) + geom_point(alpha = 0.5, color = &quot;#447e9e&quot;) + geom_hline(yintercept = 0.03, color = &quot;black&quot;, alpha = 0.5)+ geom_text( data = filter_data_ToCook %&gt;% dplyr::filter(Cook &gt;= 0.03), aes(label=index), nudge_x = 100 )+ general_theme) Es preferible estudiar los valores influyentes y outliers en conjunto, para determinar aquellos valores que tengan más problemas en conjunto without_axis_x &lt;- theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) Cook_analysis &lt;- filter_data_ToCook %&gt;% ggplot(aes(x = index, y = Cook)) + geom_point(alpha = 0.5, color = &quot;#447e9e&quot;) + geom_hline(yintercept = 0.02, color = &quot;black&quot;, alpha = 0.5)+ geom_text( data = filter_data_ToCook %&gt;% dplyr::filter(Cook &gt;= 0.02), aes(label=index), nudge_x = 100, size = 3 )+ general_theme + theme(text = element_text(size=10)) + labs(x= NULL) + without_axis_x Leverage_analysis &lt;- filter_data_ToLeverage %&gt;% ggplot(aes(x = index, y = leverage)) + geom_point(alpha = 0.5, color = &quot;#447e9e&quot;) + geom_hline(yintercept = 0.014, color = &quot;black&quot;, alpha = 0.5)+ geom_text( data = filter_data_ToLeverage %&gt;% dplyr::filter(leverage &gt;= 0.014), aes(label=index), nudge_x = 100, size = 3 )+ general_theme + labs(x= NULL) + without_axis_x ResidualSt_analysis &lt;- filter_data_ToCook %&gt;% ggplot(aes(x = index, y = rStudent)) + geom_point(alpha = 0.5, color = &quot;#447e9e&quot;) + geom_hline(yintercept = c(-3.5, 3.5), alpha = 0.5) + labs(x = &quot;Index&quot;, y = &quot;Residuales\\nestudentizados&quot;) + geom_text( data = filter_data_ToOutliers %&gt;% mutate(index = row_number()) %&gt;% dplyr::filter(rStudent&lt; -3.5 | rStudent&gt;3.5), size = 3, aes(label=index), nudge_x = 100)+ general_theme Cook_analysis / Leverage_analysis / ResidualSt_analysis Finalmente, vamos a retirar aquellas observaciones que causan mayores problemas y veamos como se comporta nuestro modelo fourth_model_3 &lt;- filter_data_ToCook %&gt;% filter(!(index %in% c(881, 190, 1858, 2023, 703, 515))) %&gt;% lm(((ing_cor^lambdaBCox_tM-1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = .) fourth_model_3 %&gt;% summary() Call: lm(formula = ((ing_cor^lambdaBCox_tM - 1)/lambdaBCox_tM) ~ log(transporte) + sqrt(alimentos) + log(limpieza) + log(personales), data = .) Residuals: Min 1Q Median 3Q Max -3.2355 -0.4063 -0.0165 0.3863 3.3151 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.8390940 0.1334738 51.239 &lt;2e-16 *** log(transporte) 0.2571418 0.0139177 18.476 &lt;2e-16 *** sqrt(alimentos) 0.0058523 0.0005006 11.690 &lt;2e-16 *** log(limpieza) 0.1543125 0.0178434 8.648 &lt;2e-16 *** log(personales) 0.1843599 0.0184677 9.983 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6263 on 2133 degrees of freedom Multiple R-squared: 0.5043, Adjusted R-squared: 0.5034 F-statistic: 542.5 on 4 and 2133 DF, p-value: &lt; 2.2e-16 fourth_model_3 %&gt;% autoplot() + general_theme Comparison_models &lt;- Comparison_models %&gt;% inner_join( fourth_model_3 %&gt;% glance() %&gt;% gather(&quot;Estadística&quot;, &quot;Cuarto Modelo_3&quot;) %&gt;% dplyr::mutate_if(is.numeric, round, digits = 8), by = &quot;Estadística&quot; ) Comparison_models # A tibble: 12 × 5 Estadística `Tercel Modelo` `Cuarto Modelo_1` `Cuarto Modelo_2` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 r.squared 0.489 0.519 0.520 2 adj.r.squared 0.488 0.518 0.519 3 sigma 0.642 0.605 0.605 4 statistic 511. 575. 575. 5 p.value 0 0 0 6 df 4 4 4 7 logLik -2089. -1951. -1949. 8 AIC 4190. 3914. 3910. 9 BIC 4224. 3948. 3944. 10 deviance 881. 778. 778. 11 df.residual 2139 2127 2125 12 nobs 2144 2132 2130 # … with 1 more variable: Cuarto Modelo_3 &lt;dbl&gt; fourth_model_3 %&gt;% tests_linearM() $`p-values` # A tibble: 4 × 2 Estadística `P-value` &lt;chr&gt; &lt;dbl&gt; 1 Shapiro-Wils 0 2 Breusch-Pagan 0.513 3 Durbin-Watson 0.0000346 4 Breusch_Godfrey 0.0000818 $vif log(transporte) sqrt(alimentos) log(limpieza) log(personales) 1.412271 1.474882 1.573000 1.670096 Con nuestro último modelo ya omitimos valores influyentes, retiramos outliers, tampoco estamos considerando algunos ceros y aplicamos diversas transformaciones, tanto para la variable dependiente y las variables independientes. Con este modelo logramos un modelo con homocedasticidad, al estar considerando una cantidad importante de datos podemos omitir el supuesto de normalidad aunque ay que tener precaución con las predicciones que se lleguen a dar. Otro punto a considerar es que el supuesto de independencia no se logro mejorar y para solucionar dicho problema tendríamos que considerar otro tipo de modelo. Unusual Observations Modelos de Regresión con R: Diagnósticos parte II Applied Statistics with R: Chapter 13 Model Diagnostics Regression Diagnostics OLS diagnostics: Influential data tests Aquí se dejan más enlaces: R Cookbook, 2nd Edition: 11 Linear Regression and ANOVA Beyond Multiple Linear Regression: Chapter 1 Review of Multiple Linear Regression Math 261A - Spring 2012: Multiple Linear Regression Data Science for Biological, Medical and Health Research: Notes for PQHS/CRSP/MPHP 431: Chapter 29 Multiple Regression: Introduction References "],["anova-vs-regresión-lineal.html", "Capítulo 8 ANOVA vs Regresión lineal", " Capítulo 8 ANOVA vs Regresión lineal pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } aov(ing_cor_log~transporte_sqrt + alimentos_sqrt +limpieza_sqrt + personales_sqrt, data = mutated_data_income) %&gt;% summary() anova(aov(ing_cor_log~transporte_sqrt + alimentos_sqrt +limpieza_sqrt + personales_sqrt, data = mutated_data_income)) La tabla ANOVA no sólo se puede utilizar en el ámbito de la regresión lineal. Si no que se puede utilizar como una prueba paramétrica (por lo que tiene una gran potencia en comparación de las pruebas no paramétricas) para determinar diferencias entre poblaciones. Aquí se deja un conjunto de enlaces: StackExchange: ANOVA : mean or variance? Why is ANOVA equivalent to linear regression? ANOVA and Linear Regression Why ANOVA and Linear Regression are the Same Analysis lindia "],["librerías-para-eda.html", "Capítulo 9 Librerías para EDA", " Capítulo 9 Librerías para EDA pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 300px; } skimr lares GGally [1] Stephanie Glen. “Hypothesis Testing” From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/ "]]
